[[transactional_topologies]]

== Transactional Topologies 

Transactional topologies is a new feature included in Storm 0.7.0, which allows the use of *transactional* schemas. We can execute a process and *commit* to set the data as processed, or *re-process* if something fails. For transactional schemas Storm uses a *once* messaging semantic, which guarantees that all *emitted transactions* will be committed in order.

Imagine that we need to create a counting system. A typical problem that Storm needs to handle is to prevent over-counting. If we count a tuple in one bolt and the next bolt fails, the tuple will be resent, so we need to make sure it's not counted twice. Transaction topologies are used to solve this kind of problem (rather than using an abstraction built on top of standard Storm spouts, bolts and topologies).

=== The design

Storm employs a mixture of parallel and sequential tuple processing. In a transactional topology, Storm creates a *Transaction Attempt*, which identifies a batch of tuples and guarantees that the batch will be *finished* or *committed* in order. A pipeline can be used to process several batches at the same time. In that case Storm divides processing into steps or phases:

- The processing phase: a fully parallel phase, many batches are executed at the same time.
- The commit phase: a strongly ordered phase, batch two is not committed until batch one has committed successfully.

We'll call both of these phases a *Storm Transaction*. If one of the two phases fails all transactions are re-processed.

TIP: Storm uses Zookeeper to store transaction metadata. By default the Zookeeper used will be the same run by the topology. To change it you can override the configuration keys *transactional.zookeeper.servers* and *transactional.zookeeper.port*.

=== Transactions in Action

To see how transactions work we'll create a word counter (as in the example in <<getting_started,chapter 2>>) based on link:http://redis.io[redis], which provides the capability to *replay* the data in case of errors.

==== The Spout

The spout in a transactional topology is completely different from a standard topology spout. Here we'll create a *sub-topology* formed by one *coordinator* (implemented by a single spout) which will be responsible for sending batches to the *emitters*. The emitters will be implemented using bolts, and will subscribe to the *coordinator* using an *all* grouping. In this example the transactional spout, the *coordinator*, will read ids from a queue of news texts. A new batch will be created for each text (we could group more than one text together if we wanted). The *emitter* will receive the batch and use it to read the text and emit it to the rest of topology.

To create a transactional spout we'll implement the interface ITransactionalSpout, as we can see in the class below:

TIP: The example can be downloaded from https://github.com/storm-book/examples-ch08-transactional-topologies

[source,java]
----
public class TextReader implements ITransactionalSpout<Integer> {

    @Override
    public backtype.storm.transactional.ITransactionalSpout.Coordinator<Integer> getCoordinator(
            Map conf, TopologyContext context) {
        return new WordsCoordinator();
    }

    @Override
    public backtype.storm.transactional.ITransactionalSpout.Emitter<Integer> getEmitter(
            Map conf, TopologyContext context) {
        return new WordEmitter();
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("tx","word"));
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}
----
_(src/main/java/spouts/TextReader.java)_

Here we can see our basic implementation of the WordReader. The first thing to notice is the use of *Generics* in the spout. The generic holds the type of metadata generated by the *coordinator* and sent to the *emitters*. In this example we'll use an Integer as our metadata because we only need to send the id's of the text to read. The *getCoordinator* method creates the coordinator object, and *getEmitter* creates all of the *emitter* objects (the number of emitters defines the parallelism of the spout)

Next we'll create a *coordinator* object:

[source,java]
----
public class TextCoordinator implements
        backtype.storm.transactional.ITransactionalSpout.Coordinator<Integer> {
    
    Jedis redisClient;
    
    public TextCoordinator() {
         redisClient = new Jedis("127.0.0.1");
    }
    
    @Override
    public void close() {
        redisClient.quit(); 
    }

    @Override
    public Integer initializeTransaction(BigInteger txid, Integer prevMetadata) {
        List<String> words = redisClient.blpop(0, "word_ids");
        if(words.size() > 0)
            return Integer.parseInt(words.get(1));
        return null;
    }

    @Override
    public boolean isReady() {
        return true;
    }
}
----
_(src/main/java/spouts/TextCoordinator.java)_

The most important method here is *initializeTransaction*, which will be responsible for creating the transaction batch. We use the *blpop* command (blocking left pop) to wait for a new element. When it is received we create a new batch with the corresponding id. These ids are emitted by the *emitter*:

[source,java]
----
public class TextEmitter implements 
        backtype.storm.transactional.ITransactionalSpout.Emitter<Integer>{

    Jedis redisClient;
    
    public TextEmitter() {
        redisClient = new Jedis("127.0.0.1"); 
    }

    @Override
    public void cleanupBefore(BigInteger txid) {
    }

    @Override
    public void close() {
        redisClient.quit();
    }

    @Override
    public void emitBatch(TransactionAttempt tx, Integer coordinatorMeta,
            BatchOutputCollector collector) {
        String word = redisClient.get(coordinatorMeta.toString());
        collector.emit(new Values(tx,word));
    }
}
---- 
_(src/main/java/spouts/TextEmitter.java)_

The *emitBatch* method receives the batch metadata (in this case the id of the text), and emits the received text. If the transaction fails the method will be called with the same parameters and emits the same tuple.

TIP: If we have a partitioned source, we can use the topology context at the Transaction spout and implement our own partitioner, or we can extend the transaction spout from IPartitionedTransactionalSpout which will solve the problem in an elegant fashion.

==== The Bolts

The topology will be formed by two bolts responsible for normalizing the words, and a *committer* bolt where we'll count all words received (in a real world example it would be in this bolt where we'd set a tuple as committed, thus allowing us to commit the next tuple).

In transaction topologies there are three kinds of bolts (actually there are really only 2, but one has a special property that we'll talk about shortly). We spoke about the *BasicBolt* in <<spouts,chapter 4>>. It doesn't really work with batches. It just emits tuples based on a single input tuple. A *BatchBolt* works as a standard bolt, but also implements the method *finishBatch*, which is called when the entire batch has been processed. A *BatchBolt* can also be set up as a *committer* bolt. This means that the *finishBatch* method will be called in a strongly ordered fashion, by transaction id. To set up a *BatchBolt* as a committer we need to implement the *ICommitter* interface, or use the method *setCommitterBolt* from the _TransactionalTopologyBuilder_ object (we'll see more about this object later in this chapter).

TIP: In a transactional topology you don't need to anchor or ack a transaction, because Storm takes responsibility for doing so, so as to optimize the process.

Coming back to the example, our word normalizer will be very similar to the example in <<getting_started,chapter 2>>, with the difference that we should emit the transaction attempt sent by the emitters.

[source,java]
----
public class WordNormalizer extends BaseBasicBolt {

    public void cleanup() {}

    public void execute(Tuple input, BasicOutputCollector collector) {
        String sentence = input.getString(1);
        String[] words = sentence.split(" ");
        for(String word : words){
            word = word.trim();
            if(!word.isEmpty()){
                word = word.toLowerCase();
                collector.emit(new Values(input.getValue(0), word));
            }
        }
    }
    

    /**
     * The bolt will only emit the field "word" 
     */
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("atemp","word"));
    }
}
----
_(src/main/java/bolts/WordNormalizer.java)_

As you can see, the code is very similar, but with the difference the the *atemp* property is emitted.

The last topology bolt to implement is the committer bolt:

[source,java]
----
public class WordCounter extends BaseTransactionalBolt implements ICommitter {

    Object id;
    String name;
    static Map<String, Integer> counters = new HashMap<String, Integer>();
    private BatchOutputCollector collector;

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id"));
    }

    @Override
    public void execute(Tuple input) {
        String str = input.getString(1);
        if(!counters.containsKey(str)){
            counters.put(str, 1);
        }else{
            Integer c = counters.get(str) + 1;
            counters.put(str, c);
        }       
    }

    @Override
    public void finishBatch() {
        System.out.println("-- Word Counter ["+name+"-"+id+"] --");
        for(Map.Entry<String, Integer> entry : counters.entrySet()){
            System.out.println(entry.getKey()+": "+entry.getValue());
        }
    }

    @Override
    public void prepare(Map conf, TopologyContext context,
            BatchOutputCollector collector, TransactionAttempt id) {
        this.collector = collector;

    }
}
----
_(src/main/java/bolts/WordCounter.java)_

There are several interesting things to note about *committer* bolts. You might ask why the counter map is _static_. Only one instance of the *BatchBolt* class is created per processing batch, so if we don't set this property as _static_ we'll only increment the value for a single batch (remember that it's an example, in a real world situation the bolt should update a persistent shared device, because it may run in different JVMs or machines). The second thing to note is the *finishBatch* method. In this example we just show the data for each batch transaction that is committed, but in a real world scenario the counter map would be protected, and when the batch finished the counters would be incremented on a shared device.

TIP: When using transaction topologies, if a transaction *fail*s we should throw a *FailedException*. The exception causes the entire batch to fail and be replayed (throwing this exception will not cause the process to crash)

==== The Topology

To create the *topology* object for a transactional topology we should create a *TransactionalTopologyBuilder*, as we can see in our topology code:

[source,java]
----
public class TopologyMain {
    public static void main(String[] args) throws Exception {
        TextReader wordReader = new TextReader();
        TransactionalTopologyBuilder builder = 
                new TransactionalTopologyBuilder("global-count", "word-reader", wordReader, 1);
        
        builder.setBolt("word-normalizer", new WordNormalizer(), 1)
                .noneGrouping("word-reader");
        builder.setBolt("word-counter", new WordCounter())
                .globalGrouping("word-normalizer");
        
        LocalCluster cluster = new LocalCluster();
        
        Config config = new Config();
        config.setDebug(false);
        cluster.submitTopology("global-count-topology", config, builder.buildTopology());
    }
}
----

It's similar to creating a standard topology, with the difference that the _TransactionalTopologyBuilder_ only accepts one bolt, and the parallelism depends on the number of *emmitters*.

TIP: When a transaction fails all subsequent transactions that have not been committed are considered failed too, and will be replayed. 

To run this topology, we need to first start the Redis server with the command *redis-server*. It will start with the default configuration. After that we can start the topology. To test it we run:

----
ID1=`redis-cli incr ids`
redis-cli set $ID1 "hi world this is my first transaction topology test" 
ID2=`redis-cli incr ids`
redis-cli set $ID2 "well, this is my second transaction topology test"
redis-cli lpush word_ids $ID1
redis-cli lpush word_ids $ID2
----

After running the topology we should see a result similar to the following:
----
tx:1
-- Word Counter [word-counter-1:-6574944285686710690] --
is: 1
test: 1
transaction: 1
topology: 1
hi: 1
first: 1
my: 1
this: 1
world: 1
tx:2
cleanup:1
-- Word Counter [word-counter-2:-7806429676133288762] --
is: 2
second: 1
test: 2
transaction: 2
topology: 2
hi: 1
well,: 1
first: 1
my: 2
this: 2
world: 1
----

=== Conclusion

In this chapter we discussed the use of transactional topologies, and how they can be used to create reliable ordered processing using Storm.
