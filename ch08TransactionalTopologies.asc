[[transactional_topologies]]

== Transactional Topologies 

Transactional topologies is a new feature include with storm 0.7.0 which adds the capability of process using the *transactional* schema where we can execute a process and *commit* to set the data as processed or *re-process* if something fails, when we are in a transaction storm will use the once messaging semantic, so storm will guarantee that all our *emmited transactions* will be commited in order.

Imagin that we are creating a counting system, a typical problem with storm will be how to reduce the overcounting if something fails, because if we count a tuple in one bolt and the next bolt fails, the tuple will be re-sended but we can't identify the error into our bolt counter to decrease our counter. So the transaction topologies are the mechanism to solve this problem, these are not other thing that an abstraction built on top of storm standar objects (spouts,bolts,topologies)

=== The design

Storm will use a mix between processing the tuples in full parallel fashion and processing the tuples in serie. Into the transactional topologies storm will create a *Transaction Attemp* which will identify a batch of tuples and guarantee that batches will be *finished* or *commited* in order, but will use pipeline to increment the amount of batches processing at the same time, to do that storm will divide the processing in to steps or phases

- The processing phase: this is a full parallel phase and can be executed by many batches at the same time
- The commit phase: it's an strongly ordered phase, so the commit for batch two is not done untile the commit for batch one has been succefull

We'll call the two phases together as an *Storm Transaction* and if one of the two phases fails all the transaction is re-process

TIP: Zookeeper

=== Transactions in Action

To see how the transactions works we'll create a word counter (like the example at the <<getting_started,chapter 2>>) based on link:http://redis.io[redis] which will provide us the capability of *replay* the data in case of errors

==== The Spout

The spout into a transactional topology is completly different from an standar topology, here we'll create *sub-topology* formed by one *coordinator* (implemented by a single spout) which will be responsible for send to the *emitters* the batches, this emiters will be implemented by bolts and will be subscribed to the *coordinator* using all grouping. In our example our transactional spout, the *coordinator* will read from a news queue the ids of texts stored creating a new batch for each text (we could group more than one text if we want), then the *emitter* will receive the batch using it to read the text and emit it to the rest of topology.

To create a transactional spout we'll implement the interface ITransactionalSpout, as we can see in the class below:

TIP: The example can be downloaded from https://github.com/storm-book/examples-ch08-transactional-topologies

[source,java]
----
public class TextReader implements ITransactionalSpout<Integer> {

    @Override
    public backtype.storm.transactional.ITransactionalSpout.Coordinator<Integer> getCoordinator(
            Map conf, TopologyContext context) {
        return new WordsCoordinator();
    }

    @Override
    public backtype.storm.transactional.ITransactionalSpout.Emitter<Integer> getEmitter(
            Map conf, TopologyContext context) {
        return new WordEmitter();
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("tx","word"));
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}
----
_(src/main/java/spouts/TextReader.java)_

Here we can see our basic implementation of the WordReader. The first thing that we can see is the use of *Generics* into the spout, this generic will be the type of the metadata generated by the *coordinator* to be sended to the *emiters*, in our case we'll use an Integer as our metadata because we only need to send the id's of the text to read. The next that we can see are the *getCoordinator* method which will create our corrdinator object and the *getEmitter* which will create all of our *emitter* object (the amout of emitters will be the spout spoutParallelism)

The next thing that we'll create will be the *coordinator* object:

[source,java]
----
public class TextCoordinator implements
        backtype.storm.transactional.ITransactionalSpout.Coordinator<Integer> {
    
    Jedis redisClient;
    
    public TextCoordinator() {
         redisClient = new Jedis("127.0.0.1");
    }
    
    @Override
    public void close() {
        redisClient.quit(); 
    }

    @Override
    public Integer initializeTransaction(BigInteger txid, Integer prevMetadata) {
        List<String> words = redisClient.blpop(0, "word_ids");
        if(words.size() > 0)
            return Integer.parseInt(words.get(1));
        return null;
    }

    @Override
    public boolean isReady() {
        return true;
    }
}
----
_(src/main/java/spouts/TextCoordinator.java)_

The most important method here is the *initializeTransaction* which will be responsible for create the transaction batch, here we can see how we are using the *blpop* command (blocking left pop) to wait for a new element, when an element is received we'll create a new batch with this id, so it's time to see how this ids are emited by the *emitter*:

[source,java]
----
public class TextEmitter implements 
        backtype.storm.transactional.ITransactionalSpout.Emitter<Integer>{

    Jedis redisClient;
    
    public TextEmitter() {
        redisClient = new Jedis("127.0.0.1"); 
    }

    @Override
    public void cleanupBefore(BigInteger txid) {
    }

    @Override
    public void close() {
        redisClient.quit();
    }

    @Override
    public void emitBatch(TransactionAttempt tx, Integer coordinatorMeta,
            BatchOutputCollector collector) {
        String word = redisClient.get(coordinatorMeta.toString());
        collector.emit(new Values(tx,word));
    }
}
---- 
_(src/main/java/spouts/TextEmitter.java)_

In this code we can see the *emitBatch* method which receive the batch metadata (in this case the if of the text) and emit the text getted, in case of a transaction fails the method will be called with the same parameters and we should emit the same tuple.

TIP: If we have a partitioned source, we could use the topology context at the Transaction spout and implements our own partitioner or we can extend the transaction spout from IPartitionedTransactionalSpout which will solve the problem in an elegant fashion

==== The Bolts

Our topology will be formed by two bolts responsibles for normalize the words and a *comitter* bolt that will be where we count all words received (in a real case is in this bolt where we'll set a tuple as commited therefore finished giving the posibility to commit the next tuple).

In transaction topologies we'll have 3 kinds of bolts (they are really 2 but we'll see an special property of one). The first bolt is the *BasicBolt* that we've spoke at the <<spouts,chapter 4>> and really doesn't work with batches, it only emit tuples based on a single tuple input. The second bolt kind is *BatchBolt*, this works as an standar bolt but implements the method *finishBatch* that is called when all batch is processed. Finally we can set up a *BatchBolt* as a *commiter* bolt, this means that the *finishBatch* method will be called strongly ordered by transaction Id, to set a *BatchBolt* as committer we can implements the *ICommitter* interface in our *BatchBolt* or use the method *setCommitterBolt* from the _TransactionalTopologyBuilder_ object (we'll se more about this object later in this chapter).

TIP: In a transactional topology you haven't anchor or acking any transaction because storm take the responsibility about it and optimize the process

Coming back to the example our word normalizer will be very similar to the example at the <<getting_started,chapter 2>> with the difference that we should emit the transaction atemp sent by the emiters

[source,java]
----
public class WordNormalizer extends BaseBasicBolt {

    public void cleanup() {}

    public void execute(Tuple input, BasicOutputCollector collector) {
        String sentence = input.getString(1);
        String[] words = sentence.split(" ");
        for(String word : words){
            word = word.trim();
            if(!word.isEmpty()){
                word = word.toLowerCase();
                collector.emit(new Values(input.getValue(0), word));
            }
        }
    }
    

    /**
     * The bolt will only emit the field "word" 
     */
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("atemp","word"));
    }
}
----
_(src/main/java/bolts/WordNormalizer.java)_

As we can say is a very similar code with the difference of emit the *atemp* property

To finish with the topology bolts we'll see the code of our committer bolt:

[source,java]
----
public class WordCounter extends BaseTransactionalBolt implements ICommitter {

    Object id;
    String name;
    static Map<String, Integer> counters = new HashMap<String, Integer>();
    private BatchOutputCollector collector;

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id"));
    }

    @Override
    public void execute(Tuple input) {
        String str = input.getString(1);
        if(!counters.containsKey(str)){
            counters.put(str, 1);
        }else{
            Integer c = counters.get(str) + 1;
            counters.put(str, c);
        }       
    }

    @Override
    public void finishBatch() {
        System.out.println("-- Word Counter ["+name+"-"+id+"] --");
        for(Map.Entry<String, Integer> entry : counters.entrySet()){
            System.out.println(entry.getKey()+": "+entry.getValue());
        }
    }

    @Override
    public void prepare(Map conf, TopologyContext context,
            BatchOutputCollector collector, TransactionAttempt id) {
        this.collector = collector;

    }
}
----
_(src/main/java/bolts/WordCounter.java)_

Here we have many interesting things to see at the *committer* bolts, the first that may be take your attention is, why is the counter map set as _static_?, well it's beacause one instance of *BatchBolt* class is created per each processing batch, so if we don't set this property as _static_ we'll only increment value for a single batch (remember that it's an example, in a real world this bolt should update a persistent shared device because it could run in differents jvm and machines). The second thing that we can see is the *finishBatch* method, in this example we only show the actual data per batch trasanction comited, but in a real scenario we should set the counter map as protected and at finish batch increment the counters into a shared device.

TIP: If should *fail* a transaction when we are using transaction topologies we shoul throw a *FailedException*, this exception will fail the batch and cause the batch to be replayes (throw this exception will not cause the process crash)

==== The Topology

To create the *topology* object for a transactional topology we should create a *TransactionalTopologyBuilder* as we can see in our topology code:

[source,java]
----
public class TopologyMain {
    public static void main(String[] args) throws Exception {
        TextReader wordReader = new TextReader();
        TransactionalTopologyBuilder builder = 
                new TransactionalTopologyBuilder("global-count", "word-reader", wordReader, 1);
        
        builder.setBolt("word-normalizer", new WordNormalizer(), 1)
                .noneGrouping("word-reader");
        builder.setBolt("word-counter", new WordCounter())
                .globalGrouping("word-normalizer");
        
        LocalCluster cluster = new LocalCluster();
        
        Config config = new Config();
        config.setDebug(false);
        cluster.submitTopology("global-count-topology", config, builder.buildTopology());
        
        Thread.sleep(30000);
        cluster.shutdown();
    }
}
----

As we can see it's similar to create an standar topology with the difference that the _TransactionalTopologyBuilder_ only accept one bolt and the parallelism set is the amount of *emmitters* 

TIP: When a transaction fails all subsequent transactions that have not been committed are considered failed too and will be replayed. 

