[[transactional_topologies]]

== Transactional Topologies 

With Storm you can guarantee message processing by using _ack_ and _fail_ strategy, as mentioned earlier in the book. But what happens if tuples are replayed? How do you make sure you won't overcount?

_Transactional Topologies_ is a new feature, included in Storm 0.7.0, that enables exactly once messaging semantics. This way we can replay tuples in a secure way and make sure we process them only once. Without support for transactional topologies we wouldn't be able to count in a fully-accurate, scalable and fault-tolerant way.

NOTE: Transactional Topologies are an abstraction built on top of standard Storm spouts and bolts.

=== The design

In a transactional topology, Storm uses a mix of parallel and sequential tuple processing. The spout generates batches of tuples, that are processed by the bolts in parallel. Some of those bolts are known as _commiters_, and they commit processed batches in a strictly ordered fashion. This means that if we have two batches, with five tuples each, both tuples will be processed in parallel by the bolts, but the commiter bolts won't commit the second tuple, until the first tuple is committed sucessfully.

This can be described as two different steps, or phases:

- The processing phase: a fully parallel phase, many batches are executed at the same time.
- The commit phase: a strongly ordered phase, batch two is not committed until batch one has committed successfully.

We'll call both of these phases a _Storm Transaction_.

TIP: Storm uses Zookeeper to store transaction metadata. By default the one used for the topology, will be used to store the metadata. You can change this by overriding the configuration key _transactional.zookeeper.servers_ and _transactional.zookeeper.port_.

=== Transactions in Action

To see how transactions work we'll create a tweeter analytics tool. We'll be reading tweets stored in a redis database, process them through a few bolts and finally store in another redis database a list of all hashtags and their frequency, a list of all users and amount of tweets they appear and finally a list of top hashtags 




counter, which use redis as a queue sytem to push new words and we'll use redis to store the word counts too (It's an example, in a real life use case is recommendable use different redis or storage systems). Using redis as database we could see in the example one strategy to generate idempotent updates.

==== The Spout

The spout in a transactional topology is completely different from a standard topology spout. Here we'll create a *sub-topology* formed by one *coordinator* (implemented by a single spout) which will be responsible for sending batches to the *emitters*. The emitters will be implemented using bolts, and will subscribe to the *coordinator* using an *all* grouping. In this example the transactional spout, the *coordinator*, will read ids from a queue of news texts. A new batch will be created for each text (we could group more than one text together if we wanted). The *emitter* will receive the batch and use it to read the text and emit it to the rest of topology.

To create a transactional spout we'll implement the interface ITransactionalSpout, as we can see in the class below:

TIP: The example can be downloaded from https://github.com/storm-book/examples-ch08-transactional-topologies

[source,java]
----
public class TextReader implements ITransactionalSpout<Integer> {

    @Override
    public backtype.storm.transactional.ITransactionalSpout.Coordinator<Integer> getCoordinator(
            Map conf, TopologyContext context) {
        return new WordsCoordinator();
    }

    @Override
    public backtype.storm.transactional.ITransactionalSpout.Emitter<Integer> getEmitter(
            Map conf, TopologyContext context) {
        return new WordEmitter();
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("tx","word"));
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}
----
_(src/main/java/spouts/TextReader.java)_

Here we can see our basic implementation of the WordReader. The first thing to notice is the use of *Generics* in the spout. The generic holds the type of metadata generated by the *coordinator* and sent to the *emitters*. In this example we'll use an Integer as our metadata because we only need to send the id's of the text to read. The *getCoordinator* method creates the coordinator object, and *getEmitter* creates all of the *emitter* objects (the number of emitters defines the parallelism of the spout)

Next we'll create a *coordinator* object:

[source,java]
----
public class TextCoordinator implements
        backtype.storm.transactional.ITransactionalSpout.Coordinator<Integer> {
    
    Jedis redisClient;
    
    public TextCoordinator() {
         redisClient = new Jedis("127.0.0.1");
    }
    
    @Override
    public void close() {
        redisClient.quit(); 
    }

    @Override
    public Integer initializeTransaction(BigInteger txid, Integer prevMetadata) {
        List<String> words = redisClient.blpop(0, "word_ids");
        if(words.size() > 0)
            return Integer.parseInt(words.get(1));
        return null;
    }

    @Override
    public boolean isReady() {
        return true;
    }
}
----
_(src/main/java/spouts/TextCoordinator.java)_

The most important method here is *initializeTransaction*, which will be responsible for creating the transaction batch. We use the *blpop* command (blocking left pop) to wait for a new element. When it is received we create a new batch with the corresponding id. These ids are emitted by the *emitter*:

[source,java]
----
public class TextEmitter implements 
        backtype.storm.transactional.ITransactionalSpout.Emitter<Integer>{

    Jedis redisClient;
    
    public TextEmitter() {
        redisClient = new Jedis("127.0.0.1"); 
    }

    @Override
    public void cleanupBefore(BigInteger txid) {
    }

    @Override
    public void close() {
        redisClient.quit();
    }

    @Override
    public void emitBatch(TransactionAttempt tx, Integer coordinatorMeta,
            BatchOutputCollector collector) {
        String word = redisClient.get(coordinatorMeta.toString());
        collector.emit(new Values(tx,word));
    }
}
---- 
_(src/main/java/spouts/TextEmitter.java)_

The *emitBatch* method receives the batch metadata (in this case the id of the text), and emits the received text. If the transaction fails the method will be called with the same parameters and emits the same tuple.

TIP: If we have a partitioned source, we can use the topology context at the Transaction spout and implement our own partitioner, or we can extend the transaction spout from IPartitionedTransactionalSpout which will solve the problem in an elegant fashion.

==== The Bolts

The topology will be formed by two bolts, one responsible for normalizing the words, and other (a *committer* bolt) which will do the partial adds and of the words received and will update this information in redis.

In transaction topologies there are three kinds of bolts (actually there are really only 2, but one has a special property that we'll talk about shortly). We spoke about the *BasicBolt* in <<spouts,chapter 4>>. It doesn't really work with batches, tt just emits tuples based on a single input tuple. A *BatchBolt* works as a standard bolt, but also implements the method *finishBatch*, which is called when the entire batch has been processed. A *BatchBolt* can also be set up as a *committer* bolt. This means that the *finishBatch* method will be called in a strongly ordered fashion, by transaction id. To set up a *BatchBolt* as a committer we need to implement the *ICommitter* interface, or use the method *setCommitterBolt* from the _TransactionalTopologyBuilder_ object (we'll see more about this object later in this chapter).

TIP: In a transactional topology you don't need to anchor or ack a transaction, because Storm takes responsibility for doing so, so as to optimize the process.

Coming back to the example, our word normalizer will be very similar to the example in <<getting_started,chapter 2>>, with the difference that we should emit the transaction attempt sent by the emitters.

[source,java]
----
public class WordNormalizer extends BaseBatchBolt {

    private BatchOutputCollector collector;
    private Object id;
    private List<String> words;
    
    public void cleanup() {}

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("atemp","word"));
    }

    public void execute(Tuple tuple) {
        String sentence = tuple.getString(1);
        String[] words = sentence.split(" ");
        for(String word : words){
            word = word.trim();
            if(!word.isEmpty()){
                word = word.toLowerCase();
                this.words.add(word);
            }
        }       
    }

    public void finishBatch() {
        for(String word : this.words)
            collector.emit(new Values(id, word));
    }

    public void prepare(Map conf, TopologyContext context,
            BatchOutputCollector collector, Object id) {
        this.collector = collector;
        this.id = id;
        this.words = new ArrayList<String>();
    }
}
----
_(src/main/java/bolts/WordNormalizer.java)_

As you can see, the code is similar to the code implemented in <<getting_started, chapter 2>>. The main difference that we can found is that here we'll generate an array with all words normalized that will be sent when all tuples owned by the attemp have been processed, so if something fails when we are working with the words all transaction will be reprocessed

This kind of bolts has two interesting properties, ones is that a new instance of this bolt is created per each transaction attemp, the second, as it bolt isn't a commiter bolt, could exist many instances of this processing in parallel fashion.

The last topology bolt to implement is the committer bolt:

[source,java]
----
public class WordCounter extends BaseTransactionalBolt implements ICommitter {

    TransactionAttempt attemp;
    String name;
    Map<String, Integer> partialCounters = new HashMap<String, Integer>();
    private BatchOutputCollector collector;
    private Jedis redisClient;

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id"));
    }

    @Override
    public void execute(Tuple input) {
        String str = input.getString(1);
        if(!partialCounters.containsKey(str)){
            partialCounters.put(str, 1);
        }else{
            Integer c = partialCounters.get(str) + 1;
            partialCounters.put(str, c);
        }       
    }

    public void finishBatch() {
        String txId = redisClient.get("lastTx");
        if(txId == null || !this.attemp.getTransactionId().equals(new BigInteger(txId))){
            Transaction redisTransaction = redisClient.multi(); 
            for(Map.Entry<String, Integer> entry : partialCounters.entrySet()){
                redisTransaction.incrBy(entry.getKey(),entry.getValue().longValue());
            }
            redisTransaction.set("lastTx", this.attemp.getTransactionId().toString());
            List<Object> result = redisTransaction.exec();
            if(result == null)
                throw new FailedException("Error running redis transaction");
        }
    }

    public void prepare(Map conf, TopologyContext context,
            BatchOutputCollector collector, TransactionAttempt attemp) {
        this.collector = collector;
        this.name = context.getThisComponentId();
        this.attemp = attemp;   
        this.redisClient = new Jedis("127.0.0.1"); 
    }

}
----
_(src/main/java/bolts/WordCounter.java)_

This bolt it's also a *BaseTransactionalBolt* and the behavior of this bolt will be similar to the *WordNormalizer bolt*, where we should process and acumulate the data into local variables that will be used in the *finishBatch* phase, the main difference with the *WordNormalizer* bolt it's as this bolt is set as *commiter* the *finisheBatch* phase of *WordCounter* bolt will be called in strong order by transaction id, so here it's where we should set up the logic to do the transactions idempotents. In this bolt to do idempotent the transaction we'll use the strategy of check about the last transaction id commited to redis, if this is different to the actual transaction id then the transaction have not been processed and we should increment the words counter, to do it in one transaction we'll use the command *multi* of redis which enable us to do many operations in one transaction, as we can see we'll run the increments of each word and after that we'll update the transaction id 

TIP: When using transaction topologies, if a transaction *fail*s we should throw a *FailedException*. The exception causes the entire batch to fail and be replayed (throwing this exception will not cause the process to crash)

==== The Topology

To create the *topology* object for a transactional topology we should create a *TransactionalTopologyBuilder*, as we can see in our topology code:

[source,java]
----
public class TopologyMain {
    public static void main(String[] args) throws Exception {
        TextReader wordReader = new TextReader();
        TransactionalTopologyBuilder builder = 
                new TransactionalTopologyBuilder("global-count", "word-reader", wordReader, 1);
        
        builder.setBolt("word-normalizer", new WordNormalizer(), 1)
                .noneGrouping("word-reader");
        builder.setBolt("word-counter", new WordCounter())
                .globalGrouping("word-normalizer");
        
        LocalCluster cluster = new LocalCluster();
        
        Config config = new Config();   
        config.setDebug(false);
        cluster.submitTopology("global-count-topology", config, builder.buildTopology());
    }
}
----

It's similar to creating a standard topology, with the difference that the _TransactionalTopologyBuilder_ only accepts one bolt, and the parallelism depends on the number of *emmitters*.

TIP: When a transaction fails all subsequent transactions that have not been committed are considered failed too, and will be replayed. 

To run this topology, we need to first start the Redis server with the command *redis-server*. It will start with the default configuration. After that we can start the topology. To test it we run:

----
ID1=`redis-cli incr ids`
redis-cli set $ID1 "hi world this is my first transaction topology test" 
ID2=`redis-cli incr ids`
redis-cli set $ID2 "well, this is my second transaction topology test"
redis-cli lpush word_ids $ID1
redis-cli lpush word_ids $ID2
----

After running the topology we could check the processed counters running:
----
redis-cli get hi
redis-cli get is
...
reids-cli get :some_other_processed_word
----

=== Opaque Transaction Topologies

Opaque transaction topologies are non idempoten transactions topologies (opaque is the opposite of idempotent), this kind of transaction topologies is usefull when a whole transaction couldn't be replayed in case of fail. In an standar transaction topology if a transaction fails the transaction will be regereted with the same id and will be reprocessed, however there are many cases where it's imposible replay a completly transaction because the used data is inaccessible at the transaction fail time or any other problem that could happend. So the opaque transaction can help us to work with this problems, this kind of topologies enable us to generate a different batch for a failed transaction id.

The problem here it's how to guarantee that a tuple is not processed again, becouse if we use the method saw before, where we check that the transaction stored is different from the actually transaction processing we could have problems because if a transaction fails and the new batch generated is different will count other time a part of the same batch or we could exclude some value. So a method to work with this problem is store the previous state of the transaction values so we could have the next 2 cases when a transaction is processed:

* The transaction id of the database is the same that we are processig: In this case the only thing that we should do it's update the value using the value stored before, If we use the example before, insted of use incrBy we should use a get of the value before, add this with the partial count and set it to redis.

* The transaction id of the database is different from which we are processing: in this case we'll move the actual value as the value before, we'll update the new value with the calculated delta and we'll update the processed transaction

Using this method we can be secure about the process only one time the information because if is the same transaction id which we are processing we base our calculous on the previuos value of the data and if the transaction id is different we'll only update the values. Same thin interesting to remark is that when a transaction fails all non commited transactions fails too so we can considerate every commit as a checkpoint and on a transactions after the checkpoint will be re-processed, with this feature we can be protected, becaous on a fail we'll be sure that the data will not processed 2 times.

To create an opaque transaction the differents from the traditional transaction toplogy will be:

* The transaction logic in our commiter bolts

* Our spout should implement *IOpaquePartitionedTransactionalSpout* or extend from any sub-class of this interface lie *BaseOpaquePartitionedTransactionalSpout*

=== Conclusion

In this chapter we discussed the use of transactional topologies, and how they can be used to create reliable ordered processing using Storm.
