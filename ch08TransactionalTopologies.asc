[[transactional_topologies]]

== Transactional Topologies 

Transactional topologies is a new feature include with storm 0.7.0 which adds the capability of process using the *transactional* schema where we can execute a process and *commit* to set the data as processed or *re-process* if something fails, when we are in a transaction storm will use the once messaging semantic, so storm will guarantee that all our *emmited transactions* will be commited in order.

Imagin that we are creating a counting system, a typical problem with storm will be how to reduce the overcounting if something fails, because if we count a tuple in one bolt and the next bolt fails, the tuple will be re-sended but we can't identify the error into our bolt counter to decrease our counter. So the transaction topologies are the mechanism to solve this problem, these are not other thing that an abstraction built on top of storm standar objects (spouts,bolts,topologies)

=== The design

Storm will use a mix between processing the tuples in full parallel fashion and processing the tuples in serie. Into the transactional topologies storm will create a *Transaction Attemp* which will identify a batch of tuples and guarantee that batches will be *finished* or *commited* in order, but will use pipeline to increment the amount of batches processing at the same time, to do that storm will divide the processing in to steps or phases

- The processing phase: this is a full parallel phase and can be executed by many batches at the same time
- The commit phase: it's an strongly ordered phase, so the commit for batch two is not done untile the commit for batch one has been succefull

We'll call the two phases together as an *Storm Transaction* and if one of the two phases fails all the transaction is re-process

TIP: Zookeeper

=== Transactions in Action

To see how the transactions works we'll create a word counter (like the example at the <<getting_started,chapter 2>>) based on link:http://redis.io[redis] which will provide us the capability of *replay* the data in case of errors

==== The Spout

The spout into a transactional topology is completly different from an standar topology, here we'll create *sub-topology* formed by one *coordinator* (implemented by a single spout) which will be responsible for send to the *emitters* the batches, this emiters will be implemented by bolts and will be subscribed to the *coordinator* using all grouping. In our example our transactional spout, the *coordinator* will read from a news queue the ids of texts stored creating a new batch for each text (we could group more than one text if we want), then the *emitter* will receive the batch using it to read the text and emit it to the rest of topology.

To create a transactional spout we'll implement the interface ITransactionalSpout, as we can see in the class below:

[soruce,java]
----
public class WordReader implements ITransactionalSpout<Integer> {

    @Override
    public backtype.storm.transactional.ITransactionalSpout.Coordinator<Integer> getCoordinator(
            Map conf, TopologyContext context) {
        return new WordsCoordinator();
    }

    @Override
    public backtype.storm.transactional.ITransactionalSpout.Emitter<Integer> getEmitter(
            Map conf, TopologyContext context) {
        return new WordEmitter();
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("tx","word"));
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        return null;
    }
}
----


TIP: partitioned

==== The Bolts

TODO

==== The Topology

TIP: Tranaction fails
