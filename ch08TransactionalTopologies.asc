[[transactional_topologies]]

== Transactional Topologies 

With Storm you can guarantee message processing by using an _ack_ and _fail_ strategy, as mentioned earlier in the book. But what happens if tuples are replayed? How do you make sure you won't overcount?

_Transactional Topologies_ is a new feature, included in Storm 0.7.0, that enables _exactly-once_ messaging semantics. That means we can replay tuples but make sure they are only processed at most once. Without support for transactional topologies we wouldn't be able to count in a fully-accurate, scalable and fault-tolerant way.

NOTE: Transactional Topologies are an abstraction built on top of standard Storm spouts and bolts.

=== The design

In a transactional topology, Storm uses a mix of parallel and sequential tuple processing. The spout generates batches of tuples that are processed by the bolts in parallel. Some of those bolts are known as _committers_, and they commit processed batches in a strictly ordered fashion. This means that if we have two batches, with five tuples each, both tuples will be processed in parallel by the bolts, but the committer bolts won't commit the second tuple until the first tuple has committed successfully.

This can be described as two different steps, or phases:

- The processing phase: a fully parallel phase, in which many batches are executed at the same time.
- The commit phase: a strongly ordered phase, in which batch two is not committed until batch one has committed successfully.

We'll call both of these phases a _Storm Transaction_.

NOTE: When dealing with transactional topologies, it is important to be able to replay batches of tuples from the source, sometimes many times. So make sure your source of data, the one that your spout will be connected to, has the ability to replay.

TIP: Storm uses Zookeeper to store transaction metadata. By default the one used for the topology will be used to store the metadata. You can change this by overriding the configuration key _transactional.zookeeper.servers_ and _transactional.zookeeper.port_.

=== Transactions in Action

To see how transactions work we'll create a tweet analytics tool. It will read tweets from a Redis database, process them using several bolts to produce the list of all hashtags and their frequency among the tweets, the list of all users and amount of tweets they appear in, and finally a list of users with their hashtags and frequency, and store this in another Redis database.

The topology we'll build for this tool is described in <<FIG801>>.

.Topology overview
[[FIG801]]
image::figs/ch08-transactional-topology.jpg[]

As you can see, +TweetsTransactionalSpout+ connects to the tweets database and emits batches of tuples across the topology. Two different bolts, +UserSplitterBolt+ and +HashtagSplitterBolt+, receive tuples from the spout. +UserSplitterBolt+ parses the tweet to look for users, that is words preceded by _@_, and emits these words in a custom stream called _users_. The +HashatagSplitterBolt+ also parses the tweet, looking for words preceded by _#_, and emits these words in a custom stream called _hashtags_. A third bolt, the +UserHashtagJoinBolt+, receives both streams and counts how many times a hashtag has appeared in a tweet where a user was named. To count and emit the result we use a +BaseBatchBolt+ (more on that later).

The last bolt, called +RedisCommitterBolt+, receives the three streams generated by +UserSplitterBolt+, +HashtagSplitterBolt+ and +UserHashtagJoinBolt+. It counts everything and once the batch of tuples has finished processing, stores it to Redis in a single transaction. This bolt is a special kind of bolt called a _committer bolt_, explained later in this chapter.

In order to build this topology we use +TransactionalTopologyBuilder+, as in the following example:

[source,java]
----
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("test", "spout", new TweetsTransactionalSpout());

builder.setBolt("users-splitter", new UserSplitterBolt(), 4).shuffleGrouping("spout");
builder.setBolt("hashtag-splitter", new HashtagSplitterBolt(), 4).shuffleGrouping("spout");

builder.setBolt("user-hashtag-merger", new UserHashtagJoinBolt(), 4)
  .fieldsGrouping("users-splitter","users", new Fields("tweet_id"))
  .fieldsGrouping("hashtag-splitter", "hashtags", new Fields("tweet_id"));

builder.setBolt("redis-commiter", new RedisCommiterCommiterBolt())
  .globalGrouping("users-splitter","users")
  .globalGrouping("hashtag-splitter", "hashtags")
  .globalGrouping("user-hashtag-merger");
----

Lets see how we can implement the spout in a transactional topology.

==== The Spout

The spout in a transactional topology is completely different from a standard spout.

[source,java]
----
public class TweetsTransactionalSpout extends BaseTransactionalSpout<TransactionMetadata>{
----

As you can see in the class definition, +TweetsTransactionalSpout+ extends +BaseTransactionalSpout+ with a generic type. The type is known as the _transaction metadata_. It will be needed later to be able to emit batches of tuples from the source.

In our example, +TransactionMetadata+ is defined as:

[source,java]
----
public class TransactionMetadata implements Serializable {
  private static final long serialVersionUID = 1L;

  long from;
  int quantity;

  public TransactionMetadata(long from, int quantity) {
    this.from = from;
    this.quantity = quantity;
  }
}
----

Here we store +from+ and +quantity+, which tell us exactly how to generate the batch of tuples.

Finally we need to implement the following 3 methods:

[source,java]
----
@Override
public ITransactionalSpout.Coordinator<TransactionMetadata> getCoordinator(
Map conf, TopologyContext context) {
  return new TweetsTransactionalSpoutCoordinator();
}

@Override
public backtype.storm.transactional.ITransactionalSpout.Emitter<TransactionMetadata> getEmitter(
Map conf, TopologyContext context) {
  return new TweetsTransactionalSpoutEmitter();
}

@Override
public void declareOutputFields(OutputFieldsDeclarer declarer) {
  declarer.declare(new Fields("txid", "tweet_id", "tweet"));
}
----

In the +getCoordinator+ method we tell Storm which class will coordinate the generation of batches of tuples. +getEmitter+ indicates which class is responsible for reading batches of tuples from the source and emitting them to a stream in the topology.
And finally, as before, we need to declare which fields are emitted.

===== The RQ class

To make the example simpler, we encapsulate all operations communications with Redis in a single class.

[source,java]
----
public class RQ {
  public static final String NEXT_READ = "NEXT_READ";
  public static final String NEXT_WRITE = "NEXT_WRITE";

  Jedis jedis;

  public RQ() {
    jedis = new Jedis("localhost");
  }

  public long getAvailableToRead(long current) {
    return getNextWrite() - current;
  }

  public long getNextRead() {
    String sNextRead = jedis.get(NEXT_READ);
    if(sNextRead == null)
      return 1;
    return Long.valueOf(sNextRead);
  }

  public long getNextWrite() {
    return Long.valueOf(jedis.get(NEXT_WRITE));
  }

  public void close() {
    jedis.disconnect();
  }

  public void setNextRead(long nextRead) {
    jedis.set(NEXT_READ, ""+nextRead);
  }

  public List<String> getMessages(long from, int quantity) {
    String[] keys = new String[quantity];

    for (int i = 0; i < quantity; i++)
      keys[i] = ""+(i+from);

    return jedis.mget(keys);
  }
}
----

Read the implementation of each method carefully and make sure you understand what they do.

===== The Coordinator

Lets see the implementation of the coordinator of our example.

[source,java]
----
public static class TweetsTransactionalSpoutCoordinator implements ITransactionalSpout.Coordinator<TransactionMetadata> {
  TransactionMetadata lastTransactionMetadata;
  RQ rq = new RQ();
  long nextRead = 0;

  public TweetsTransactionalSpoutCoordinator() {
    nextRead = rq.getNextRead();
  }

  @Override
  public TransactionMetadata initializeTransaction(BigInteger txid, TransactionMetadata prevMetadata) {
    long quantity = rq.getAvailableToRead(nextRead);
    quantity = quantity > MAX_TRANSACTION_SIZE ? MAX_TRANSACTION_SIZE : quantity;
    TransactionMetadata ret = new TransactionMetadata(nextRead, (int)quantity);

    nextRead += quantity;
    return ret;
  }

  @Override
  public boolean isReady() {
    return rq.getAvailableToRead(nextRead) > 0;
  }

  @Override
  public void close() {
    rq.close();
  }
}
----

It's important to note that *across the entire topology there will be only one coordinator instance*. 
When the coordinator is instantiated, it retrieves a sequence from Redis that tells the coordinator which is the next tweet to read. The first time, this value will be 1, which means that the next tweet to read is the first one.

The first method called is +isReady+. *It will always be called before +initializeTransaction+*, to make sure the source is ready to be read from. You should return +true+ or +false+ accordingly. In this example we retrieve the number of tweets and compare them with how many tweets we read. The difference between them is the number of tweets available to read. If it is greater than 0, it means we have tweets to read.

Finally the +initializeTransaction+ method is executed. As you can see, we get +txid+ and +prevMetadata+ as parameters. +txid+ is a unique transaction id generated by Storm, which identifies the batch of tuples to be generated. +prevMetadata+ is the metadata generated by the coordinator, from the previous transaction.

In this example, we first check how many tweets are available to read. Then we create a new +TransactionMetadata+, indicating the first tweet to read +from+, and the +quantity+ of tweets to read.

As soon as we return the metadata, Storm stores it, with the +txid+, in Zookeeper. This guarantees that if something goes wrong, Storm will be able to replay this with the emitter, to resend the batch.

===== The Emitter

The last component of a transactional spout is the emitter.

Lets start with the implementation:

[source,java]
----
public static class TweetsTransactionalSpoutEmitter implements ITransactionalSpout.Emitter<TransactionMetadata> {

  RQ rq = new RQ();

  public TweetsTransactionalSpoutEmitter() {
  }

  @Override
  public void emitBatch(TransactionAttempt tx, TransactionMetadata coordinatorMeta, BatchOutputCollector collector) {
    rq.setNextRead(coordinatorMeta.from+coordinatorMeta.quantity);
    List<String> messages = rq.getMessages(coordinatorMeta.from, coordinatorMeta.quantity);

    long tweetId = coordinatorMeta.from;

    for (String message : messages) {
      collector.emit(new Values(tx, ""+tweetId, message));
      tweetId++;
    }
  }

  @Override
  public void cleanupBefore(BigInteger txid) {
  }

  @Override
  public void close() {
    rq.close();
  }
}
----

Emitters read the source and send tuples to a stream. It is very important that emitters always send the same batch of tuples for the same _transaction id_ and _transaction metadata_. That way, if something goes wrong during the processing of a batch, Storm will be able to repeat the same _transaction id_ and _transaction metadata_ with the emitter, and make sure the batch of tuples are repeated. Storm will increase the _attempt id_ in the +TransactionAttempt+ so we know that the batch is repeated.

The most important method is +emitBatch+, which uses the metadata given as a parameter to get tweets from Redis. It increases the sequence in Redis that keeps track of how many tweets we've read so far, and emits the tweets to the topology.

==== The Bolts

First lets see the standard bolts of this topology.

[source,java}
----
public class UserSplitterBolt implements IBasicBolt{
  private static final long serialVersionUID = 1L;

  @Override
  public void declareOutputFields(OutputFieldsDeclarer declarer) {
    declarer.declareStream("users", new Fields("txid", "tweet_id", "user"));
  }

  @Override
  public Map<String, Object> getComponentConfiguration() {
    return null;
  }

  @Override
  public void prepare(Map stormConf, TopologyContext context) {
  }

  @Override
  public void execute(Tuple input, BasicOutputCollector collector) {
    String tweet = input.getStringByField("tweet");
    String tweetId = input.getStringByField("tweet_id");
    StringTokenizer strTok = new StringTokenizer(tweet, " ");
    TransactionAttempt tx = (TransactionAttempt)input.getValueByField("txid");
    HashSet<String> users = new HashSet<String>();

    while(strTok.hasMoreTokens()) {
      String user = strTok.nextToken();

      // Ensure that the current word is a user, and that it's not repeated in this tweet.
      if(user.startsWith("@") && !users.contains(user)) {
        collector.emit("users", new Values(tx, tweetId, user));
        users.add(user);
      }
    }
  }

  @Override
  public void cleanup() {

  }
}
----

As mentioned earlier in this chapter, +UserSplitterBolt+ receives tuples, parses the text of the tweet and emits words preceded by _@_ which are twitter users. +HashtagSplitterBolt+ works in a very similar way.

[source,java]
----
public class HashtagSplitterBolt implements IBasicBolt{

  private static final long serialVersionUID = 1L;

  @Override
  public void declareOutputFields(OutputFieldsDeclarer declarer) {
    declarer.declareStream("hashtags", new Fields("txid", "tweet_id", "hashtag"));
  }

  @Override
  public Map<String, Object> getComponentConfiguration() {
    return null;
  }

  @Override
  public void prepare(Map stormConf, TopologyContext context) {
  }

  @Override
  public void execute(Tuple input, BasicOutputCollector collector) {
    String tweet = input.getStringByField("tweet");
    String tweetId = input.getStringByField("tweet_id");
    StringTokenizer strTok = new StringTokenizer(tweet, " ");
    TransactionAttempt tx = (TransactionAttempt)input.getValueByField("txid");
    HashSet<String> words = new HashSet<String>();

    while(strTok.hasMoreTokens()) {
      String word = strTok.nextToken();

      if(word.startsWith("#") && !words.contains(word)) {
        collector.emit("hashtags", new Values(tx, tweetId, word));
        words.add(word);
      }
    }
  }

  @Override
  public void cleanup() {
  }
}
----

Now, lets see what happens in +UserHashtagJoinBolt+.
The first important thing to notice is that it is a +BaseBatchBolt+. That means the +execute+ method operates on received tuples but doesn't emit any new tuples. When the batch is finished Storm will call the +finishBatch+ method.

[source,java]
----
public void execute(Tuple tuple) {
  String source = tuple.getSourceStreamId();
  String tweetId = tuple.getStringByField("tweet_id");

  if("hashtags".equals(source)) {
    String hashtag = tuple.getStringByField("hashtag");
    add(tweetHashtags, tweetId, hashtag);
  } else if("users".equals(source)) {
    String user = tuple.getStringByField("user");
    add(userTweets, user, tweetId);
  }
}
----

To associate all the hashtags of a tweet with the users mentioned in that tweet, and count how many times they appeared, we need to join the two streams of the previous bolts.
We do that for the entire batch, and once it finishes the +finishBatch+ method is called.

[source,java]
----
@Override
public void finishBatch() {

  for (String user : userTweets.keySet()) {
    Set<String> tweets = getUserTweets(user);
    HashMap<String, Integer> hashtagsCounter = new HashMap<String, Integer>();
    for (String tweet : tweets) {
      Set<String> hashtags = getTweetHashtags(tweet);
      if(hashtags != null) {
        for (String hashtag : hashtags) {
          Integer count = hashtagsCounter.get(hashtag);
          if(count == null)
            count = 0;
          count ++;
          hashtagsCounter.put(hashtag, count);
        }

      }
    }

    for (String hashtag : hashtagsCounter.keySet()) {
      int count = hashtagsCounter.get(hashtag);
      collector.emit(new Values(id, user, hashtag, count));
    }
  }
}
----

In this method we generate and emit a tuple for each user-hashtag and the number of times it occurred.

You can see the complete implementation at link:https://github.com/storm-book/examples-ch08-transactional-topologies[ch08-transactional topologies]

==== The Committer Bolts

Batches of tuples are sent by the coordinator and emitters across the topology, and processed in parallel without any specific order.

The _coordinator bolts_, are special batch bolts that implement +ICommitter+ or have been set with +setCommitterBolt+ in the +TransactionalTopologyBuilder+. The main difference with regular batch bolts is that the +finishBatch+ method of committer bolts is executed when the batch is ready to be committed, and this happens when all previous transactions have been committed successfully. Also +finishBatch+ is executed sequentially. So if a batch with transaction id 1 and a batch with transaction id 2 are being processed in parallel in the topology, the +finishBatch+ method of the committer bolt that is processing the batch with transaction id 2 will get executed only when the +finishBatch+ method of the batch with transaction id 1 has finished without any errors.

The implementation of this class is below:

[source,java]
----
public class RedisCommiterCommiterBolt extends BaseTransactionalBolt implements ICommitter {
  public static final String LAST_COMMITED_TRANSACTION_FIELD = "LAST_COMMIT";
  TransactionAttempt id;
  BatchOutputCollector collector;
  Jedis jedis;


  @Override
  public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt id) {
    this.id = id;
    this.collector = collector;
    this.jedis = new Jedis("localhost");
  }

  HashMap<String, Long> hashtags = new HashMap<String, Long>();
  HashMap<String, Long> users = new HashMap<String, Long>();
  HashMap<String, Long> usersHashtags = new HashMap<String, Long>();

  private void count(HashMap<String, Long> map, String key, int count) {
    Long value = map.get(key);
    if(value == null)
    value = (long) 0;
    value += count;
    map.put(key, value);
  }

  @Override
  public void execute(Tuple tuple) {
    String origin = tuple.getSourceComponent();
    if("users-splitter".equals(origin)) {
      String user = tuple.getStringByField("user");
      count(users, user, 1);
    } else if("hashtag-splitter".equals(origin)) {
      String hashtag = tuple.getStringByField("hashtag");
      count(hashtags, hashtag, 1);
    } else if("user-hashtag-merger".equals(origin)) {
      String hashtag = tuple.getStringByField("hashtag");
      String user = tuple.getStringByField("user");
      String key = user + ":" + hashtag;
      Integer count = tuple.getIntegerByField("count");
      count(usersHashtags, key, count);
    }
  }


  @Override
  public void finishBatch() {
    String lastCommitedTransaction = jedis.get(LAST_COMMITED_TRANSACTION_FIELD);
    String currentTransaction = ""+id.getTransactionId();

    if(currentTransaction.equals(lastCommitedTransaction))
      return ;

    Transaction multi = jedis.multi();

    multi.set(LAST_COMMITED_TRANSACTION_FIELD, currentTransaction);

    Set<String> keys = hashtags.keySet();
    for (String hashtag : keys) {
      Long count = hashtags.get(hashtag);
      multi.hincrBy("hashtags", hashtag, count);  
    }

    keys = users.keySet();
    for (String user : keys) {
      Long count = users.get(user);
      multi.hincrBy("users", user, count);  
    }

    keys = usersHashtags.keySet();
    for (String key : keys) {
      Long count = usersHashtags.get(key);
      multi.hincrBy("users_hashtags", key, count);  
    }

    multi.exec();
  }

  @Override
  public void declareOutputFields(OutputFieldsDeclarer declarer) {
  }
}
----

There is an important detail in the +finishBatch+ method:

[source,java]
----
...
multi.set(LAST_COMMITED_TRANSACTION_FIELD, currentTransaction);
...
----

Here we store the last transaction id committed to the database. Why?
Remember that if a transaction fails, Storm will replay it as many times as necessary. We need to check if we have already processed the transaction to avoid overcounting it. So *remember to store the last transaction id committed and check it before committing again*.

=== Partitioned Transactional Spouts

It is common for a spout to read batches of tuples from a set of partitions.
Continuing from our example, we could have several Redis databases with tweets split across them.
Storm offers some help with partition state management and replay guarantees through the +IPartitionedTransactionalSpout+ interface.

Lets see how we can modify our previous +TweetsTransactionalSpout+ so it can handle partitions.

First we need to extend +BasePartitionedTransactionalSpout+, which implements +IPartitionedTransactionalSpout+.

[source,java]
----
public class TweetsPartitionedTransactionalSpout extends BasePartitionedTransactionalSpout<TransactionMetadata> {
...
}
----

And we need to implement a coordinator.

[source,java]
----
public static class TweetsPartitionedTransactionalCoordinator implements Coordinator {
  @Override
  public int numPartitions() {
    return 4;
  }

  @Override
  public boolean isReady() {
    return true;
  }

  @Override
  public void close() {
  }
}
----

In this case the coordinator is very simple. In the +numPartitions+ method we tell Storm how many partitions we have. Notice that we don't return any metadata.
In an +IPartitionedTransactionalSpout+ metadata is managed by the emitter directly.

Lets see how the emitter is implemented.

[source,java]
----
public static class TweetsPartitionedTransactionalEmitter implements Emitter<TransactionMetadata> {
  PartitionedRQ rq = new PartitionedRQ();

  @Override
  public TransactionMetadata emitPartitionBatchNew(TransactionAttempt tx, BatchOutputCollector collector, int partition, TransactionMetadata lastPartitionMeta) {
    long nextRead;

    if(lastPartitionMeta == null)
      nextRead = rq.getNextRead(partition);
    else {
      nextRead = lastPartitionMeta.from + lastPartitionMeta.quantity;
      rq.setNextRead(partition, nextRead); // Move the cursor
    }

    long quantity = rq.getAvailableToRead(partition, nextRead);
    quantity = quantity > MAX_TRANSACTION_SIZE ? MAX_TRANSACTION_SIZE : quantity;
    TransactionMetadata metadata = new TransactionMetadata(nextRead, (int)quantity);

    emitPartitionBatch(tx, collector, partition, metadata);
    return metadata;
  }

  @Override
  public void emitPartitionBatch(TransactionAttempt tx, BatchOutputCollector collector, int partition, TransactionMetadata partitionMeta) {
    if(partitionMeta.quantity <= 0)
      return ;

    List<String> messages = rq.getMessages(partition, partitionMeta.from, partitionMeta.quantity);
    long tweetId = partitionMeta.from;
    for (String msg : messages) {
      collector.emit(new Values(tx, ""+tweetId, msg));
      tweetId ++;
    }
  }

  @Override
  public void close() {
  }
}
----

There are 2 important methods here, +emitPartitionBatchNew+ and +emitPartitionBatch+.
Storm passes a +partition+ parameter to +emitPartitionBatch+ which tells us which partition we should retrieve the batch of tuples from. In this method we decide which tweets we are going to retrieve, generate the corresponding metadata, call +emitPartitionBatch+ and return the metadata, which will be stored immediately in Zookeeper.

Storm will send the same transaction id for every partition, as the transaction is consistent across all partitions.

We read tweets from the partition in the +emitPartitionBatch+ method, and of course we emit the tuples of the batch to the topology.

If the batch fails, storm will call +emitPartitionBatch+ with the stored metadata to replay the batch.

TIP: You can see the full code at link:https://github.com/storm-book/examples-ch08-transactional-topologies[ch08-transactional topologies]

=== Opaque Transactional Topologies

So far we have assumed that it's always possible to replay a batch of tuples for the same transaction id. But that might not be feasible in some scenarios. What happens then?

It turns out that you can still achieve exactly-once semantics, but it requires more development effort, as you will need to keep previous state in case the transaction is replayed by Storm. Since you can get different tuples for the same transaction id when emitting at different moments in time, you'll need to reset to that previous state and go from there.
For example, you've counted 5 tweets so far, and receive a transaction with id 321 with 8 more. You would then have the values +previousCount=5+, +currentCount=13+ and +lastTransactionId=321=. If you were to then receive a transaction with id 321 with 4 tuples instead of 8, the committer would detect that the transaction id is the same and reset to the +previousCount+ of 5, and add the 4 new tuples to give a +currentCount+ of 9.

Every transaction that is being processed in parallel will be cancelled when a previous transaction in cancelled. This in to ensure that you won't miss anything in the middle.

Your spout should implement +IOpaquePartitionedTransactionalSpout+. As you can see the coordinator and emitters are very simple.

[source,java]
----
public static class TweetsOpaquePartitionedTransactionalSpoutCoordinator implements IOpaquePartitionedTransactionalSpout.Coordinator {
  @Override
  public boolean isReady() {
    return true;
  }
}
----


[source,java]
----
public static class TweetsOpaquePartitionedTransactionalSpoutEmitter implements IOpaquePartitionedTransactionalSpout.Emitter<TransactionMetadata> {
  PartitionedRQ rq = new PartitionedRQ();

  @Override
  public TransactionMetadata emitPartitionBatch(TransactionAttempt tx, BatchOutputCollector collector, int partition, TransactionMetadata lastPartitionMeta) {
    long nextRead;

    if(lastPartitionMeta == null)
      nextRead = rq.getNextRead(partition);
    else {
      nextRead = lastPartitionMeta.from + lastPartitionMeta.quantity;
      rq.setNextRead(partition, nextRead); // Move the cursor
    }

    long quantity = rq.getAvailableToRead(partition, nextRead);
    quantity = quantity > MAX_TRANSACTION_SIZE ? MAX_TRANSACTION_SIZE : quantity;
    TransactionMetadata metadata = new TransactionMetadata(nextRead, (int)quantity);
    emitMessages(tx, collector, partition, metadata);
    return metadata;
  }


  private void emitMessages(TransactionAttempt tx, BatchOutputCollector collector, int partition, TransactionMetadata partitionMeta) {
    if(partitionMeta.quantity <= 0)
      return ;

    List<String> messages = rq.getMessages(partition, partitionMeta.from, partitionMeta.quantity);
    long tweetId = partitionMeta.from;
    for (String msg : messages) {
      collector.emit(new Values(tx, ""+tweetId, msg));
      tweetId ++;
    }
  }

  @Override
  public int numPartitions() {
    return 4;
  }

  @Override
  public void close() {
  }
}
----

The most interesting method is +emitPartitionBatch+ which receives the previous committed metadata. You should use that information to generate a batch of tuples. As mentioned earlier, this batch won't necessarily be the same as it may not be possible to reproduce the same batch.

The rest of the job is handled by the committer bolts that use the previous state as described above.