[[transactional_topologies]]

== Transactional Topologies 

With Storm you can guarantee message processing by using _ack_ and _fail_ strategy, as mentioned earlier in the book. But what happens if tuples are replayed? How do you make sure you won't overcount?

_Transactional Topologies_ is a new feature, included in Storm 0.7.0, that enables exactly once messaging semantics. This way we can replay tuples in a secure way and make sure we process them only once. Without support for transactional topologies we wouldn't be able to count in a fully-accurate, scalable and fault-tolerant way.

NOTE: Transactional Topologies are an abstraction built on top of standard Storm spouts and bolts.

=== The design

In a transactional topology, Storm uses a mix of parallel and sequential tuple processing. The spout generates batches of tuples, that are processed by the bolts in parallel. Some of those bolts are known as _commiters_, and they commit processed batches in a strictly ordered fashion. This means that if we have two batches, with five tuples each, both tuples will be processed in parallel by the bolts, but the commiter bolts won't commit the second tuple, until the first tuple is committed sucessfully.

This can be described as two different steps, or phases:

- The processing phase: a fully parallel phase, many batches are executed at the same time.
- The commit phase: a strongly ordered phase, batch two is not committed until batch one has committed successfully.

We'll call both of these phases a _Storm Transaction_.

NOTE: When dealing with transactional toplogies, it is important to be able to replay batch of tuples from the source, and sometimes even several times. So make sure your source of data, the one that your spout will be connected to, has the ability to do that.

TIP: Storm uses Zookeeper to store transaction metadata. By default the one used for the topology, will be used to store the metadata. You can change this by overriding the configuration key _transactional.zookeeper.servers_ and _transactional.zookeeper.port_.

=== Transactions in Action

To see how transactions work we'll create a tweeter analytics tool. We'll be reading tweets stored in a redis database, process them through a few bolts and finally store in another redis database the list of all hashtags and their frequency among the tweets, the list of all users and amount of tweets they appear in, and finally a list of users with their hashtags and frequency. 

The topology we'll build for this tool is described in <<FIG801>>.

.Topology overview
[[FIG801]]
image::figs/ch08-transactional-topology.jpg[]

As you can see, +TweetsTransactionalSpout+ is the spout that will be connecting to our tweets database and will be emitting batches of tuples across the topology. Two different bolts, +UserSplitterBolt+ and +HashtagSplitterBolt+, will will be receiving tuples from the spout. +UserSplitterBolt+ will parse the tweet and look for users, that is words preceded by _@_ and will emit these words in a custom stream called _users_. The +HashatagSplitterBolt+ will also parse the tweet, looking for words preceded by _#_, and will emit these words in a custom stream called _hashtags_. A third bolt, the +UserHashtagJoinBolt+, will receive both streams and will count how many times a hashtag has appeared in a tweet were a user was named. In order to do counting and emitting the result, this bolt will be a +BaseBatchBolt+ (more on that later).

Finally, a last bolt, called +RedisCommitterBolt+, will receive the three streams, the ones generated by +UserSplitterBolt+, +HashtagSplitterBolt+ and +UserHashtagJoinBolt+. It will count everything and once finished processing the batch of tuples, it will send everything to redis, in one transaction. This bolt is a special kind of bolt, explined later in this chapter, known as a _committer bolt_.

In order to build this topology we use +TransactionalTopologyBuilder+, like the following example:

[source,java]
----
TransactionalTopologyBuilder builder = new TransactionalTopologyBuilder("test", "spout", new TweetsTransactionalSpout());

builder.setBolt("users-splitter", new UserSplitterBolt(), 4).shuffleGrouping("spout");
builder.setBolt("hashtag-splitter", new HashtagSplitterBolt(), 4).shuffleGrouping("spout");

builder.setBolt("user-hashtag-merger", new UserHashtagJoinBolt(), 4)
  .fieldsGrouping("users-splitter","users", new Fields("tweet_id"))
  .fieldsGrouping("hashtag-splitter", "hashtags", new Fields("tweet_id"));

builder.setBolt("redis-commiter", new RedisCommiterCommiterBolt())
  .globalGrouping("users-splitter","users")
  .globalGrouping("hashtag-splitter", "hashtags")
  .globalGrouping("user-hashtag-merger");
----

Lets see how we can implement the spout in a transactional toplogy.

==== The Spout

The spout in a transactional topology is completely different from a standard spout.

[source,java]
----
public class TweetsTransactionalSpout extends BaseTransactionalSpout<TransactionMetadata>{
----

As you can see in the class definition, +TweetsTransactionalSpout+ extends +BaseTransactionalSpout+ with a generic type. The type you set there is something known as the _transaction metadata_. It will be usee later, to be able to emit batches of tuples from the source.

In our example, +TransactionMetadata+ is defined as:

[source,java]
----
public class TransactionMetadata implements Serializable {
  private static final long serialVersionUID = 1L;

  long from;
  int quantity;

  public TransactionMetadata(long from, int quantity) {
    this.from = from;
    this.quantity = quantity;
  }
}
----

Here we store +from+ and +quantity+, which will tell us exactly how to generate the batch of tuples.

To finish the implementation of the spout, we need to implement the following 3 methods:

[source,java]
----
@Override
public ITransactionalSpout.Coordinator<TransactionMetadata> getCoordinator(
Map conf, TopologyContext context) {
  return new TweetsTransactionalSpoutCoordinator();
}

@Override
public backtype.storm.transactional.ITransactionalSpout.Emitter<TransactionMetadata> getEmitter(
Map conf, TopologyContext context) {
  return new TweetsTransactionalSpoutEmitter();
}

@Override
public void declareOutputFields(OutputFieldsDeclarer declarer) {
  declarer.declare(new Fields("txid", "tweet_id", "tweet"));
}
----

In the +getCoordinator+ method we tell storm which class will coordinate the generation of batches of tuples. With +getEmitter+ we tell storm which class will be responsible for reading batches of tuples from the source and emitting them to a stream in the topology.
And finally, as we already did before, we need to declare which fields are emitted.

===== The RQ class

To make the example easier, we've decided to encapsulate all operations with redis in one single class.

[source,java]
----
public class RQ {
  public static final String NEXT_READ = "NEXT_READ";
  public static final String NEXT_WRITE = "NEXT_WRITE";

  Jedis jedis;

  public RQ() {
    jedis = new Jedis("localhost");
  }

  public long getAvailableToRead(long current) {
    return getNextWrite() - current;
  }

  public long getNextRead() {
    String sNextRead = jedis.get(NEXT_READ);
    if(sNextRead == null)
      return 1;
    return Long.valueOf(sNextRead);
  }

  public long getNextWrite() {
    return Long.valueOf(jedis.get(NEXT_WRITE));
  }

  public void close() {
    jedis.disconnect();
  }

  public void setNextRead(long nextRead) {
    jedis.set(NEXT_READ, ""+nextRead);
  }

  public List<String> getMessages(long from, int quantity) {
    String[] keys = new String[quantity];

    for (int i = 0; i < quantity; i++)
      keys[i] = ""+(i+from);

    return jedis.mget(keys);
  }
}
----

Read carefully the implementation of each method, and make sure you understand what they do.

===== The Coordinator

Lets see the implementation of the coordinator of our example.

[source,java]
----
public static class TweetsTransactionalSpoutCoordinator implements ITransactionalSpout.Coordinator<TransactionMetadata> {
  TransactionMetadata lastTransactionMetadata;
  RQ rq = new RQ();
  long nextRead = 0;

  public TweetsTransactionalSpoutCoordinator() {
    nextRead = rq.getNextRead();
  }

  @Override
  public TransactionMetadata initializeTransaction(BigInteger txid, TransactionMetadata prevMetadata) {
    long quantity = rq.getAvailableToRead(nextRead);
    quantity = quantity > MAX_TRANSACTION_SIZE ? MAX_TRANSACTION_SIZE : quantity;
    TransactionMetadata ret = new TransactionMetadata(nextRead, (int)quantity);

    nextRead += quantity;
    return ret;
  }

  @Override
  public boolean isReady() {
    return rq.getAvailableToRead(nextRead) > 0;
  }

  @Override
  public void close() {
    rq.close();
  }
}
----

It is important to mention that *among the entire topology there will be only one coordinator instance*. 
When the coordinator is instanciated, it retrieves from redis a sequence that tell the coordinator which is the next tweet to read. The first time, this value will be 1, which means that the next tweet to read is the first one.

The first method that will be called is +isReady+. *It will always be called before +initializeTransaction+*, to make sure the source is ready to be read from. You should return +true+ or +false+ accordingly. In this example we retrieve the ammount of tweets and compare them with how many tweets we read. The difference between them is the amount to available tweets to read. If it is greater than 0, it means we have tweets to read.

Finally the +initializeTransaction+ is executed. As you can see, we get +txid+ and +prevMetadata+ as parameters. The first one, is a unique transaction id, generated by storm, which identifies the batch of tuples to be generated. +prevMetadata+ is the metadata, generated by the coordinator, of the previous transaction.

In this example, we first make sure how many tweets are available to read. And once we have sorted that out, we create a new +TransactionMetadata+, indicating which is the first tweets to read +from+, and which is the +quantity+ of tweets to read.

As soon as we return the metadata, Storm stores it, with the +txid+, in zookeeper. This guarantee that if something goes wrong, Storm will be able to replay this with the emitter, to resend the batch.

===== The Emitter

The last part when creating a transactional spout, is implementing the emitter.

Lets start with the implementation:

[source,java]
----
public static class TweetsTransactionalSpoutEmitter implements ITransactionalSpout.Emitter<TransactionMetadata> {

  RQ rq = new RQ();

  public TweetsTransactionalSpoutEmitter() {
  }

  @Override
  public void emitBatch(TransactionAttempt tx, TransactionMetadata coordinatorMeta, BatchOutputCollector collector) {
    rq.setNextRead(coordinatorMeta.from+coordinatorMeta.quantity);
    List<String> messages = rq.getMessages(coordinatorMeta.from, coordinatorMeta.quantity);

    long tweetId = coordinatorMeta.from;

    for (String message : messages) {
      collector.emit(new Values(tx, ""+tweetId, message));
      tweetId++;
    }
  }

  @Override
  public void cleanupBefore(BigInteger txid) {
  }

  @Override
  public void close() {
    rq.close();
  }
}
----

Emitters are the one who will be reading the source and sending tuples to a stream. It is very important for the emitters to be able to always send the same batch of tuples for the same _transaction id_ and _transaction metadata_. This way, if something go wrong during the processing of a batch, Storm will be able to repeat the same _transaction id_ and _transaction metadata_ with the emitter, and make sure the batch of tuples are repeated.
Storm will increase the _attempt id_ in the +TransactionAttempt+. This way we know that the batch is repeated.

The important method here is +emitBatch+. In this method we use the metadata, given as a parameter, to get tweets from redis. We also increase the sequence in redis that keeps track of how many tweets we've read so far. And of course we emit the tweets to the topology.

==== The Bolts

First lets see the standard bolts of this topology.

[source,java}
----
public class UserSplitterBolt implements IBasicBolt{
  private static final long serialVersionUID = 1L;

  @Override
  public void declareOutputFields(OutputFieldsDeclarer declarer) {
    declarer.declareStream("users", new Fields("txid", "tweet_id", "user"));
  }

  @Override
  public Map<String, Object> getComponentConfiguration() {
    return null;
  }

  @Override
  public void prepare(Map stormConf, TopologyContext context) {
  }

  @Override
  public void execute(Tuple input, BasicOutputCollector collector) {
    String tweet = input.getStringByField("tweet");
    String tweetId = input.getStringByField("tweet_id");
    StringTokenizer strTok = new StringTokenizer(tweet, " ");
    TransactionAttempt tx = (TransactionAttempt)input.getValueByField("txid");
    HashSet<String> users = new HashSet<String>();

    while(strTok.hasMoreTokens()) {
      String user = strTok.nextToken();

      // Ensure that the current word is a user, and that it's not repeated in this tweet.
      if(user.startsWith("@") && !users.contains(user)) {
        collector.emit("users", new Values(tx, tweetId, user));
        users.add(user);
      }
    }
  }

  @Override
  public void cleanup() {

  }
}
----

As mentioned earlier in this chapter, +UserSplitterBolt+ receives tuples, parses the text of the tweet and emits words preceded by _@_ which are twitter users. +HashtagSplitterBolt+ works in a very similar way.

[source,java]
----
public class HashtagSplitterBolt implements IBasicBolt{

  private static final long serialVersionUID = 1L;

  @Override
  public void declareOutputFields(OutputFieldsDeclarer declarer) {
    declarer.declareStream("hashtags", new Fields("txid", "tweet_id", "hashtag"));
  }

  @Override
  public Map<String, Object> getComponentConfiguration() {
    return null;
  }

  @Override
  public void prepare(Map stormConf, TopologyContext context) {
  }

  @Override
  public void execute(Tuple input, BasicOutputCollector collector) {
    String tweet = input.getStringByField("tweet");
    String tweetId = input.getStringByField("tweet_id");
    StringTokenizer strTok = new StringTokenizer(tweet, " ");
    TransactionAttempt tx = (TransactionAttempt)input.getValueByField("txid");
    HashSet<String> words = new HashSet<String>();

    while(strTok.hasMoreTokens()) {
      String word = strTok.nextToken();

      if(word.startsWith("#") && !words.contains(word)) {
        collector.emit("hashtags", new Values(tx, tweetId, word));
        words.add(word);
      }
    }
  }

  @Override
  public void cleanup() {
  }
}
----

Now, lets see what happens in +UserHashtagJoinBolt+.
The first important thing to notice, is that it is a +BaseBatchBolt+. This means that the +execute+ method will be operating on the received tuples, but won't be emitting any new tuple. Eventually, when the batch is finished, storm will call the +finishBatch+ method.

[source,java]
----
public void execute(Tuple tuple) {
  String source = tuple.getSourceStreamId();
  String tweetId = tuple.getStringByField("tweet_id");

  if("hashtags".equals(source)) {
    String hashtag = tuple.getStringByField("hashtag");
    add(tweetHashtags, tweetId, hashtag);
  } else if("users".equals(source)) {
    String user = tuple.getStringByField("user");
    add(userTweets, user, tweetId);
  }
}
----

Since we need to associate all the hashtags of a tweet, with the users mentioned in that tweet, and count how many times they appeared, we need to join the two streams of the previous bolts.
We do that for the entire batch, and once it finishes, the +finishBatch+ method is called.

[source,java]
----
@Override
public void finishBatch() {

  for (String user : userTweets.keySet()) {
    Set<String> tweets = getUserTweets(user);
    HashMap<String, Integer> hashtagsCounter = new HashMap<String, Integer>();
    for (String tweet : tweets) {
      Set<String> hashtags = getTweetHashtags(tweet);
      if(hashtags != null) {
        for (String hashtag : hashtags) {
          Integer count = hashtagsCounter.get(hashtag);
          if(count == null)
            count = 0;
          count ++;
          hashtagsCounter.put(hashtag, count);
        }

      }
    }

    for (String hashtag : hashtagsCounter.keySet()) {
      int count = hashtagsCounter.get(hashtag);
      collector.emit(new Values(id, user, hashtag, count));
    }
  }
}
----

In this method we generate and emit a tuple for each user-hashtag, and the amount of times it occured.

You can see the complete implementation at link:https://github.com/storm-book/examples-ch08-transactional-topologies[ch08-transactional topologies]

==== The Commiter Bolts

As we learned until now, batches of tuples are sent by the coordinator and emitters across the topology. Those batched of tuples are processed in parallel, without any specific order.

The _coordinator bolts_, are special batch bolts that implement +ICommitter+ or have been set with +setCommiterBolt+ in the +TransactionalTopologyBuilder+. The main difference with regular batch bolts, is that the +finishBatch+ method of commiter bolts is executed when the batch is ready to be committed, and this happens when all previous transactions have been committed successfully. Also +finishBatch+ method is executed sequentially. So if batch with transaction id 1 and batch with transaction id 2 are being processed in parallel in the topology, the +finishBatch+ method of the committer bolt that is processing batch with transaction id 2 will get executed only when the +finishBatch+ of batch with transaction id 1 has finished and without any errors.

The implementation of this class is like the following code:

[source,java]
----
public class RedisCommiterCommiterBolt extends BaseTransactionalBolt implements ICommitter {
  public static final String LAST_COMMITED_TRANSACTION_FIELD = "LAST_COMMIT";
  TransactionAttempt id;
  BatchOutputCollector collector;
  Jedis jedis;


  @Override
  public void prepare(Map conf, TopologyContext context, BatchOutputCollector collector, TransactionAttempt id) {
    this.id = id;
    this.collector = collector;
    this.jedis = new Jedis("localhost");
  }

  HashMap<String, Long> hashtags = new HashMap<String, Long>();
  HashMap<String, Long> users = new HashMap<String, Long>();
  HashMap<String, Long> usersHashtags = new HashMap<String, Long>();

  private void count(HashMap<String, Long> map, String key, int count) {
    Long value = map.get(key);
    if(value == null)
    value = (long) 0;
    value += count;
    map.put(key, value);
  }

  @Override
  public void execute(Tuple tuple) {
    String origin = tuple.getSourceComponent();
    if("users-splitter".equals(origin)) {
      String user = tuple.getStringByField("user");
      count(users, user, 1);
    } else if("hashtag-splitter".equals(origin)) {
      String hashtag = tuple.getStringByField("hashtag");
      count(hashtags, hashtag, 1);
    } else if("user-hashtag-merger".equals(origin)) {
      String hashtag = tuple.getStringByField("hashtag");
      String user = tuple.getStringByField("user");
      String key = user + ":" + hashtag;
      Integer count = tuple.getIntegerByField("count");
      count(usersHashtags, key, count);
    }
  }


  @Override
  public void finishBatch() {
    String lastCommitedTransaction = jedis.get(LAST_COMMITED_TRANSACTION_FIELD);
    String currentTransaction = ""+id.getTransactionId();

    if(currentTransaction.equals(lastCommitedTransaction))
      return ;

    Transaction multi = jedis.multi();

    multi.set(LAST_COMMITED_TRANSACTION_FIELD, currentTransaction);

    Set<String> keys = hashtags.keySet();
    for (String hashtag : keys) {
      Long count = hashtags.get(hashtag);
      multi.hincrBy("hashtags", hashtag, count);  
    }

    keys = users.keySet();
    for (String user : keys) {
      Long count = users.get(user);
      multi.hincrBy("users", user, count);  
    }

    keys = usersHashtags.keySet();
    for (String key : keys) {
      Long count = usersHashtags.get(key);
      multi.hincrBy("users_hashtags", key, count);  
    }

    multi.exec();
  }

  @Override
  public void declareOutputFields(OutputFieldsDeclarer declarer) {
  }
}
----

This is all very straightforward. But there is a very important detail in the +finishBatch+ method.

[source,java]
----
...
multi.set(LAST_COMMITED_TRANSACTION_FIELD, currentTransaction);
...
----

Here we are storing in our database the last transaction id commited. Why we do that?
Remember that if a transaction fails, Storm will be replaying it as many times as necessary. If we don't make sure that we already processed the transaction, we could overcount, and the whole idea of a transactional topology would be useless.
So *remember storing the las transaction id committed and checking against it before committing*.

=== Partitioned Transactional Spouts

It is very common for a spout to read batches of tuples from a set of partitions.
Continuing our example, we could have several redis databases and the tweets could be splitted across those redis databases.
Storm offer some facilities, by implementing +IPartitionedTransactionalSpout+, to manage the state for every partition and guarantee replayability.

Lets see how we modify our previous +TweetsTransactionalSpout+ so it can handle partitions.

We first extends +BasePartitionedTransactionalSpout+, which implements +IPartitionedTransactionalSpout+.

[source,java]
----
public class TweetsPartitionedTransactionalSpout extends BasePartitionedTransactionalSpout<TransactionMetadata> {
...
}
----

We need to tell storm which is out coordinator.

[source,java]
----
public static class TweetsPartitionedTransactionalCoordinator implements Coordinator {
  @Override
  public int numPartitions() {
    return 4;
  }

  @Override
  public boolean isReady() {
    return true;
  }

  @Override
  public void close() {
  }
}
----

In this case the coordinator is very simple. In the +numPartitions+ method we tell storm how many partitions we have. And also notice that we don't return any metadata.
In an +IPartitionedTransactionalSpout+ the metadata is managed by the emitter directly.

Lets see now how the emitter is implemented.

[source,java]
----
public static class TweetsPartitionedTransactionalEmitter implements Emitter<TransactionMetadata> {
  PartitionedRQ rq = new PartitionedRQ();

  @Override
  public TransactionMetadata emitPartitionBatchNew(TransactionAttempt tx, BatchOutputCollector collector, int partition, TransactionMetadata lastPartitionMeta) {
    long nextRead;

    if(lastPartitionMeta == null)
      nextRead = rq.getNextRead(partition);
    else {
      nextRead = lastPartitionMeta.from + lastPartitionMeta.quantity;
      rq.setNextRead(partition, nextRead); // Move the cursor
    }

    long quantity = rq.getAvailableToRead(partition, nextRead);
    quantity = quantity > MAX_TRANSACTION_SIZE ? MAX_TRANSACTION_SIZE : quantity;
    TransactionMetadata metadata = new TransactionMetadata(nextRead, (int)quantity);

    emitPartitionBatch(tx, collector, partition, metadata);
    return metadata;
  }

  @Override
  public void emitPartitionBatch(TransactionAttempt tx, BatchOutputCollector collector, int partition, TransactionMetadata partitionMeta) {
    if(partitionMeta.quantity <= 0)
      return ;

    List<String> messages = rq.getMessages(partition, partitionMeta.from, partitionMeta.quantity);
    long tweetId = partitionMeta.from;
    for (String msg : messages) {
      collector.emit(new Values(tx, ""+tweetId, msg));
      tweetId ++;
    }
  }

  @Override
  public void close() {
  }
}
----

There are 2 important methods here, +emitPartitionBatchNew+ and +emitPartitionBatch+.
In +emitPartitionBatch+ we receive from storm the +partition+ parameter, which tells us which partition we should retrieve the batch of tuples from. In this method we decide which tweets we are going to retrieve, we generate the corresponding metadata, call +emitPartitionBatch+ and return the metadata, which will be stored immediately in zookeeper.

Storm will send the same transaction id for every partition, as the transaction is across all the partitions.

We read from the partition the tweets in the +emitPartitionBatch+ method, and of course we emit the tuples of the batch to the topology.

If the batch fails, storm will call +emitPartitionBatch+ with the stored metadata to replay the batch.

TIP: You can check the code at link:https://github.com/storm-book/examples-ch08-transactional-topologies[ch08-transactional topologies]

=== Opaque Transactional Topologies

So far we have assumed that it's always possible to replay a batch of tuples for the same transaction id. But that might not be feasible in some scenarios. What happens then?

It turns out that you can still achieve exactly-once semantics, but it requires some more development effort, as you will need to keep previous state in case the transaction is replayed by storm. Since you can get different tuples for the same transaction id, when emitting in different moments in time, you'll need to reset to that previous state and go from there.
So for example, if you are counting total received tweets, you have currently counted 5 and in the last transaction, with id 321, you count 8 more. You would keep those 3 values, +previousCount=5+, +currentCount=13+ and +lastTransactionId=321=. In case transaction id 321 is emitted again and since you get different tuples, you count 4 more instead of 8, the commiter will detect that is the same transaction id, it would reset to the +previousCount+ of 5, and will add those new 4 and update +currentCount+ to 9.

Also, every transaction that is being processed in parallel will be cancelled when a previous transaction in cancelled. This in to ensure that you won't miss anything in the middle.

Your spout should implement +IOpaquePartitionedTransactionalSpout+ and as you can see the coordinator and emitters are very simple.

[source,java]
----
public static class TweetsOpaquePartitionedTransactionalSpoutCoordinator implements IOpaquePartitionedTransactionalSpout.Coordinator {
  @Override
  public boolean isReady() {
    return true;
  }
}
----


[source,java]
----
public static class TweetsOpaquePartitionedTransactionalSpoutEmitter implements IOpaquePartitionedTransactionalSpout.Emitter<TransactionMetadata> {
  PartitionedRQ rq = new PartitionedRQ();

  @Override
  public TransactionMetadata emitPartitionBatch(TransactionAttempt tx, BatchOutputCollector collector, int partition, TransactionMetadata lastPartitionMeta) {
    long nextRead;

    if(lastPartitionMeta == null)
      nextRead = rq.getNextRead(partition);
    else {
      nextRead = lastPartitionMeta.from + lastPartitionMeta.quantity;
      rq.setNextRead(partition, nextRead); // Move the cursor
    }

    long quantity = rq.getAvailableToRead(partition, nextRead);
    quantity = quantity > MAX_TRANSACTION_SIZE ? MAX_TRANSACTION_SIZE : quantity;
    TransactionMetadata metadata = new TransactionMetadata(nextRead, (int)quantity);
    emitMessages(tx, collector, partition, metadata);
    return metadata;
  }


  private void emitMessages(TransactionAttempt tx, BatchOutputCollector collector, int partition, TransactionMetadata partitionMeta) {
    if(partitionMeta.quantity <= 0)
      return ;

    List<String> messages = rq.getMessages(partition, partitionMeta.from, partitionMeta.quantity);
    long tweetId = partitionMeta.from;
    for (String msg : messages) {
      collector.emit(new Values(tx, ""+tweetId, msg));
      tweetId ++;
    }
  }

  @Override
  public int numPartitions() {
    return 4;
  }

  @Override
  public void close() {
  }
}
----

The most interesting method is +emitPartitionBatch+ which receives the previous committed metadata. You should use that information to generate a batch of tuples. This batch won't be necessarily the same, like we said earlier that youmight not be able to reproduce the same batch.

The rest of the job is handled by the commiter bolts, that would use the previous state as mentioned earlier, as mentioned earlier.
