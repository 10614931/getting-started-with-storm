[[spouts]]
== Spouts

In this chapter we’ll take a look at the most commonly used strategies for designing the entry point for a topology (a spout) and at how to make spouts fault-tolerant.

=== Reliable vs Unreliable messages

When designing a topology, one important thing to keep in mind is message reliability. If a message can't be processed we need to decide what to do with the individual message and what to do with the topology as a whole. For example when processing bank deposits it is important not to lose a single transaction message. But if we're processing millions of tweets looking for some statistical metric, and one tweet gets lost, we can assume that the metric will be still be fairly accurate.

In storm it is the author's responsibility to guarantee message reliability, according to the needs of each topology. This involves a trade-off. A reliable topology must manage lost messages, which requires more resources. A less reliable topology may lose some messages but is less resource intensive.

To manage reliability at the spout, we can include a message id with the tuple at _emit_ time (*collector.emit(new Values(…),tupleId)*). The methods *ack* and *fail* are called when a tuple is processed correctly or fails respectively. Tuple processing succeeds when the tuple is processed by all target bolts and all anchored bolts (we will see how to anchor a bolt to a tuple in the chapter <<bolts,Bolts>>). 
Tuple processing fails when:

* *collector.fail(tuple)* is called by the target spout.
* processing time exceeds the configured timeout.
* the number of tuples emitted without receiving an ack or fail exceeds the configured limit.

Let's take a look at an example. Imagine we are processing bank transactions, and we have the following requirements:

* If a transaction fails, re-send the message
* If the transaction fails too many times, terminate the topology

We'll create a spout that sends 100 random transaction ids, and a bolt that fails for 80% of tuples received (you can find the complete example at link:https://github.com/storm-book/examples-ch04-spouts/[ch04-spout examples]). We'll implement the spout using a _Map_ to emit transaction message tuples so that it's easy to re-send messages.

[source, java]
----
public void nextTuple() {
    if(!toSend.isEmpty()){
        for(Map.Entry<Integer, String> transactionEntry : toSend.entrySet()){
            Integer transactionId = transactionEntry.getKey();
            String transactionMessage = transactionEntry.getValue();
            collector.emit(new Values(transactionMessage),transactionId);
        }
        toSend.clear();
    }
    try {
        Thread.sleep(1);
    } catch (InterruptedException e) {}
}
----

If there are messages waiting to be sent, get each transaction message and its associated id and emit them as a tuple, then clear the message queue. Note that it's safe to call *clear* on the map, because *nextTuple*, *fail* and *ack* are the only methods that modify the map and they all run in the same Thread.

We maintain two maps to keep track of transaction messages waiting to be sent, and the number of times each transaction has failed. The *ack* method simply removes the transaction message from each list.

[source, java]
----
public void ack(Object msgId) {
    messages.remove(msgId);
    failCounterMessages.remove(msgId);
}
----

The *fail* method decides whether to re-send a transaction message or terminate the topology if it has failed too many times.

[source, java]
----
public void fail(Object msgId) {
    Integer transactionId = (Integer) msgId;
    // Check the number of times the transaction has failed
    Integer failures = transactionFailureCount.get(transactionId) + 1;
    
    if(fails >= MAX_FAILS){
        // If the number of failures is too high, terminate the topology
        throw new RuntimeException("Error, transaction id ["+transactionId+"] has had too many errors ["+failures+"]");
    }
    
    // If the number of failures is less than the maximum, save the number and re-send the message 
    transactionFailureCount.put(transactionId, failures);
    toSend.put(transactionId,messages.get(transactionId));
    LOG.info("Re-sending message ["+msgId+"]");
}
----

First we check the number of times the transaction has failed. If a transaction fails too many times we throw a _RuntimeException_ to terminate the topology. Otherwise we save the failure count and put the transaction message in the *toSend* queue so that it will be re-sent when *nextTuple* is called.

CAUTION: Storm nodes do not maintain state, so if you store information in memory (as in this example) and the node goes down you will lose all stored information.

TIP: Storm is a fast-fail system. If an exception is thrown the topology will go down, but Storm will restart the process in a consistent state so that it can recover correctly.

=== Getting data

Here we'll take a look at some common techniques for designing spouts that collect data efficiently from multiple sources.


==== Direct messages

In a direct message architecture, the spout connects directly to a message emitter.

image::figs/ch04-direct-message.jpg[Direct message spout]

This architecture is simple to implement, particularly when the message emitter is a well known device or a well known device group. A well known device is one which is known at startup, and remains the same throughout the life of the topology. An unknown device is one which is added after the topology is already running. A well know device group is one in which all devices in the group are known at start time.

As an example, we'll create a spout to read the Twitter stream using the link:https://dev.twitter.com/docs/streaming-api[Twitter streaming API]. The spout will connect directly to the API, which serves as the message emitter. We'll filter the stream to get all public tweets that match the _track_ parameter (as documented on the Twitter dev page). The complete example can be found at link:https://github.com/storm-book/examples-ch04-spouts/[Twitter Example].

The spout gets the connection parameters from the configuration object (track, user and password) and creates a connection to the API (in this case using the _link:http://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/impl/client/DefaultHttpClient.html[DefaultHttpClient]_ from link:http://apache.org/[Apache]). It reads the connection one line at a time, parses the line from JSON format into a Java object and emits it.

[source,java]
----
public void nextTuple() {
        //Create the client to the known source
        client = new DefaultHttpClient();
        client.setCredentialsProvider(credentialProvider);
        HttpGet get = new HttpGet(STREAMING_API_URL+track);     
        
        ....

                //Execute get and create reader for streammin api
                BufferedReader reader = ... 
                String in;
                //Read line by line
                while((in = reader.readLine())!=null){
                    try{
                        //Parse and emit
                        Object json = jsonParser.parse(in);
                        collector.emit(new Values(track,json));
                    }catch (ParseException e) {
                        LOG.error("Error parsing message from twitter",e);
                    }
                }

        ....
    }
----

TIP: here we are locking the the *nextTuple* method so we never execute the *ack* and *fail* methods, in a real application is recommendable do the locking into a separately thread and use an internal queue to exchange information (we'll how to do that in the next example <<enqueued_messages, Enqueued Messages>>)

This is great!!!

We're reading the Twitter stream with a single spout. If we parallelize the topology we'll have several spouts reading the same stream, which doesn't make sense. So how do we parallelize processing if we have several streams to read? One interesting feature of Storm is that we can access the *TopologyContext* from any node, which means that we can divide the streams between our spout instances.

[source, java]
----
    public void open(Map conf, TopologyContext context,
            SpoutOutputCollector collector) {

       //Get the spout size from the context
        int spoutsSize = context.getComponentTasks(context.getThisComponentId()).size();

        //Get the id of this spout
        int myIdx = context.getThisTaskIndex();

        String[] tracks = ((String) conf.get("track")).split(",");
        StringBuffer tracksBuffer = new StringBuffer();
        for(int i=0; i< tracks.length;i++){

            //Check if this spout must read the track word
            if( i % spoutsSize == myIdx){
                tracksBuffer.append(",");
                tracksBuffer.append(tracks[i]);
            }
        }
        if(tracksBuffer.length() == 0)
            throw new RuntimeException("No track found for spout" +
                    " [spoutsSize:"+spoutsSize+", tracks:"+tracks.length+"] the amount" +
                    " of tracks must be more then the spout paralellism");
        this.track =tracksBuffer.substring(1).toString();

        ....

   }
----

Using this technique we can distribute collectors evenly across data sources. The same technique can be applied in other situations, for example for collecting log files from web servers.


image::figs/ch04-directconnection-hashing.jpg[Direct connection hashing]


In the example above we connected the spout to a well known device. We can use the same approach to connect to unknown devices using a coordinating system to maintain the device list. The coordinator detects changes to the list and creates and destroys connections. For example when collecting log files from web servers, the list of web servers may change over time. When a web server is added the coordinator detects the change and creates a new spout for it.

image::figs/ch04-directconnection-coordinator.jpg[Direct connection coordinator]


TIP: It's recommended to create connections from spouts to message emitters rather than the other way around. If the machine on which a spout is running goes down, Storm will restart it on another machine, so it's easier for the spout to locate the message emitter than for the message emitter to keep track of which machine the spout is on.


[[enqueued_messages]]
====  Enqueued messages

The second approach it's connect our spouts to a _queue_ system which will receive the messages from the messages emiters and will leave the messages availables to consume by the spouts. The advantage of use a _queue_ system is that we'll use it as middleware to comunicate the spouts with the data source, then we don't need to know anything about the message emiters and the process of add and remove emiters is easier than the direct connection. The problem with this architecture is that the queue will be our point of failure and we'll be adding a new layer to our processing flow.

Here we have the architecture schema:

image::figs/ch04-queueconnection.jpg[]

TIP: we can use round-robin pull or hashing queues (divide the queue messages by hash to send it to the spouts or create many queues) to parallelize the processing through queues dividing the messages between many spouts

We'll create an example using (link:http://redis.io/[redis]) as queue system and their java library link:https://github.com/xetorthio/jedis[Jedis]
In our example we'll create a log processor to collect logs from unknonw source using the command lpush to insert messages into the queue and blpop that enable us wait for a message, if we have many processes doing blpop these receive the messages in round-robin fashion.

To hear the messages from redis we'll use a thread created at the *open* spout, (using a thread we'll not lock the main loop where the *nextTuple* method is):

[source, java]
----
    new Thread(new Runnable() {
            @Override
            public void run() {
                while(true){
                    try{
                        Jedis client = new Jedis(redisHost, redisPort);
                        List<String> res = client.blpop(Integer.MAX_VALUE, queues);
                        messages.offer(res.get(1));
                    }catch(Exception e){
                        LOG.error("Error reading queues from redis",e);
                        try {
                            Thread.sleep(100);
                        } catch (InterruptedException e1) {}
                    }
                }
                
            }
     }).start()
----
The only purpose of this thread is create the connection and execute the blpop command, when a message is received this is added to an internal queue of _messages_ that will be consumed by the *nextTuple* method. Here we can see how the source is the redis queue and we don't know which is the message emiter neither the quantity of this

TIP: Is not recommendable create many threads by spout because each spout runs in a different thread, instead of create many threads is recommendable increase the parallelism, this will create more thread in a distributed fashion through the storm cluster

In our *nextTuple* method the only thing that we'll do is receive the messages and emmit

[source, java]
----
    public void nextTuple() {
        while(!messages.isEmpty()){
            collector.emit(new Values(messages.poll()));
        }
        try {
            Thread.sleep(1);
        } catch (InterruptedException e) {}
    } 
----



==== DRPC

DRPCSpout is an spout implementation to receive a function invocation stream from the DRPC server, and process it (we have an example into the chapter <<topologies, Topologies>>). In the most common casses use the _link:http://nathanmarz.github.com/storm/doc/backtype/storm/drpc/DRPCSpout.html[backtype.storm.drpc.DRPCSpout]_ will be enough but is possible to create our own implementation using the DRPC classes included with the storm package

=== Conclusions

We have seen the common spouts implementation patterns, their advantages and how to do the messages reliables. It's important define the
 spout communication based on the problem that we are working on, really there aren't a unique method to solve all topologies, if we know the sources or we can control these sources, it's recommendable to use the direct connection while if we need the capacity to add unknown sources or receive messages from variety sources will be recommendable to use queued connection, however if we need an on-line process we will need to use DRPCSpouts or implement something like that. Although we have seen 3 main types of connections there are an infinity ways to do it and will depends on our needs

