[[spouts]]
== Spouts

In this chapter we’ll take a look at the most commonly used strategies for designing the entry point for a topology (a spout) and at how to make spouts fault-tolerant.

=== Reliable vs Unreliable messages

When designing a topology, one important thing to keep in mind is message reliability. If a message can't be processed we need to decide what to do with the individual message and what to do with the topology as a whole. For example when processing bank deposits it is important not to lose a single transaction message. But if we're processing millions of tweets looking for some statistical metric, and one tweet gets lost, we can assume that the metric will be still be fairly accurate.

In storm it is the author's responsibility to guarantee message reliability, according to the needs of each topology. This involves a trade-off. A reliable topology must manage lost messages, which requires more resources. A less reliable topology may lose some messages but is less resource intensive.

To manage reliability at the spout, we can include a message id with the tuple at _emit_ time (*collector.emit(new Values(…),tupleId)*). The methods *ack* and *fail* are called when a tuple is processed correctly or fails respectively. Tuple processing succeeds when the tuple is processed by all target bolts and all anchored bolts (we will see how to anchor a bolt to a tuple in the chapter <<bolts,Bolts>>). Tuple processing fails when
. *collector.fail(tuple)* is called by the target spout.
. processing time exceeds the configured timeout.
. the number of tuples emitted without receiving an ack or fail exceeds the configured limit.

Let's take a look at an example. Imagine we are processing bank transactions, and we have the following requirements:
. If a transaction fails, re-send the message
. If the transaction fails too many times, terminate the topology
We'll create a spout that sends 100 random transaction ids, and a bolt that fails for 80% of tuples received (you can find the complete example at link:https://github.com/storm-book/examples-ch04-spouts/[ch04-spout examples]). We'll implement the spout using a _Map_ to emit transaction message tuples so that it's easy to re-send messages.

[source, java]
----
public void nextTuple() {
    if(!toSend.isEmpty()){
        for(Map.Entry<Integer, String> transactionEntry : toSend.entrySet()){
            Integer transactionId = transactionEntry.getKey();
            String transactionMessage = transactionEntry.getValue();
            collector.emit(new Values(transactionMessage),transactionId);
        }
        toSend.clear();
    }
    try {
        Thread.sleep(1);
    } catch (InterruptedException e) {}
}
----

If there are messages waiting to be sent, get each transaction message and its associated id and emit them as a tuple, then clear the message queue. Note that it's safe to call *clear* on the map, because *nextTuple*, *fail* and *ack* are the only methods that modify the map and they all run in the same Thread.

We maintain two maps to keep track of transaction messages waiting to be sent, and the number of times each transaction has failed. The *ack* method simply removes the transaction message from each list.

[source, java]
----
public void ack(Object msgId) {
    messages.remove(msgId);
    failCounterMessages.remove(msgId);
}
----

The *fail* method decides whether to re-send a transaction message or terminate the topology if it has failed too many times.

[source, java]
----
public void fail(Object msgId) {
    Integer transactionId = (Integer) msgId;
    // Check the number of times the transaction has failed
    Integer fails = failCounterMessages.get(transactionId) + 1;
    
    if(fails >= MAX_FAILS){
        // If the number of failures is too high, terminate the topology
        throw new RuntimeException("Error, transaction id ["+transactionId+"] has had too many errors ["+fails+"]");
    }
    
    // If the number of failures is less than the maximum, save the number and re-send the message 
    failCounterMessages.put(transactionId, fails);
    toSend.put(transactionId,messages.get(transactionId));
    LOG.info("Re-sending message ["+msgId+"]");
}
----

First we check the number of times the transaction has failed. If a transaction fails too many times we throw a _RuntimeException_ to terminate the topology. Otherwise we save the failure count and put the transaction message in the *toSend* queue so that it will be re-sent when *nextTuple* is called.

CAUTION: Storm nodes do not maintain state, so if you store information in memory (as in this example) and the node goes down you will lose all stored information.

TIP: Storm is a fast-fail system. If an exception is thrown the topology will go down, but Storm will restart the process in a consistent state so that it can recover correctly.

=== Getting data

Here we'll take a look at some common techniques for designing spouts that collect data efficiently from multiple sources.


==== Direct messages

In a direct message architecture, the spout connects directly to a message emitter.

image:figs/ch04-direct-message.jpg[Direct message spout]

This architecture is simple to implement, particularly when the message emitter is a well known device or a well known device group. A well known device is one which is known at startup, and remains the same throughout the life of the topology. An unknown device is one which is added after the topology is already running. A well know device group is one in which all devices in the group are known at start time.

As an example, we'll create a spout to read the Twitter stream using the link:https://dev.twitter.com/docs/streaming-api[Twitter streaming API]. The spout will connect directly to the API, which serves as the message emitter. We'll filter the stream to get all public tweets that match the _track_ parameter (as documented on the Twitter dev page). The complete example can be found at link:https://github.com/storm-book/examples-ch04-spouts/[Twitter Example].

The spout gets the connection parameters from the configuration object (track, user and password) and creates a connection to the API (in this case using the _link:http://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/impl/client/DefaultHttpClient.html[DefaultHttpClient]_ from link:http://apache.org/[Apache]). It reads the connection one line at a time, parses the line from JSON format into a Java object and emits it.

[source,java]
----
public void nextTuple() {
        //Create the client
        client = new DefaultHttpClient();
        client.setCredentialsProvider(credentialProvider);
        HttpGet get = new HttpGet(STREAMING_API_URL+track);     
        HttpResponse response;
        try {
            //Execute the get
            response = client.execute(get);
            StatusLine status = response.getStatusLine();
            if(status.getStatusCode() == 200){
                InputStream inputStream = response.getEntity().getContent();
                BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream));
                String in;
                //Read line by line
                while((in = reader.readLine())!=null){
                    try{
                        //Parse and emit
                        Object json = jsonParser.parse(in);
                        collector.emit(new Values(track,json));
                    }catch (ParseException e) {
                        LOG.error("Error parsing message from twitter",e);
                    }
                }
            }
        } catch (IOException e) {
            LOG.error("Error in communication with Twitter API ["+get.getURI().toString()+"]");
            try {
                //Sleep before re-trying
                Thread.sleep(10000);
            } catch (InterruptedException e1) {}
        } 
    }
----

This is great!!!

We're reading the Twitter stream with a single spout. If we parallelize the topology we'll have several spouts reading the same stream, which doesn't make sense. So how do we parallelize processing if we have several streams to read? One interesting feature of Storm is that we can access the *TopologyContext* from any node, which means that we can divide the streams between our spout instances.

[source, java]
----
    public void open(Map conf, TopologyContext context,
            SpoutOutputCollector collector) {

        //Get the spout size from the context
        int spoutsSize = context.getComponentTasks(context.getThisComponentId()).size();

        //Get the id of this spout
        int myIdx = context.getThisTaskIndex();

        String[] tracks = ((String) conf.get("track")).split(",");
        StringBuffer tracksBuffer = new StringBuffer();
        for(int i=0; i< tracks.length;i++){

            //Check if this spout must read the track word
            if( i % spoutsSize == myIdx){
                tracksBuffer.append(",");
                tracksBuffer.append(tracks[i]);
            }
        }
        if(tracksBuffer.length() == 0)
            throw new RuntimeException("No track found for spout" +
                    " [spoutsSize:"+spoutsSize+", tracks:"+tracks.length+"] the amount" +
                    " of tracks must be more then the spout paralellism");
        this.track =tracksBuffer.substring(1).toString();

        .........
   }
----

Using this technique we can distribute collectors evenly across data sources. The same technique can be applied in other situations, for example for collecting log files from web servers.

image:figs/ch04-directconnection-hashing.jpg[]

In the example above we connected the spout to a well known device. We can use the same approach to connect to unknown devices using a coordinating system to maintain the device list. The coordinator detects changes to the list and creates and destroys connections. For example when collecting log files from web servers, the list of web servers may change over time. When a web server is added the coordinator detects the change and creates a new spout for it.

image:figs/ch04-directconnection-coordinator.jpg[]

TIP: It's recommended to create connections from spouts to message emitters rather than the other way around. If the machine on which a spout is running goes down, Storm will restart it on another machine, so it's easier for the spout to locate the message emitter than for the message emitter to keep track of which machine the spout is on.


====  Enqueued messages

text

==== DRPC

=== Conclusions
