[[bolts]]
== Bolts

As we learned along the book Bolts are key components in a Storm cluster.
In this chapter weâ€™ll learn how to implement bolts, their lifecycle and different strategies for designing them.

=== Bolts Lifecycle

A Bolt is a component that takes tuples as input and produces tuples as output. Usually you will implement the +IRichBolt+ interface when writing you own Bolt.
When it comes to the Bolt lifecycle you have to be very carefull and realize that Bolts will be created on the client machine, serialized into the topology and submitted to the master machine of the cluster. At this stage the cluster will launch workers that will deserialize the Bolt and call +prepare+ on it and then start processing tuples.

In case you want to customize a Bolt you should set parameter's through its constructor and save them as instance variable so they will get serialized when submitting the bolt to the cluster.

=== Bolt Structure

A Bolt has the following basic structure.

* +cleanup()+: Called when a bolt is going to shutdown.
* +execute(Tuple input)+: Process a single tuple of input.
* +prepare(java.util.Map stormConf, TopologyContext context, OutputCollector collector)+: Called just before this bolt will start processing tuples.
* +declareOutputFields(OutputFieldsDeclarer declarer)+: Declare the output schema for this bolt.

Lets see an example of a Bolt that will split words of sentences.

[source, java]
----
class SplitSentence implements IRichBolt {
    private OutputCollector collector;
    
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        this.collector = collector;
    }

    public void execute(Tuple tuple) {
        String sentence = tuple.getString(0);
        for(String word: sentence.split(" ")) {
            collector.emit(new Values(word));
        }
    }

    public void cleanup() {
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }        
}
----

As you can see this is very straightforward and almost doesn't need further explanation. It is worth to mention that in this example there is no message guaranteeing. This means that if the bolt discard a message for some reason, either because it goes down or just simply because you decided to discard it programatically, the spout that generated this message will never get notify, neither all the bolts between that spout and this bolt.

In many cases you'll want to guarantee message processing through the entire Toplogy. 

=== Reliable vs Unreliable bolts

As we said before Storm guarantees that each message sent by a spout will be fully processed by the bolts. But this is a design consideration, meaning that you will write your bolts to consider that, which means it won't happen automatically for you.

If you think that a Topology is in a way a tree of nodes, in which a message travels in one or more branches, each node will have to +ack(tuple)+ or +fail(tuple)+ so Storm knows that a message failed and to notify the Spout or Spouts that originated the message.
Since a Storm topology runs in highly parallelized environment, the way to track down the original Spout instance is to actually pass along, when emitting, the tuple, which has a reference to the Spout that originated it.
This technique is called Anchoring and lets see how we can change SplitSentence bolt that we just saw so it guarantees message processing.

[source, java]
----
class SplitSentence implements IRichBolt {
    private OutputCollector collector;
    
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
        this.collector = collector;
    }

    public void execute(Tuple tuple) {
        String sentence = tuple.getString(0);
        for(String word: sentence.split(" ")) {
            collector.emit(tuple, new Values(word));
        }
        collector.ack(tuple);
    }

    public void cleanup() {
    }

    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }        
}
----

The exact line where anchoring happens is +collector.emit(tuple, new Values(word));+. As we said, passing along the tuple is the way Storm track which are the originating Spouts.
By calling +collector.ack(tuple)+ and +collector.fail(tuple)+ you are notifiying Spouts about what happened with the messages.
Storm also handle a default timeout of 30 seconds in case you don't +ack+ or +fail+, after which it will send a fail message to the Spout.

Of course the Spout need to take care of failing message and retry or discard accordingly.

=== Multiple streams

A bolt can emit tuples to multiple streams. This can be very usefull in some cases.
In order to do that just use +emit(streamId, tuple)+, where +stremId+' a string that identifies the stream. Later, from the +TopologyBuilder+ you can decide to which stream you susbcribe.

=== Multiple Anchoring

Some times you will use your bolts to do stream joins or aggregations. To do that youll will have to buffer more than one tuple in memory.
In order to message guarantee in this scenario you'll have to anchor the stream to more than one tuple. This is done by calling +emit+ with a +List+ of tuples, like this:
[source, java]
----
...
List<Tuple> anchors = new ArrayList<Tuple>();
anchors.add(tuple1);
anchors.add(tuple2);
_collector.emit(anchors, values);
...
----

This way any time a bolt ack or fail, it will notify the tree, and since you anchored the stream to more than one tuple, all spouts invovled will get notified.

=== Using IBasicBolt to do acking automatically

Now as you probably noticed, there are lots of cases where you don't want to deal with message guaranteeing. Storm provides another interface for bolts called +IBasicBolt+ that is useful in these cases.
Basically the message get acked right after the +execute+ method finished executing. Also you'll be using +BasicOutputCollector+, where tuples emitted are automatically anchored to the input tuples.
