[[a_real_life_example]]
== Introduction

The idea of this chapter is to illustrate a quite different Architecture and use case of storm. In the most general use case for storm is to receive a huge amount of information through the spouts, transform it using a chain of bolts (generally to join or aggregate this data) and finally store the transformed information.

Typical applications for this architecture are:

* Real time Twitter feeds analysis.
* Real time WebSite performance analysis.
* Stock markets information analysis.

But in this chapter we will change those characters and bring into life a completely different set of applications. Lets make the topology receive queries through the spouts and process them using a set of bolts that will take advantage of distributed CPU and maybe data locality. And in the end another set of bolts will send the response back to the user that requested it.

This architecture is called DRPC (Distributed Remote Process Communication) and maybe one of the mosts attractive algorithms to implement using this architecture is the Google's Map Reduce.

In this chapter we will explore the idea of large e-commerce platform search engine implemented using a Storm Topology and a Node.Js web server.

The most important requirement is: given a query request i need to search through a huge amount of data in real time and bring the information back to the user immediately.  Another constraint is that items information changes all the time, and the search DB needs to be updated.

For example, we query our system to look for an mp3 player:

[source, javascript]
----
> curl http://localhost/mp3
[
     {
          "id": 1,
          "title": "mp3 player 16Gb",
          "price": 750
     },
     {
          "id": 7,
          "title": "mp3 player 8Gb",
          "price": 450
     }
]
----


We will walk through this chapter implementing a solution for this problem using Storm.

=== Requirements:
TODO: Download the sources:
git clone ...
TODO: Install Node.Js
node.org

== 1st Iteration: The Client-Client problem (The selfish guy).

As we said, we'll read the queries using Spouts. But Spouts are not designed to be WebServers they are more clients than servers. 

So, here we have the 1st problem we need to solve. We need an intermediate WebServer as an adapter. We need to establish a client-client communication.

There is a Node.Js WebServer shipped with the source code in the path (node-js-server/node-drpc-server.js).

We won't go in deep detail of how it is implemented, but we will play with this server in order to understand "What it does".

TODO: ## GRAPH OF THE GUYS TALKING ##

The server listens to HTTP protocol in three different ports, each of them serving for different porpouses:

8080: The real client connections. Here is where the guy making the questions connects to:

e.g.: 
> curl localhost:8080/where-is-the-money


8081: Is where the client interested in answering something connects to get the questions issued by the previous one.

e.g.: 
> curl localhost:8081/pendings/
127.0.0.1
0
/where-is-the-money


8082: Is where the answer is pushed back to.

e.g.:
> curl -X 'POST' --data "We're not a bank" localhost:8082/answer?id=0


TODO: ### PRINTS OF THE CURLS ##

TODO: Practical steps list.


== 2nd Iteration: The Storm Client Iteration (The selfish topology).

But the big question now is. What is the relation between those curls and the selfish guy with a storm topology? And worse, what is the relation between these and a search engine?. Ok, in this iteration we will focus in the first question, and let the second one for the next.

As we've said, topologies (Spouts & Bolts) can behave as clients, but the guy who makes query two. So, if we want the topology to behave as a server we need the adapter that we discussed in the previous iteration.

So, here we go, lets implement the same example of the selfish guy as a storm topology. 

image::figs/ch06-selfish-topology.jpg[]

You can download the sources of this example from: TODO

Running the example: TODO

=== Topology components

==== ClientSpout:

As we saw in previous chapters, Spouts are used to feed the topology with data. In this case, data is in the form of user requests that needs to be processed by the topology.

In this specific case, the client Spout is the one which reads pending transactions from the WebServer. Once transaction is read, it emmits the tuple, in order for the cluster to manage this tuple according its defined in the topology.

We use the HttpClient library to get the pending requests from the WebServer executing an HTTP GET method in the port 8081. After that we parse the response and emmit de tuples.


TODO: ## SRC SPOUT ##

==== SelfishBolt:
In previous chapters we've used Bolts to do the required work with the data feeded by the spouts, and we played a bit with topologies distribution abilities. In this iteration, processing is quite simple, we just need to write answer the WebServer with a predefined answer. So, there is no complex computation at all, but we use it to execute an HTTP POST to the 8082 port of the WebServer using the same library than before.


TODO: ## SRC BOLT ##


== 3rd Iteration: Searching Items.

Now that we understood the details of how the communication between the Client, the WebServer and the Storm Topology works we'll work in the search engine itself.

If we're going to search for something we need a few things: The information where we will perform the search, A criteria to search this information, an efficient datastructure to solve the searching problem, and an efficient computation distribution layout in order for this query to return fast.

=== The information:

We have an items REST API which provides us with the items information on demand. As the items information changes all the time, and we have the requirement to be 100% up to date someone will notify us when an item changes some of its information executing an HTTP GET int the URL we provide.

     * This notification mechanism is widely used in WebPlatforms APIs like facebook graph API.
     * A Mock server of the items REST API is provided with the source code.

TODO: Playing with the MOCK server.

=== The search criteria:

Our simple search engine, will apply a very simple criteria. The query will be a set of words which should be contained in the title of every item that matches in the results.

For simplicity, the search string will be case sensitive, required words will be separated by "-" and we'll not use any kind of steeming nor soft match algorithm neither.

=== The datastructure:

There is a very simple and efficient datastructure to search for words in a collection, it's called "Inverted Index" and we'll be implementing one in this iteration.

TODO: Explain the inverted index.


=== The computation distribution layout:

This is a very important issue, may be, the most important for the topic of this book. For this reason we will leave this issue till the next iteration which will focus exactly on that.


=== Implementation:

::image::figs/ch06-simple-search-engine.jpg[]


TODO: Show simple-search most important source code pieces and explain components.



image::figs/ch06-complete-topology.jpg[]


