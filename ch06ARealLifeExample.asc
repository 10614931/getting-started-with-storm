[[a_real_life_example]]
= A Real Life Example

== Introduction

The idea of this chapter is to illustrate quite a different architecture and use case for Storm. The most common use case is to receive a huge amount of information through the spouts, transform it using a chain of bolts (generally to join or aggregate the data) and finally store the transformed information.

Typical applications for this architecture are:

* Real time Twitter feed analysis.
* Real time web site performance analysis.
* Stock market information analysis.

But in this chapter we'll change those characters and bring to life a completely different set of applications. Let's make the topology receive queries through the spouts and process them using a set of bolts that will take advantage of distributed CPU and maybe data locality. And in the end another set of bolts will send the response back to the user that requested it.

This architecture is called DRPC (Distributed Remote Process Communication), and perhaps one of the most attractive algorithms to implement using this architecture is Google's Map Reduce.

In this chapter we'll explore the idea of a large e-commerce platform search engine implemented using a Storm topology and a Node.js web server.

The most important requirement is: given a query request we need to search through a huge amount of data in real time and bring the information back to the user immediately. Another constraint is that items' information changes frequently, and the search database needs to be updated accordingly.

For example, if we query our system to look for an mp3 player, it should give us a result like the following:

----
> curl http://localhost/mp3
[
     {
          "id": 1,
          "title": "mp3 player 16Gb",
          "price": 750
     },
     {
          "id": 7,
          "title": "mp3 player 8Gb",
          "price": 450
     }
]
----


In this chapter we will walk through a solution for this problem using Storm.

=== Requirements:
TODO: Download the sources:
git clone ...
TODO: Install Node.Js
node.org

== 1st Iteration: The Client-Client problem (The selfish guy).

As we said, we'll read the queries using spouts. But spouts are not designed to be web servers, they are more likely to act as clients. They are deployed automatically to a given machine in the cluster, and may be reallocated, so it isn't always easy to locate them. It's easier to implement a client that pulls tasks from a known source than to let other actors in the system push information to the spout.

So the first problem we need to solve is to create an intermediate web server as an adapter in order to establish client-client communication. As this isn't the main problem we want to deal with in this book, we implemented a server that does just that (see node-js-server/node-drpc-server.js).

We won't go into the detail of how it's implemented, but we will play with this server in order to understand what it does.

In this first iteration we'll use a simple client-server example: One client (asking for money) and one server (denying that money) and make them behave like two clients interacting through the server.

image::figs/ch06-im-not-a-bank.jpg[]

=== The web server
The server listens for HTTP requests on three different ports:

==== Port 8080:
The real client connection. This is where the guy asking the questions connects to:

----
> curl localhost:8080/please-give-me-some-money
----

==== Port 8081:
Where the client that answers questions connects to, to get the questions issued by the previous client.

----
> curl localhost:8081/
127.0.0.1
0
/please-give-me-some-money
----


==== Port 8082: 
Where the topology pushes the answer back to.

----
> curl -X 'POST' --data "We're not a bank" localhost:8082/?id=0
----

==== Playing with the server:
We strongly suggest you play with this server for a while in order to understand how it works. Here we list a few experiments that you can do:

1) Try opening a few terminals and execute different requests to port 8080. Then curl to port 8081 and you'll see all the queries you made with an assigned id. After that you can POST some content to port 8082 with the issued id and see how the different curls you made to port 8080 respond with the content you posted.

2) Try doing a GET to port 8081 before issuing any queries to port 8080, and you'll see that it blocks until you execute a GET to port 8080.

TIP: To start the server run
node ./node-js-server/node-drpc-server.js
in the root of the example directory.


== 2nd Iteration: The Storm Client Iteration.

But the big question now is, what do those curls have to do with the selfish guy with a Storm topology? And what do they have to do with a search engine?. In this iteration we'll focus on the first question, and leave the second question for the next iteration.

As we mentioned, topologies (spouts & bolts) can behave as clients, but so does the guy who asks questions. So if we want the topology to behave as a server we need to use the adapter that we discussed in the previous iteration.

Let's implement the same example of the selfish guy as a Storm topology:

image::figs/ch06-selfish-topology.jpg[]

You can download the sources of this example from: TODO (REPO LINK)

If you want to start the topology




=== Topology components

==== ClientSpout:

As we saw in previous chapters, spouts are used to feed the topology with data. In this case, data is in the form of user requests that are processed by the topology, and the client spout reads pending transactions from the web server. When a transaction is read it is emitted as a tuple so that the cluster can manage the tuple according to how the topology is defined.

We use the HttpClient library to get the pending requests from the web server, by executing an HTTP GET on port 8081. Then we parse the response and emit the tuples. As we will use this type of spout a lot in subsequent iterations, we've created an abstract class called AbstractClientSpout. In this example we create a subclass to look for pending requests on localhost on port 8081, that accepts a maximum of ten requests for each GET.

[source, java]
----
package selfish;
...
public class ClientSpout extends AbstractClientSpout {
	@Override
	protected String getPullHost() {
		return "localhost:8081";
	}

	@Override
	protected int getMaxPull() {
		return 10;
	}
}
----

But we're hiding a lot of interesting details here. Let's take a look at the *nextTuple()* method implemented in the abstract class.

[source, java]
----
package storm.utils;
...
public abstract class AbstractClientSpout implements IRichSpout {
...
	transient LinkedBlockingQueue<Request> pendingRequests;

	@Override
	public void nextTuple() {
		Request req;
		try {
			req = pendingRequests.poll(1, TimeUnit.SECONDS);
			collector.emit(new Values(req.origin, req.id, req.content));
		} catch (InterruptedException e) {
			log.error("Polling interrrupted", e);
		}
	}
...
----

As you can see, there is no action being executed directly here. It's better not to run long blocking operations in this method because as we saw in previous chapters, it's called in a loop with the *ack* and *fail* methods, so blocking it can slow or hang the system. Instead we read from a LinkedBlockingQueue and the pull operation is executed in a separate Thread.

TIP: Notice that we're not anchoring the tuples, so message delivery is not guaranteed in these examples.

[source, java]
----
package storm.utils;
...
public abstract class AbstractClientSpout implements IRichSpout {
...
	@Override
	public void open(@SuppressWarnings("rawtypes") Map conf, TopologyContext context,
			SpoutOutputCollector collector) {
		log = Logger.getLogger(this.getClass());
		this.conf= conf;
		this.context= context;
		this.collector= collector;
		this.pendingRequests = new LinkedBlockingQueue<Request>(1000);
		this.isDying = false;
		reconnect();
		t = new Thread("ClientSpout["+this.getClass().getName()+"] pulling:"+"http://"+getPullHost()+"/?max="+getMaxPull()){
			@Override
			public void run() {
				while(true) {
					try {
						if(isDying)
							return ;
						executeGet();
					} catch(Throwable t) {
						log.error("Error in executeGet", t);
						try {Thread.sleep(1000);} catch (InterruptedException e) {}
					}
				}
			}
		};
		t.start();
	}
	
	...

	public void executeGet(){
		HttpResponse response;
		BufferedReader reader= null;
		try {
			response = httpclient.execute(httpget);
			HttpEntity entity = response.getEntity();
			reader= new BufferedReader(new InputStreamReader(entity.getContent()));
			String origin= reader.readLine();
			while(true) {
				Request req= new Request();
				req.origin = origin;
				req.id = reader.readLine();
				req.content = reader.readLine();
				
				if(req.id == null || req.content==null)
					break;
				else {
					req.content = req.content.substring(1);
					boolean inserted = pendingRequests.offer(req, 10, TimeUnit.SECONDS);
					if(!inserted) 
						log.error("pendingRequests queue is full, discarding request ["+req+"]");
				}
			} 
		} catch (Exception e) {
		...
----

The *executeGet* method feeds the queue with requests, and is called in a separate Thread.

So, wrapping up, the ClientSpout is an implementation of the AbstractClientSpout which has two abstract methods. *getPullHost()* to define where to read the requests and *getMaxPull()* which tell us how many requests will be read each GET.


==== SelfishBolt:
In previous chapters we've used bolts to do the required work with the data fed by the spouts, and we played a bit with distributed topologies. In this iteration, processing is quite simple, we just need to write a web server with a predefined answer to questions. There is no complex computation, we just use it to execute an HTTP POST to port 8082 of the web server using the same library as before.
Again, as we're going to implement several spouts like this during this chapter we created an abstraction, the AbstractAnswerBolt which we extend in order to implement the bolt we need.

[source, java]
----
package selfish;
...
public class AnswerBolt extends AbstractAnswerBolt {
	private static final long serialVersionUID = 1L;
	
	@Override
	public void execute(Tuple input) {
		String origin= input.getString(0);
		String requestId= input.getString(1);
		sendBack(origin, requestId, "I'm not a bank!");
	}

	@Override
	protected int getDestinationPort() {
		return 8082;
	}
}
----

The most important implementation missing in the previous piece of code is the *sendBack* method, which is in charge of sending the response back to the web server. Here's its implementation:

[source, java]
----
...
package storm.utils;
...
	protected void sendBack(String origin, String id, String content){
		String to= "http://"+origin+":"+getDestinationPort()+"/?id="+id;
		log.debug("Answering to:"+to);
		HttpPost post= new HttpPost(to);
		try {
			StringEntity entity= new StringEntity(content);
			post.setEntity(entity);
			HttpResponse response= client.execute(post);
			InputStream is= response.getEntity().getContent();
			is.close();			
		} catch (Exception e) {
			log.error("Answering to:["+to+"] with ["+content+"]", e);
		}
	}
...
----
==== The Topology 

The topology for this example is very simple. We just need to connect our spout with our bolt, and in this case we'll just run it in a LocalCluster.

[source, java]
----
package selfish;
...
public class SelfishTopologyStarter {
	public static StormTopology createTopology() {
		TopologyBuilder builder= new TopologyBuilder();
		builder.setSpout("questions-spout", new ClientSpout(), 1);
		builder.setBolt("answers-bolt", new AnswerBolt(), 1).allGrouping("questions-spout");
		return builder.createTopology();
	}
	
	
	public static void main(String[] args) {
		Logger.getRootLogger().setLevel(Level.ERROR);
		Logger.getLogger("org.apache.http.wire").setLevel(Level.ERROR);
		Logger.getLogger("selfish").setLevel(Level.DEBUG);
		
		LocalCluster cluster = new LocalCluster();
		StormTopology topology = createTopology();
		cluster.submitTopology("Test-Topology", new Config(), topology);
	}
}
----


=== Running the example and playing with it

You can run this example by opening two ssh terminals and running the following from the example path:

==== First terminal: Run the server and the topology
----
>./prepare-test-environment.sh 
>mvn package
...  
	Lots of standard output...
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 32.480s
[INFO] Finished at: Tue Feb 07 01:51:22 GMT-03:00 2012
[INFO] Final Memory: 11M/81M
[INFO] ------------------------------------------------------------------------
>cd bin
>./run_selfish_topology.sh 
----

TIP: You can take a look at this script in the repository to understand how the topology and the server are launched.

==== Second terminal: Run the query as a user of the system
----
>curl localhost:8080/where-is-the-money
I'm not a bank!
>
----


== 3rd Iteration: Searching Items.

Now that we understand the details of how the communication between the client, the web server and the Storm topology works, we'll work on the search engine itself.

In order to search for something we need a few bits of information: where to perform the search, search criteria, an efficient data structure to solve the search problem, and an efficient computation distribution layout in order for this query to return quickly.

=== The information:

We have an items REST API which provides us with the item information on demand. As the item information changes frequently, and we have the requirement of being 100% up to date, we need to be notified when some piece of information about an item changes. We'll provide a GET URL for these notifications.

     * This notification mechanism is widely used in web platform APIs like Facebook Graph API.
     * A mock server for the items REST API is provided with the source code.

For this iteration we will skip this issues and will just hardcode some items in the Bolt. In further iterations we will cover the full indexing problem.

=== The search criteria:

Our search engine will apply two criteria. The query will be a set of words, all of which must be in the title of an item for it to match the search, and the results will be sorted by price, highest to lowest

For simplicity, the search string will be case sensitive, required words will be separated by "-" and we won't use any kind of steeming  or soft match algorithm.

=== The data structure:

There is a simple and efficient data structure to search for words in a collection called "Inverted Index", and we'll implement it in this iteration. We've implemented an ItemsContainer and an Item class that solve the search problem by storing the items and the inverted index in memory.

==== The Item entity

A simple class with public fields representing the item information.
[source, java]
----
package search.model;
...
public class Item implements Serializable {
...
	public long id;
	public String title;
	public double price;
...
----

TIP: Check the full implementation to for the details, like the hashCode and equals implementations.

==== The ItemContainer class

This class contains the in-memory inverted index and item information. The most important methods are those which allows us to modify the structure (add, remove, update) and to search into it (getItemsContainingWords):

[source, java]
----
package search.model;
...
public class ItemsContainer {
...
	public void add(Item i){
	public void update(Item i){
	public void remove(int itemId){
	public void remove(Item i){
	public Set<Item> getItemsContainingWords(String words){
...
----

TIP: To see how it works in detail we suggest you take a look at the source code in the repository.

=== The computation distribution layout:

This is a very important issue, maybe the most important in this topic, so we'll leave it for the next iteration.


=== Components

image::figs/ch06-simple-search-engine.jpg[]


==== Topology Layout

As you can see in the diagram above, the topology layout is simple. Here's the implementation:

[source, java]
----
package simplesearch;
...
public class SimpleSearchEngineTopologyStarter {
	public static StormTopology createTopology() {
		TopologyBuilder builder= new TopologyBuilder();

		builder.setSpout("queries-spout", new QueriesSpout(), 1);
		builder.setBolt("queries-processor", new SearchBolt(), 1).allGrouping("queries-spout");
		builder.setBolt("answer-query", new AnswerQueryBolt(), 2).allGrouping("queries-processor");
		return builder.createTopology();
	}
...
----

==== The QueriesSpout

This is an implementation of the AbstractClientSpout that we explained in the previous iteration. The QueriesSpout is in charge of feeding the topology with new work. There are no big changes to that class in this iteration, we suggest you check out the source code for details.

==== The AnswerQueryBolt

The AnswerQueryBolt is quite similar to the AnswerBolt in the previous iteration, but now we output the search results in JSON format using json-simple lib.

[source, java]
----
package simplesearch;
...
public class AnswerQueryBolt extends AbstractAnswerBolt {
...
	public void execute(Tuple input) {
		String origin= input.getString(0);
		String requestId= input.getString(1);
		List<Item> finalResult= su.fromByteArray(input.getBinary(2));
		
		JSONArray list = new JSONArray();
		for (Item item : finalResult) {
			JSONObject obj= new JSONObject();
			obj.put("title", item.title);
			obj.put("id", item.id);
			obj.put("price", item.price);
			list.add(obj);
		}
		String json= JSONValue.toJSONString(list);
		sendBack(origin, requestId, json);
	}
...
----

=== The SearchBolt

This bolt is responsible for executing the search itself, and emitting the top 5 results. For simplicity, in this iteration we've cut out the indexing features, so the items are hard-coded in the *populateItems* method, which is called from *prepare*.
The *execute* method gets the query information, executes the query and emits the results that will be sent to the AnswerQueryBolt. Note that all the search stuff is done inside of the ItemsContainer class. The results are sorted and limited to the top five items.

[source, java]
----
package simplesearch;
...
public class SearchBolt implements IRichBolt {
...
	ItemsContainer myItems;
...
	private void populateItems() {
		myItems.add(new Item(0, "old dvd player", Math.random()*Math.random()*100));
		myItems.add(new Item(1, "new cell phone", Math.random()*100));
		myItems.add(new Item(2, "old pirates coin", Math.random()*100));
		myItems.add(new Item(3, "fashion sun glasses", Math.random()*100));
...
	@Override
	public void execute(Tuple input) {
		String origin= input.getString(0);
		String requestId= input.getString(1);
		String query= input.getString(2);
		List<Item> results= executeQuery(query, 5);
		collector.emit(new Values(origin, requestId, su.toByteArray(results)));
	}
...
	private List<Item> executeQuery(String query, int quantity) {
		List<Item> items= new ArrayList<Item>(myItems.getItemsContainingWords(query));
		Collections.sort(items, new Comparator<Item>() {
			@Override
			public int compare(Item o1, Item o2) {
				double diff= o1.price-o2.price;
				if(diff>0)
					return -1;
				else
					return 1;
			}
		});
		
		if(items.size()>quantity)
			items = items.subList(0, quantity-1);
		return items;
	}
...
----

=== Running and playing

To run this topology you first need to package the project. If you ran the previous iteration, you've already done this step. If not, please check the build process from the previous iteration.

==== Starting the web server and the topology

From the repository root directory
----
>cd bin
>./run_simple_search.sh
----

==== Playing with the topology

Just make some queries to the simple search engine and have fun!

----
>curl localhost:8080/new
[{"id":23,"title":"new home theater audio system","price":99.93124903743946},{"id":24,"title":"new high speed gamer computer","price":91.32348802527693},{"id":19,"title":"brand new suv","price":82.34854032658393},{"id":18,"title":"brand new car","price":82.31161566843997}]

>curl localhost:8080/cell-phone
[{"id":1,"title":"new cell phone","price":80.74995117131074},{"id":6,"title":"cheap cell phone case","price":13.004691751903996},{"id":26,"title":"car cell phone adapter","price":6.569241592964081}]
...
----


=== Problems with this approach

This is a very simple approach to a search engine. But it faces a few severe problems for a real use case.

* Item volume scalability.
* Throughput scalability.
* Information can't be updated.

In the next iteration we will focus on solving those problems, and really taking advantage of Storm's features to do so.

== 4th Iteration: Shard & Distribute

=== Scaling the Storm Topology

The idea behind scaling this solution is: Divide and Conquer!. We will split the data and the processing through different servers and merge the results at the end. And here, in this process, is were Storm starts to be very useful.

==== Spliting the search process
Basically, we will send the same query to, for example 10, different instances of the SearchBolt each of them with the potential to run in a different server and hosting a different portion of the items. This concept is usually called sharding and each of the parts running a query is called shard.

This solves the problem of scaling the number of items in the system. Because if we have to serve a bigger number of Items, we just need to increase the number of SearchBolts in the topology and as long as we run this topology into a large enough cluster we will scale our memory availability horizontaly without problems.

Also improves our ability to serve a higher queries throghput, because we are executing all of our stuff in a parallel way increasing the amount of computing power that we can use. 

So, by modifying this, we are in a much better solution.

image::figs/ch06-allgrouping.jpg[]

TIP: There are no major differences between the SearchBolt we'll be using here than in the previous iteration, and, in fact they use exactly the same data structure for items and indexes.

==== Merging the information back

After each SearchBolt processes an independent part of the query, information needs to be joined. We will introduce the MergeBolt, that, is a new component that merges all the SearchBolts results, keeps the top 5 results and emmits a tuple once all the bolts answered or a timeout is reached. 
This Bolt has two main responsabilities: Holding the search results and identifying the moment in which the final result will be emmited.

===== Holding the partial responses

The MergeBolt has an in memory HashMap called *inCourse* that holds the results that have arrived divided by request. Note that for this Bolt the request id generated by the Node.Js server is not enough because as we will see soon, different Node.Js servers can repeat the same id, generating collisions. For this reason, the key used for the *inCourse* map is the combination of the *origin* (Hostname of the origin) and the *requestId* (The internal id in the origin server).

[source, java]
----
...
	HashMap<String, Merger> inCourse= new HashMap<String, Merger>();
...
----

The inner class *Merger* in the bolt has the -quite obvious- responsability of Merging the results coming from the different shards and keep just the top results according with the sort criteria which is the price in this case. The main methods of this class are:

[source, java]
----
public static class Merger {
	public void merge(List<Item> newItems);
	public List<Item> getResults();
	public String getRequestId();
	public int getTotalMerged();
}
----

The *merge* method is in charge of merging a set of new results with the previously stored ones. The *getResults* method will answer with the current top results for this *requestId*. The *getRequestId* method gives you the id of the merged request, has the same value of the key of this *Merger* instance in the *inCourse* map. The *getTotalMerged* method returns the times *merge* has been called so far.

TIP: Note that one Merger is created for each query processed by the system and the merge method is called as many times as the number of SearchBolt instances are running in parallel in the topoloogy.


===== Finishing the request

The input of this Bolt are the partial responses, the output is one unified Tuple. As we perform a wait and hold operation, a very important decision we need to make is "When do I stop waiting for new results and emmit a Tuple?". There are two different situations in which we will finish a request and emmit its results: All the parallelized *SearchBolt* instances have finished or a timeout is reached.

If all the results have reached this bolt, it's time to emmit a result and remove the entry for the current request from the *inCourse* map. Here, in the *execute* method is where we take that decision.

[source, java]
----
	public void execute(Tuple input) {
		...
		merger.merge(shardResults);

		if(merger.getTotalMerged()>=totalShards)
			finish(merger);
	}
	...
----

TIP: The *totalShard* variable is set in the *prepare* method by reading the parallelism hint. You can take a look at the source code if you want greater detail on this.

===== Managing Timeouts

When a partial result is delayed, we will ignore it. That's because it's expected for the system to have a predictable response time, remember that on top of all this, there is a user waiting on the screen. And because in the event of a outage of a small part of the system we would like to have an answer within the available nodes, even if the response is not complete it's better than nothing. Also if the system is strongly sharded, the impact of the outage of one shard is really small, eg: if we parallelize in 100 shards only 1% of the results will be missing at the most, and that's assuming that the unavailable shard contains only top ranked results.

We implement the timeouts by sheduling a *TimerTask* in the *prepare* method that monitors the age of each *inCourse* request and forces a call to the *finish* method in order to end the task and emmit the tuple with the results gathered so far. 

[source, java]
----
public class MergeBolt implements IRichBolt {
...
	public void prepare(@SuppressWarnings("rawtypes") Map stormConf, TopologyContext context, ...
	...
		TimerTask t= new TimerTask() {
			@Override
			public void run() {
				ArrayList<Merger> mergers;
				
				synchronized (inCourse) {
					mergers= new ArrayList<MergeBolt.Merger>(inCourse.values());
				}
				
				for (Merger merger : mergers) {
					if(merger.getAge()>1000)
						finish(merger);
				}
			}
		};
		Timer timer= new Timer();
		timer.scheduleAtFixedRate(t, 1000, 1000);
	...
	}
}
----

Even those two situations are very different, both end up calling the *finish* method that emmits the final result and removes the entry from the *inCourse* structure.

[source, java]
----
...
	protected void finish(Merger merger){
		collector.emit(new Values(merger.getOrigin(), merger.getRequestId(), su.toByteArray(merger.getResults())));
		synchronized (inCourse) {
			inCourse.remove(merger.getId());	
		}
	}
...
----

TIP: Syncronization is necessary because the TimerTask and the *execute* method will be running in different threads, and we need to prevent race situations.

==== The MergeBolt in the topology

The *MergeBolt* is added to the topology between the *SearchBolt* and the *AnswerQueryBolt*. And in the source code below it's parallelized 3 times. It recieves Tuples from all the *SearchBolt* instances. As we saw in the paragraphs above, in order to be successful a MergeBolt do

[source, java]
----
package search;
...
public class SearchEngineTopologyStarter {
	...
	public static StormTopology createTopology() {
		...
		builder.setSpout("queries-spout", new QueriesSpout(), 1);
		builder.setBolt("search", new SearchBolt(), 5).allGrouping("queries-spout").allGrouping("read-item-data");
		builder.setBolt("merge", new MergeBolt(), 2).fieldsGrouping("search", new Fields("origin", "requestId"));
		builder.setBolt("answer-query", new AnswerQueryBolt(), 2).fieldsGrouping("merge", new Fields("origin"));
		...
	}
}
----

TODO: GRAPH FieldsGrouping

=== Scaling the WebLayer

	TODO: Write

=== Indexing

	TODO: Write

=== Complete Search Engine

	TODO: Write

image::figs/ch06-complete-topology.jpg[]

==== Spouts

	TODO: Write

==== Bolts

	TODO: Write

==== Topology Building

	TODO: Write

== 5th Iteration: Deploy in Amazon EC2

	TODO; Drawing full implementation
	TODO: Write

=== Deploying the Topology

	TODO: Write

==== The storm deploy project

	TODO: Write

=== Deploying the WebServer layer

	TODO: Write

==== WebServers creation

	TODO: Write

==== Elastic Load Balancer

	TODO: Write
