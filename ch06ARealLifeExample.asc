[[a_real_life_example]]
== A Real Life Example

=== Introduction

The idea of this chapter is to illustrate quite a different architecture and use case for Storm. The most common use case is to receive a huge amount of information through the spouts, transform it using a chain of bolts (generally to join or aggregate the data) and finally store the transformed information.

Typical applications for this architecture are:

* Real time Twitter feed analysis.
* Real time web site performance analysis.
* Stock market information analysis.

But in this chapter we'll change the problem characteristics and bring to life a completely different set of applications. We'll make a topology that receives queries through spouts, and processes them using a set of bolts that take advantage of distributed CPU and data locality. Finally another set of bolts will send the response back to the user that sent the request.

This architecture is called DRPC (Distributed Remote Process Communication), and perhaps one of the most attractive algorithms to implement using this architecture is Google's Map Reduce.

In this chapter we'll explore the idea of a large e-commerce platform search engine implemented using a Storm topology and a Node.js web server.

The most important requirement is: given a query request we need to search through a huge amount of data in real time and bring the information back to the user immediately. Another constraint is that items' information changes frequently, and the search database needs to be updated accordingly.

For example, if we query our system to look for an mp3 player, it should give us a result like the following:

----
> curl http://localhost/mp3
[
     {
          "id": 1,
          "title": "mp3 player 16Gb",
          "price": 750
     },
     {
          "id": 7,
          "title": "mp3 player 8Gb",
          "price": 450
     }
]
----


In this chapter we'll walk through a solution for this problem using Storm.

==== Requirements:
TODO: Download the sources:
git clone ...
TODO: Install Node.Js
node.org

=== 1st Iteration: The Client-Client problem (The selfish guy).

As we said, we'll read the queries using spouts. But spouts are not designed to be web servers, they are more likely to act as clients. They are deployed automatically to a given machine in the cluster, and may be reallocated, so it isn't always easy to locate them. It's easier to implement a client that pulls tasks from a known source than to let other actors in the system push information to the spout.

So the first problem we need to solve is to create an intermediate web server as an adapter in order to establish client-client communication. As this isn't the main problem we want to deal with in this book, we implemented a server that does just that (see node-js-server/node-drpc-server.js).

We won't go into the detail of how it's implemented, but we will play with this server in order to understand what it does.

In this first iteration we'll use a simple client-server example: One client (asking for money) and one server (denying that money) and make them behave like two clients interacting through the server.

image::figs/ch06-im-not-a-bank.jpg[]

==== The web server
The server listens for HTTP requests on three different ports:

===== Port 8080:
The real client connection. This is where the guy asking the questions connects to:

----
> curl localhost:8080/please-give-me-some-money
----

===== Port 8081:
Where the client that answers connects to, to get the questions issued by the previous client.

----
> curl localhost:8081/
127.0.0.1
0
/please-give-me-some-money
----


===== Port 8082: 
Where the topology pushes the answer back to.

----
> curl -X 'POST' --data "We're not a bank" localhost:8082/?id=0
----

===== Playing with the server:
We strongly suggest you play with this server for a while in order to understand how it works. Here we list a few experiments that you can do:

1) Try opening a few terminal windows and execute different requests to port 8080. Then curl to port 8081 and you'll see all the queries you made with an assigned id. After that you can POST some content to port 8082 with the issued id and see how the different curls you made to port 8080 respond with the content you posted.

2) Try doing a GET to port 8081 before issuing any queries to port 8080, and you'll see that it blocks until you execute a GET to port 8080.

TIP: To start the server run
node ./node-js-server/node-drpc-server.js
in the root of the example directory.


=== 2nd Iteration: The Storm Client Iteration.

But the big question now is, what do those curls have to do with the selfish guy with a Storm topology? And what do they have to do with a search engine? In this iteration we'll focus on the first question, and leave the second question for the next iteration.

As we mentioned, topologies (spouts & bolts) can behave as clients, but so does the guy who asks questions. So if we want the topology to behave as a server we need to use the adapter that we discussed in the previous iteration.

Let's implement the same example of the selfish guy as a Storm topology:

image::figs/ch06-selfish-topology.jpg[]

You can download the source for this example from: TODO (REPO LINK)

If you want to start the topology




==== Topology components

===== ClientSpout:

As we saw in previous chapters, spouts are used to feed the topology with data. In this case, data is in the form of user requests that are processed by the topology, and the client spout reads pending transactions from the web server. When a transaction is read it is emitted as a tuple so that the cluster can manage the tuple according to how the topology is defined.

We use the HttpClient library to get the pending requests from the web server, by executing an HTTP GET on port 8081. Then we parse the response and emit the tuples. As we will use this type of spout a lot in subsequent iterations, we've created an abstract class called AbstractClientSpout. In this example we create a subclass to look for pending requests on localhost on port 8081, that accepts a maximum of ten requests for each GET.

[source, java]
----
package selfish;
...
public class ClientSpout extends AbstractClientSpout {
	@Override
	protected String getPullHost() {
		return "localhost:8081";
	}

	@Override
	protected int getMaxPull() {
		return 10;
	}
}
----

But we're hiding a lot of interesting details here. Let's take a look at the *nextTuple()* method implemented in the abstract class.

[source, java]
----
package storm.utils;
...
public abstract class AbstractClientSpout implements IRichSpout {
...
	transient LinkedBlockingQueue<Request> pendingRequests;

	@Override
	public void nextTuple() {
		Request req;
		try {
			req = pendingRequests.poll(1, TimeUnit.SECONDS);
			collector.emit(new Values(req.origin, req.id, req.content));
		} catch (InterruptedException e) {
			log.error("Polling interrrupted", e);
		}
	}
...
----

As you can see, there is no action being executed directly here. It's better not to run long blocking operations in this method because as we saw in previous chapters, it's called in a loop with the *ack* and *fail* methods, so blocking it can slow or hang the system. Instead we read from a LinkedBlockingQueue and the pull operation is executed in a separate Thread.

TIP: Notice that we're not anchoring the tuples, so message delivery is not guaranteed in these examples.

[source, java]
----
package storm.utils;
...
public abstract class AbstractClientSpout implements IRichSpout {
...
	@Override
	public void open(@SuppressWarnings("rawtypes") Map conf, TopologyContext context,
			SpoutOutputCollector collector) {
		log = Logger.getLogger(this.getClass());
		this.conf= conf;
		this.context= context;
		this.collector= collector;
		this.pendingRequests = new LinkedBlockingQueue<Request>(1000);
		this.isDying = false;
		reconnect();
		t = new Thread("ClientSpout["+this.getClass().getName()+"] pulling:"+"http://"+getPullHost()+"/?max="+getMaxPull()){
			@Override
			public void run() {
				while(true) {
					try {
						if(isDying)
							return ;
						executeGet();
					} catch(Throwable t) {
						log.error("Error in executeGet", t);
						try {Thread.sleep(1000);} catch (InterruptedException e) {}
					}
				}
			}
		};
		t.start();
	}
	
	...

	public void executeGet(){
		HttpResponse response;
		BufferedReader reader= null;
		try {
			response = httpclient.execute(httpget);
			HttpEntity entity = response.getEntity();
			reader= new BufferedReader(new InputStreamReader(entity.getContent()));
			String origin= reader.readLine();
			while(true) {
				Request req= new Request();
				req.origin = origin;
				req.id = reader.readLine();
				req.content = reader.readLine();
				
				if(req.id == null || req.content==null)
					break;
				else {
					req.content = req.content.substring(1);
					boolean inserted = pendingRequests.offer(req, 10, TimeUnit.SECONDS);
					if(!inserted) 
						log.error("pendingRequests queue is full, discarding request ["+req+"]");
				}
			} 
		} catch (Exception e) {
		...
----

The *executeGet* method feeds the queue with requests, and is called in a separate Thread.

So, wrapping up, the ClientSpout is an implementation of the AbstractClientSpout which has two abstract methods. *getPullHost()* to define where to read the requests and *getMaxPull()* which tells us how many requests will be read each GET.


===== SelfishBolt:
In previous chapters we've used bolts to do the required work with the data fed by the spouts, and we played a bit with distributed topologies. In this iteration, processing is quite simple, we just need to write a web server with a predefined answer to questions. There is no complex computation, we just use it to execute an HTTP POST to port 8082 of the web server using the same library as before.
Again, as we're going to implement several spouts like this during this chapter we created an abstraction, the AbstractAnswerBolt which we extend in order to implement the bolt we need.

[source, java]
----
package selfish;
...
public class AnswerBolt extends AbstractAnswerBolt {
	private static final long serialVersionUID = 1L;
	
	@Override
	public void execute(Tuple input) {
		String origin= input.getString(0);
		String requestId= input.getString(1);
		sendBack(origin, requestId, "I'm not a bank!");
	}

	@Override
	protected int getDestinationPort() {
		return 8082;
	}
}
----

The most important implementation missing in the previous piece of code is the *sendBack* method, which is in charge of sending the response back to the web server. Here's its implementation:

[source, java]
----
...
package storm.utils;
...
	protected void sendBack(String origin, String id, String content){
		String to= "http://"+origin+":"+getDestinationPort()+"/?id="+id;
		log.debug("Answering to:"+to);
		HttpPost post= new HttpPost(to);
		try {
			StringEntity entity= new StringEntity(content);
			post.setEntity(entity);
			HttpResponse response= client.execute(post);
			InputStream is= response.getEntity().getContent();
			is.close();			
		} catch (Exception e) {
			log.error("Answering to:["+to+"] with ["+content+"]", e);
		}
	}
...
----
===== The Topology 

The topology for this example is very simple. We just need to connect our spout with our bolt, and in this case we'll just run it in a LocalCluster.

[source, java]
----
package selfish;
...
public class SelfishTopologyStarter {
	public static StormTopology createTopology() {
		TopologyBuilder builder= new TopologyBuilder();
		builder.setSpout("questions-spout", new ClientSpout(), 1);
		builder.setBolt("answers-bolt", new AnswerBolt(), 1).allGrouping("questions-spout");
		return builder.createTopology();
	}
	...
}
----


==== Running the example and playing with it

You can run this example by opening two ssh terminals and running the following from the example path:

===== First terminal: Run the server and the topology
----
>./prepare-test-environment.sh 
>mvn package
...  
	Lots of standard output...
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 32.480s
[INFO] Finished at: Tue Feb 07 01:51:22 GMT-03:00 2012
[INFO] Final Memory: 11M/81M
[INFO] ------------------------------------------------------------------------
>cd bin
>./run_selfish_topology.sh 
----

TIP: You can take a look at this script in the repository to understand how the topology and the server are launched.

===== Second terminal: Run the query as a user of the system
----
>curl localhost:8080/where-is-the-money
I'm not a bank!
>
----


=== 3rd Iteration: Searching Items.

Now that we understand the details of how the communication between the client, the web server and the Storm topology works, we'll work on the search engine itself.

In order to search for something we need a few bits of information: where to perform the search, search criteria, an efficient data structure to solve the search problem, and an efficient computation distribution layout in order for this query to return quickly.

==== The information:

We have an items REST API which provides us with the item information on demand. As the item information changes frequently, and we have the requirement of being 100% up to date, we need to be notified when some piece of information about an item changes. We'll provide a GET URL for these notifications.

     * This notification mechanism is widely used in web platform APIs like Facebook Graph API.
     * A mock server for the items REST API is provided with the source code.

For this iteration we'll skip these issues and just hardcode some items in the Bolt. In later iterations we will cover the full indexing problem.

==== The search criteria:

Our search engine will apply two criteria. The query will be a set of words, all of which must be in the title of an item for it to match the search, and the results will be sorted by price, highest to lowest

For simplicity, the search string will be case sensitive, required words will be separated by "-" and we won't use any kind of streaming or soft match algorithm.

==== The data structure:

There is a simple and efficient data structure to search for words in a collection called "Inverted Index", and we'll implement it in this iteration. We've implemented an ItemsContainer and an Item class that solve the search problem by storing the items and the inverted index in memory.

===== The Item entity

A simple class with public fields representing the item information.
[source, java]
----
package search.model;
...
public class Item implements Serializable {
...
	public long id;
	public String title;
	public double price;
...
----

TIP: Check the full implementation to for the details, like the hashCode and equals implementations.

===== The ItemContainer class

This class contains the in-memory inverted index and item information. The most important methods are those which allows us to modify the structure (add, remove, update) and to search into it (getItemsContainingWords):

[source, java]
----
package search.model;
...
public class ItemsContainer {
...
	public void add(Item i){
	public void update(Item i){
	public void remove(int itemId){
	public void remove(Item i){
	public Set<Item> getItemsContainingWords(String words){
...
----

TIP: To see how it works in detail we suggest you take a look at the source code in the repository.

==== The computation distribution layout:

This is a very important issue, maybe the most important in this topic, so we'll leave it for the next iteration.


==== Components

image::figs/ch06-simple-search-engine.jpg[]


===== Topology Layout

As you can see in the diagram above, the topology layout is simple. Here's the implementation:

[source, java]
----
package simplesearch;
...
public class SimpleSearchEngineTopologyStarter {
	public static StormTopology createTopology() {
		TopologyBuilder builder= new TopologyBuilder();

		builder.setSpout("queries-spout", new QueriesSpout(), 1);
		builder.setBolt("queries-processor", new SearchBolt(), 1).allGrouping("queries-spout");
		builder.setBolt("answer-query", new AnswerQueryBolt(), 2).allGrouping("queries-processor");
		return builder.createTopology();
	}
...
----

===== The QueriesSpout

This is an implementation of the AbstractClientSpout that we explained in the previous iteration. The QueriesSpout is in charge of feeding the topology with new work. There are no big changes to that class in this iteration, we suggest you check out the source code for details.

===== The AnswerQueryBolt

The AnswerQueryBolt is quite similar to the AnswerBolt in the previous iteration, but now we output the search results in JSON format using json-simple lib.

[source, java]
----
package simplesearch;
...
public class AnswerQueryBolt extends AbstractAnswerBolt {
...
	public void execute(Tuple input) {
		String origin = input.getString(0);
		String requestId = input.getString(1);
		List<Item> finalResult = su.fromByteArray(input.getBinary(2));
		
		JSONArray list = new JSONArray();
		for (Item item : finalResult) {
			JSONObject obj = new JSONObject();
			obj.put("title", item.title);
			obj.put("id", item.id);
			obj.put("price", item.price);
			list.add(obj);
		}
		String json = JSONValue.toJSONString(list);
		sendBack(origin, requestId, json);
	}
...
----

==== The SearchBolt

This bolt is responsible for executing the search itself, and emitting the top 5 results. For simplicity, in this iteration we've cut out the indexing features, so the items are hard-coded in the *populateItems* method, which is called from *prepare*.
The *execute* method gets the query information, executes the query and emits the results that will be sent to the AnswerQueryBolt. Note that all the search stuff is done inside of the ItemsContainer class. The results are sorted and limited to the top five items.

[source, java]
----
package simplesearch;
...
public class SearchBolt implements IRichBolt {
...
	ItemsContainer myItems;
...
	private void populateItems() {
		myItems.add(new Item(0, "old dvd player", Math.random()*Math.random()*100));
		myItems.add(new Item(1, "new cell phone", Math.random()*100));
		myItems.add(new Item(2, "old pirates coin", Math.random()*100));
		myItems.add(new Item(3, "fashion sun glasses", Math.random()*100));
...
	@Override
	public void execute(Tuple input) {
		String origin = input.getString(0);
		String requestId = input.getString(1);
		String query = input.getString(2);
		List<Item> results = executeQuery(query, 5);
		collector.emit(new Values(origin, requestId, su.toByteArray(results)));
	}
...
	private List<Item> executeQuery(String query, int quantity) {
		List<Item> items = new ArrayList<Item>(myItems.getItemsContainingWords(query));
		Collections.sort(items, new Comparator<Item>() {
			@Override
			public int compare(Item o1, Item o2) {
				double diff = o1.price-o2.price;
				if(diff>0)
					return -1;
				else
					return 1;
			}
		});
		
		if(items.size()>quantity)
			items = items.subList(0, quantity-1);
		return items;
	}
...
----

==== Running and playing

To run this topology you first need to package the project. If you ran the previous iteration, you've already done this step. If not, please check the build process from the previous iteration.

===== Starting the web server and the topology

From the repository root directory
----
>cd bin
>./run_simple_search.sh
----

===== Playing with the topology

Just make some queries to the simple search engine and have fun!

----
>curl localhost:8080/new
[{"id":23,"title":"new home theater audio system","price":99.93124903743946},{"id":24,"title":"new high speed gamer computer","price":91.32348802527693},{"id":19,"title":"brand new suv","price":82.34854032658393},{"id":18,"title":"brand new car","price":82.31161566843997}]

>curl localhost:8080/cell-phone
[{"id":1,"title":"new cell phone","price":80.74995117131074},{"id":6,"title":"cheap cell phone case","price":13.004691751903996},{"id":26,"title":"car cell phone adapter","price":6.569241592964081}]
...
----


==== Problems with this approach

This is a very simple approach to a search engine. But it faces a few severe problems for a real use case.

* Item volume scalability.
* Throughput scalability.
* Information can't be updated.

In the next iteration we will focus on solving those problems, and really taking advantage of Storm's features to do so.

=== 4th Iteration: Shard & Distribute

==== Scaling the Storm Topology

The strategy we'll use to scale the system is: Divide and Conquer! We'll split the data and processing across multiple servers and merge the results at the end. This is where Storm starts to show its true strength.

==== Splitting the search process
We'll send the same query to, say, 5 different instances of the SearchBolt, each potentially running on a separate server and hosting a separate portion of the items. This concept is called sharding and each of the parts running a query is called shard.

This solves the problem of scaling the number of items in the system. If we need to serve a bigger number of Items, we just increase the number of SearchBolts in the topology and as long as we run this topology in a large enough cluster we can scale memory availability horizontally limitlessly.

It also improves query throughput, because we're executing everything in parallel, thus increasing the amount of computing power that we can use. 

TIP: There are no major differences between the SearchBolt we'll be using here and the one in the previous iteration, and in fact they use exactly the same data structure for items and indexes.

So with these modifications we'll be in a much better situation. Let's see how easy it is to implement it with Storm using the *allGrouping* of the *InputDeclarer* class. 

[source, java]
----
package search;
...
public class SearchEngineTopologyStarter {
	...
	public static StormTopology createTopology() {
		...
		builder.setSpout("queries-spout", new QueriesSpout(), 1);
		builder.setBolt("search", new SearchBolt(), 10).allGrouping("queries-spout")
		...
	}
}
----

And the resulting topology will look like this:

image::figs/ch06-allgrouping.jpg[]

TIP: This picture and source code only represent the input layer of the topology.

==== Merging the results

After each SearchBolt processes an independent part of the query, the resulting information needs to be joined. We'll introduce a MergeBolt, a new component that merges all the SearchBolts' results, keeps the top 5 results and emits a tuple once all the bolts have answered or a timeout is reached. 
This Bolt has two main responsibilities: Holding the search results and identifying the moment at which to emit the final result.

===== Holding the partial responses

The MergeBolt has an in-memory HashMap called *inCourse* that holds the results that have arrived, divided by request. Note that for this Bolt the request id generated by the Node.js server is not enough to identify the request, because as we will soon see, different Node.js servers can repeat the same id, generating collisions. For this reason, the key used for the *inCourse* map is the combination of the *origin* (the origin hostname) and the *requestId* (the internal id from the origin server).

[source, java]
----
...
	HashMap<String, Merger> inCourse= new HashMap<String, Merger>();
...
----

The inner class *Merger* in the bolt has the -quite obvious- responsibility of Merging the results coming from the different shards and discarding all but the top results, according to the sort criteria (which in this case is the price). The main methods of this class are:

[source, java]
----
public static class Merger {
	public void merge(List<Item> newItems);
	public List<Item> getResults();
	public String getRequestId();
	public int getTotalMerged();
}
----

The *merge* method is in charge of merging a set of new results with the previously stored ones. The *getResults* method will return the current top results for this *requestId*. The *getRequestId* method gives you the id of the merged request, and has the same value of the key of this *Merger* instance in the *inCourse* map. The *getTotalMerged* method returns the number of times *merge* has been called so far.

TIP: Note that one Merger is created for each query processed by the system and the merge method is called as many times as the number of SearchBolt instances running in parallel in the topology.


===== Finishing the request

The input for this Bolt is the partial responses and the output is a single unified tuple. As we perform a wait and hold operation, a very important decision we need to make is "When do I stop waiting for new results and emit a tuple?". There are two different situations in which we will finish a request and emit its results: When all parallelized *SearchBolt* instances have finished, or when a timeout is reached.

If all the results have reached this bolt, it's time to emit a result and remove the entry for the current request from the *inCourse* map. The decision is made in the *execute* method.

[source, java]
----
	public void execute(Tuple input) {
		...
		merger.merge(shardResults);

		if(merger.getTotalMerged()>=totalShards)
			finish(merger);
	}
	...
----

TIP: The *totalShard* variable is set in the *prepare* method by reading the parallelism hint. You can take a look at the source code for more detail.

===== Managing Timeouts

When a partial result is delayed we'll ignore it. The system must have a predictable response time, because remember that on the other end of the line there's a user waiting for the screen to load. In the event of an outage of a small part of the system we'd like to have an answer from the available nodes. Even if the response is incomplete it's better than nothing. If the system is highly sharded, the impact of the outage of one shard is small. For example if we break the system into 100 parallel shards only 1% of the results will be missing at the most, and that's assuming that the unavailable shard contains only top ranked results.

We implement timeouts by scheduling a *TimerTask* in the *prepare* method that monitors the age of each *inCourse* request. If the timeout expires it forces a call to the *finish* method in order to end the task and emit the tuple with the results gathered so far.

[source, java]
----
public class MergeBolt implements IRichBolt {
...
	public void prepare(@SuppressWarnings("rawtypes") Map stormConf, TopologyContext context, ...
	...
		TimerTask t= new TimerTask() {
			@Override
			public void run() {
				ArrayList<Merger> mergers;
				
				synchronized (inCourse) {
					mergers= new ArrayList<MergeBolt.Merger>(inCourse.values());
				}
				
				for (Merger merger : mergers) {
					if(merger.getAge()>1000)
						finish(merger);
				}
			}
		};
		Timer timer= new Timer();
		timer.scheduleAtFixedRate(t, 1000, 1000);
	...
	}
}
----

Although the two situations are very different, both end up calling the *finish* method that emits the final result and removes the entry from the *inCourse* structure.

[source, java]
----
...
	protected void finish(Merger merger){
		collector.emit(new Values(merger.getOrigin(), merger.getRequestId(), su.toByteArray(merger.getResults())));
		synchronized (inCourse) {
			inCourse.remove(merger.getId());	
		}
	}
...
----

TIP: Synchronization is necessary because the TimerTask and the *execute* method will be running in different threads, and we need to prevent race conditions.

==== The MergeBolt in the topology

The *MergeBolt* is added to the topology between the *SearchBolt* and the *AnswerQueryBolt*. In the source code below it's parallelized 2 times. It receives Tuples from all the *SearchBolt* instances. As we saw above, in order to be successful a MergeBolt needs to receive all the results for a specific request. For this reason its input is grouped using the *fieldsGrouping* method, and the *origin* server and its internally generated *requestId*. Different instances of the *MergeBolt* will receive different request pieces, but each will receive all pieces of those requests that it handles.

TIP: *AnswerQueryBolt* is just an implementation of *AbstractAnswerBolt*. The only detail introduced in this implementation is that we are parallelizing the task. In our topology we're using two *AnswerQueryBolt* instances. For optimization purposes we group them by *origin*, because these Bolts will manage a connection pool, and for large implementations it's important to make the same Bolts connect to the same group of servers to keep the number of connections under control.

[source, java]
----
package search;
...
public class SearchEngineTopologyStarter {
	...
	public static StormTopology createTopology() {
		...
		builder.setSpout("queries-spout", new QueriesSpout(), 1);
		builder.setBolt("search", new SearchBolt(), 5).allGrouping("queries-spout").allGrouping("read-item-data");
		builder.setBolt("merge", new MergeBolt(), 2).fieldsGrouping("search", new Fields("origin", "requestId"));
		builder.setBolt("answer-query", new AnswerQueryBolt(), 2).fieldsGrouping("merge", new Fields("origin"));
		...
	}
}
----

image::figs/ch06-fields-grouping.jpg[]

TIP: This picture only represents the output layer.

==== Scaling the Web Layer

The web layer is quite easy to scale, we can just add web servers and balance the incoming traffic through them to the client and task list ports. The response port doesn't need to be balanced because the topology will connect directly to the server that is waiting.
It's important to maintain a reasonable balance between the cluster/topology and the web layer size. For example, if you have 10 web servers, having just one instance of the QueriesSpout doesn't make sense.
We can use the Amazon Web Services Elastic Load Balancer with Auto Scaling to implement balancing. We'll go through the steps to create the necessary servers and topology in the next iteration.


==== Indexing

As we mentioned in the introduction to this example, the objective for this search engine is to be updated in real time about all modifications to item data. Let's see how to solve this problem.
We have an external server which is pushing us notifications about items changing in the form of HTTP GETs to a web server or cluster of web servers. In the picture below its represented as *Items news*. We'll run the indexer web server on a different port than the queries web server.
The topology has only three new components. Two of them are the *ItemsNewsFeedSpout* and the *AnswerItemsFeedBolt*, implementations of *AbstractClientSpout* and *AbstractAnswerBolt* that as previously explained are meant to handle the communication between the web server and the topology.
The other component is the *ReadItemDataBolt* in charge of reading the items API to get the full fresh information about a particular item. Remember that the Items News carries only the modified/deleted/inserted item id, not the full information about the item.
After reading the information about an item we will send the details to the *SearchBolt* instances in order for them to refresh their in memory indexes and data structures, and notify the web server about the correct indexing of the item.

==== Complete Search Engine
image::figs/ch06-indexer.jpg[]

We won't go into great detail about the *ItemsNewsFeedSpout* and the *AnswerItemsFeedBolt* because they are identical to the *QueriesSpout* and the *AnswerQueryBolt*. The only differences are the ports used, and that *AnswerItemsFeedBolt* just answers with an "OK" instead of a JSON object.

==== ReadItemDataBolt

===== Spouts
As you can see in the code below, the *ReadItemDataBolt* just reads the tuple data and executes the *readItem* method that will perform the HTTP request to the Items API to get fresh information. After that it will either emit the fresh item information, or null if the item doesn't exist any more.

[source, java]
----
public class ReadItemDataBolt implements IRichBolt {
	...
	@Override
	public void execute(Tuple input) {
		String origin = input.getString(0);
		String requestId = input.getString(1);
		int itemId = Integer.valueOf(input.getString(2));
----
===== Bolts

[source, java]		
----
        Item i;
		try {
			i = readItem(itemId);
			log.debug("Item readed "+ itemId+" ["+i+"]");
			if(i==null) {
				collector.emit(new Values(origin, requestId, itemId, null));
			} else {
				collector.emit(new Values(origin, requestId, itemId, su.itemToByteArray(i)));
			}
		} catch (Exception e) {
			log.error("Error ["+origin+"] ["+requestId+"] ["+itemId+"]", e);
		}
	}
	...
}
----

The *ReadItemDataBolt* blocks while reading the items API, so it's best to parallelize it if it's expected to receive many modifications per second.

===== Topology Building
TIP: Take a look at the source code to view the full implementation of the *readItem* method.

==== Updating the SearchBolt instances

Storm has a feature for this purpose called State Spouts. Unfortunately, this feature isn't available at the time of writing, so we'll implement a work around. It's not a perfect solution at all, but it'll work. We'll review the possible problems with this implementation after explaining it. 
The work around is to send the item information to all the *SearchBolt* instances using the *allGrouping* method in the topology creation, regardless of whether they own the item or not.

[source, java]
----
package search;
...
public class SearchEngineTopologyStarter {
	...
	public static StormTopology createTopology() {
		...
		builder.setBolt("search", new SearchBolt(), 10).allGrouping("queries-spout").allGrouping("read-item-data");
		...
	}
...
}
----

The *SearchBolt* takes the decision about updating it's structure or ignoring the received item using the *isMine* method.

[source, java]
----
package search;
...
public class SearchBolt implements IRichBolt {
	...
	private boolean isMine(int itemId) {
		int remain = itemId % totalShards; 
		return remain == currentShard; 
	}
	...
	@Override
	public void execute(Tuple input) {
		if(input.getSourceComponent().equals("read-item-data")){
			int itemId= input.getInteger(2);
			if(isMine(itemId)){
				log.debug("Mine! "+currentShard+"/"+totalShards);
				byte[] ba = input.getBinary(3);
				if(ba==null) {
					log.debug("Removing item id:"+itemId);
					shard.remove(itemId);
				} else {
					Item i= su.itemFromByteArray(ba);
					log.debug("Updating item index: "+i);
					shard.update(i);
				}
			}
			return ;
		}
		...
	}
...
}
----

TIP: Note that in the first line of the *execute* method we check the source of the input, to identify whether it's an item update or a query.

==== Problems with this approach

There are a few problems with this approach, but the main two are:

1) *SearchBolt* failure: If the process holding a *SearchBolt* instance fails, it won't recover the information about the item it has. It will repopulate as items are changed.

2) *SearchBolt* receives more messages than necessary. The information is sent to all shards, not just the one that will process it. In a highly sharded topology this will lead to a huge overhead.


=== Complete Search Engine

Let's now take a final look at the full topology layout. Keep in mind that in the following diagram there is just one image of each component type, but that many instances of each component can be launched to make the topology scale and work faster. But the diagram is useful to represent the information flow in the system.

image::figs/ch06-complete-topology.jpg[]

=== Testing the Search Engine

We used two types of tests for the search engine: Functional Tests and Unit Tests. Both groups of tests are coded in Groovy. Functional Tests, require a testing environment running and the external services to be mocked in order to run. Unit tests are intended for a more detailed testing of the Helper classes without the context of the full problem.

In this picture we can observe the interaction between the different types of tests, the environment and the mocks. We will go deeper into the Functional tests that require the environment to be setted up and had some Storm related stuff. The unit tests have no difference with other kind of project, you're invited to review those tests source code by yourself.

image::figs/ch06-testing-environment.jpg[]

==== Setting up the environment

There is an script that you must run previous to perform any build action -that will run the tests- in order to start the *External Environment*. This script is called ./prepare-test-environment.sh and perform these three tasks:

* Start the queries web server.
* Start the news feed web server.
* Start the Items API mock server.

Almost any build action that you run using the *mvn* command will execute the tests, but if you just want to test the project you can run the following:

----
> mvn test
----

This command will compile the project and it's related tests and execute them. Those tests are located in the /src/test/groovy/* path. 

----
>tree src/test/groovy/
src/test/groovy/
├── functional
│   ├── AbstractSearchIntegrationTest.groovy
│   ├── EnvironmentSetUpTest.groovy
│   └── SearchTest.groovy
└── unit
    ├── MergerTest.groovy
    └── SearchBucketTests.groovy
----

In the functional directory we have the *AbstractSearchIntegrationTest* class which all the other functional tests implement. 

[source, java]
----
package functional
...
public abstract class AbstractSearchIntegrationTest extends Assert {
	...
    public static topologyStarted = false
    public static sync= new Object()

    @Before
    public void startTopology(){
        synchronized(sync){
            if(!topologyStarted){
                LocalTopologyStarter.main(null);
                topologyStarted = true;
                Thread.sleep(1000);
            }
        }
    }
	...
}
----

The *startTopology* method has the *@Before* annotation indicating JUnit that this method must be called before starting with the tests. This method is in charge of starting a LocalCluster in the the current JVM and deploying the Search Engine Topology into it. Here we can see the *LocalTopologyStarter.main* method implementation:

[source, java]
----
package search.utils;
...
public class LocalTopologyStarter {
    public static void main(String[] args) {
		...
		LocalCluster cluster = new LocalCluster();
		StormTopology topology = SearchEngineTopologyStarter.createTopology();
		Config conf = SearchEngineTopologyStarter.createConf("127.0.0.1:8081", "127.0.0.1:9091", "127.0.0.1:8888", 10);
		conf.setDebug(false);
		cluster.submitTopology("TestTopology", conf, topology);
	}
}
----

It basically do these 4 actions:

* Create a *LocalCluster*.
* Create the topology using the methods we saw in iteration 4.
* Configure the topology to use the local servers that we setted up in the "External Environment".
* Submit the configured topology in the *LocalCluster*.


In the *AbstractSearchIntegrationTest* class we also have the following methods that will help us to implements the tests:

[source, java]
----
package functional
...
public abstract class AbstractSearchIntegrationTest extends Assert {
	...
	public void clearItems() {
	...
    public void addItem(int id, String title, int price) {
	...
    public void removeItem(int id) {
	...
    public Object readItem(int id) {
	...
    public void notifyItemChange(int id) {
	...
    public Object search(String query) {
	...
}
----

All the functional tests extend this class, so they will have this methods available to assert something. The *clearItems* method makes an HTTP call to the Items API Mock clearing all the items it keeps in memory. The *addItem* method, adds an item to the Items API Mock memory for future tests. The *removeItem* and *readItem* methods removes and reads items information from the same mock respectively. The *notifyItemChange* performs an HTTP call to the news receiving web server that will notify the topology about the change. And the *search* method receives a query and perform the HTTP GET to the queries web server and returns the search results.

TIP: You can browse the source code of this class for deeper detail.

==== Implementing functional tests

With all this stuff, we're ready to test! Let's take a look at the *SearchTest* class and pick the *newsFeedTest* and see how it's implemented.

[source, java]
----
package functional
...
public class SearchTest extends AbstractSearchIntegrationTest {

    @Test
    public void newsFeedTest() {
        // Verify Empty
        def result = search("new")
        assertEquals(result.size(), 0)

        // Publish items
        addItem(1, "new dvd player", 100)
        addItem(2, "new digital camera", 80)
        addItem(3, "new laptop computer", 70)
        notifyItemChange(1)
        notifyItemChange(2)
        notifyItemChange(3)

        result = search("drive")
        assertEquals(0, result.size())

        // Verify a query with results
        result = search("new")
        assertEquals(3, result.size())

        // Delete & modify items
        removeItem(1)
        addItem(1, "new dvd player just sold", 100)
        removeItem(2)
        removeItem(3)
        notifyItemChange(1)
        notifyItemChange(2)
        notifyItemChange(3)

        result = search("new")
        assertEquals(1, result.size())

        result = search("sold")
        assertEquals(1, result.size())

        result = search("new-dvd-player-just-sold")
        assertEquals(1, result.size())
    }
	...
}
----

This method basically performs assertions of expected behavior using the *AbstractSearchIntegrationTest* methods to interact with the testing environment. Explained in english the above source code replicates the following use case:

* Search for items with the word "new": It's expected a 0 sized response, because we didn't add items to the Search Engine yet.
* Add 3 items to the Search Engine and perform two searches asserting the number of results answered by the system.
* Remove and add a few items and assert 3 new search results.

TIP: It's usually not recommended to perform several tests in the same method, we made it that way to simplify the explanation. Ideally we should create three methods to test the add, remove and modify operations at least.

=== Full Hardware Layout and EC2 deployment

In this iteration the idea is to show you the full hardware stack for a production implementation

image::figs/ch06-hardwarelayout.jpg[]

TODO: Link to appendix installing Storm-deploy, setting up your amazon account.

=== Creating the Storm Cluster

TODO: Setup of conf/clusters.yaml According desired to cluster size.

Starting:

lein run :deploy --start --name SearchEngine --release 0.6.2

Stoping:

lein run :deploy --stop --name SearchEngine


==== Deploying the WebServer layer

TODO: Setup of node.js servers

===== Elastic Load Balancers

TODO Setting up elbs

==== Deploying the Topology

TODO Deploy in the cluster

