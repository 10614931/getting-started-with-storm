[[a_real_life_example]]
= A Real Life Example

== Introduction

The idea of this chapter is to illustrate a quite different Architecture and use case of storm. In the most general use case for storm is to receive a huge amount of information through the spouts, transform it using a chain of bolts (generally to join or aggregate this data) and finally store the transformed information.

Typical applications for this architecture are:

* Real time Twitter feeds analysis.
* Real time WebSite performance analysis.
* Stock markets information analysis.

But in this chapter we will change those characters and bring into life a completely different set of applications. Lets make the topology receive queries through the spouts and process them using a set of bolts that will take advantage of distributed CPU and maybe data locality. And in the end another set of bolts will send the response back to the user that requested it.

This architecture is called DRPC (Distributed Remote Process Communication) and maybe one of the mosts attractive algorithms to implement using this architecture is the Google's Map Reduce.

In this chapter we will explore the idea of large e-commerce platform search engine implemented using a Storm Topology and a Node.Js web server.

The most important requirement is: given a query request i need to search through a huge amount of data in real time and bring the information back to the user immediately.  Another constraint is that items information changes all the time, and the search DB needs to be updated.

For example, we query our system to look for an mp3 player:

----
> curl http://localhost/mp3
[
     {
          "id": 1,
          "title": "mp3 player 16Gb",
          "price": 750
     },
     {
          "id": 7,
          "title": "mp3 player 8Gb",
          "price": 450
     }
]
----


We will walk through this chapter implementing a solution for this problem using Storm.

=== Requirements:
TODO: Download the sources:
git clone ...
TODO: Install Node.Js
node.org

== 1st Iteration: The Client-Client problem (The selfish guy).

As we said, we'll read the queries using Spouts. But Spouts are not designed to be WebServers they are more likely to be clients. This is because this isn't easy to locate them, the deploy automatically in the cluster and maybe can be automatically reallocated during runtime. So it's much easier to implement a pull client to get the tasts, than to let other actors in the system to push information to us directly.

So, here we have the first problem we need to solve: We need an intermediate WebServer as an adapter in order to establish a client-client communication. As this isn't the main problem we want to deal with in this book we implemented and shipped an implementation of a server behaving like that. (It's path: node-js-server/node-drpc-server.js).

We won't go in deep detail of how it is implemented, but we will play with this server in order to understand "What it does".

In this iteration, we will use a simple client server example: One client (asking for money) and one server (denying that money) and make them behave like two clients interacting through the server. Maybe the following picture helps you to figure it out: 


image::figs/ch06-im-not-a-bank.jpg[]

=== The Node.JS WebServer
The server listens to HTTP protocol in three different ports, each of them serving for different porpouses:

==== Port 8080: 
The real client connections. Here is where the guy making the questions connects to:

----
> curl localhost:8080/please-give-me-some-money
----

==== Port 8081: 
Is where the client interested in answering something connects to get the questions issued by the previous one.

----
> curl localhost:8081/
127.0.0.1
0
/please-give-me-some-money
----


==== Port 8082: 
Is where the topology pushes the answer back to.

----
> curl -X 'POST' --data "We're not a bank" localhost:8082/?id=0
----

==== Playing with the server:
We strongly suggest you to play with this server for a while in order to understand how it works. Here we list a few experiments that you can do:

1) Try opening a few terminals and excecute different requests to the port 8080. After that curl the 8081 and you'll see all the queries you made with an assigned ID. After that you can POST some content to the port 8082 with the issued ID and see how the different curls you've made to port 8080 unblocks with the content you posted.

2) You can try doing the GET to the 8081 befor issuing queries to the 8080. And you'll see how it blocks until you execute a GET to the 8080.

TIP: To start the server run: node ./node-js-server/node-drpc-server.js in the root of the example directory.


== 2nd Iteration: The Storm Client Iteration.

But the big question now is: What is the relation between those curls and the selfish guy with a storm topology? And also: What is the relation between these and a search engine?. Ok, in this iteration we will focus in the first question, and let the second one for the next.

As we've said, topologies (Spouts & Bolts) can behave as clients, but the guy who makes query two. So, if we want the topology to behave as a server we need the adapter that we discussed in the previous iteration.

So, here we go, lets implement the same example of the selfish guy as a storm topology:

image::figs/ch06-selfish-topology.jpg[]

You can download the sources of this example from: TODO (REPO LINK)

If you want to start the topology




=== Topology components

==== ClientSpout:

As we saw in previous chapters, Spouts are used to feed the topology with data. In this case, data is in the form of user requests that needs to be processed by the topology.

In this specific case, the client Spout is the one which reads pending transactions from the WebServer. Once transaction is read, it emmits the tuple, in order for the cluster to manage this tuple according its defined in the topology.

We use the HttpClient library to get the pending requests from the WebServer executing an HTTP GET method in the port 8081. After that we parse the response and emmit the tuples. As we will widely use this type of Spout in the next iterations, we've created an abstract class called AbstractClientSpout and here we subclass it to implement this particular case specifying only that for this purpouse we will look for pending requests in a localhost server listening in the 8081 port, and that we will accept a maximum of ten requests in one GET excecution at the most.

The AbstractClientSpout subclassing for this example looks like this:

[source, java]
----
...
public class ClientSpout extends AbstractClientSpout {
	@Override
	protected String getPullHost() {
		return "localhost:8081";
	}

	@Override
	protected int getMaxPull() {
		return 10;
	}
}
----

But, we are hiding a lot of interesting details here, let's take a look at the nextTuple() method implemented in the abstract class

[source, java]
----
...
	transient LinkedBlockingQueue<Request> pendingRequests;

	@Override
	public void nextTuple() {
		Request req;
		try {
			req = pendingRequests.poll(1, TimeUnit.SECONDS);
			collector.emit(new Values(req.origin, req.id, req.content));
		} catch (InterruptedException e) {
			log.error("Polling interrrupted", e);
		}
	}
...
----

As we can notice, there is no action being executed directly here. It is not recommended to run large blocking operations in this method becouse as we saw in previous chapters, this is called in a loop with the ack and fail methods and blocking it will make the other operations to starv. This is the reason why we read from a LinkedBlockingQueue and execute the concrete pulling operation inside a thread in the open method.

TIP: Notice that we're not anchoring the tuples, so, no message delivery warranties are applied in those examples.

[source, java]
----
...
	@Override
	public void open(@SuppressWarnings("rawtypes") Map conf, TopologyContext context,
			SpoutOutputCollector collector) {
		log = Logger.getLogger(this.getClass());
		this.conf= conf;
		this.context= context;
		this.collector= collector;
		this.pendingRequests = new LinkedBlockingQueue<Request>(1000);
		this.isDying = false;
		reconnect();
		t = new Thread("ClientSpout["+this.getClass().getName()+"] pulling:"+"http://"+getPullHost()+"/?max="+getMaxPull()){
			@Override
			public void run() {
				while(true) {
					try {
						if(isDying)
							return ;
						executeGet();
					} catch(Throwable t) {
						log.error("Error in executeGet", t);
						try {Thread.sleep(1000);} catch (InterruptedException e) {}
					}
				}
			}
		};
		t.start();
	}
	
	...

	public void executeGet(){
		HttpResponse response;
		BufferedReader reader= null;
		try {
			response = httpclient.execute(httpget);
			HttpEntity entity = response.getEntity();
			reader= new BufferedReader(new InputStreamReader(entity.getContent()));
			String origin= reader.readLine();
			while(true) {
				Request req= new Request();
				req.origin = origin;
				req.id = reader.readLine();
				req.content = reader.readLine();
				
				if(req.id == null || req.content==null)
					break;
				else {
					req.content = req.content.substring(1);
					boolean inserted = pendingRequests.offer(req, 10, TimeUnit.SECONDS);
					if(!inserted) 
						log.error("pendingRequests queue is full, discarding request ["+req+"]");
				}
			} 
		} catch (Exception e) {
		...
----

The executeGet method called in a loop inside the mentioned thread is the one who is in charge of running the operation itself feeding the in memory queue.

So, wrapping up, the ClientSpout is an implementation of the AbstractClientSpout which has two abstract methods 


==== SelfishBolt:
In previous chapters we've used Bolts to do the required work with the data feeded by the spouts, and we played a bit with topologies distribution abilities. In this iteration, processing is quite simple, we just need to write answer the WebServer with a predefined answer. So, there is no complex computation at all, but we use it to execute an HTTP POST to the 8082 port of the WebServer using the same library than before.
Again, as we're going to implement several spouts like this during this chapter we created an abstraction, the AbstractAnswerBolt which we extend in order to implement the Bolt we need.

[source, java]
----
public class AnswerBolt extends AbstractAnswerBolt {
	private static final long serialVersionUID = 1L;
	
	@Override
	public void execute(Tuple input) {
		String origin= input.getString(0);
		String requestId= input.getString(1);
		sendBack(origin, requestId, "I'm not a bank!");
	}

	@Override
	protected int getDestinationPort() {
		return 8082;
	}
}
----

The most important implementation missing in the previous piece of code is the sendBack method which is in charge of sending the response back to the WebServer. Here is its implementation:

[source, java]
----
...
	protected void sendBack(String origin, String id, String content){
		String to= "http://"+origin+":"+getDestinationPort()+"/?id="+id;
		log.debug("Answering to:"+to);
		HttpPost post= new HttpPost(to);
		try {
			StringEntity entity= new StringEntity(content);
			post.setEntity(entity);
			HttpResponse response= client.execute(post);
			InputStream is= response.getEntity().getContent();
			is.close();			
		} catch (Exception e) {
			log.error("Answering to:["+to+"] with ["+content+"]", e);
		}
	}
...
----
=== Running the example and playing with it

You can run this example by opening two ssh terminals and running the following from the example path:

==== First terminal: Run the server and the topology
----
>./prepare-test-environment.sh 
>mvn package
...  
	Lots of standard outputs...
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 32.480s
[INFO] Finished at: Tue Feb 07 01:51:22 GMT-03:00 2012
[INFO] Final Memory: 11M/81M
[INFO] ------------------------------------------------------------------------
>cd bin
>./run_selfish_topology.sh 
----

==== Second terminal: Run the query as a user of the system
----
>curl localhost:8080/where-is-the-money
I'm not a bank!
>
----


== 3rd Iteration: Searching Items.

Now that we understood the details of how the communication between the Client, the WebServer and the Storm Topology works we'll work in the search engine itself.

If we're going to search for something we need a few things: The information where we will perform the search, A criteria to search this information, an efficient datastructure to solve the searching problem, and an efficient computation distribution layout in order for this query to return fast.

=== The information:

We have an items REST API which provides us with the items information on demand. As the items information changes all the time, and we have the requirement to be 100% up to date someone will notify us when an item changes some of its information executing an HTTP GET int the URL we provide.

     * This notification mechanism is widely used in WebPlatforms APIs like facebook graph API.
     * A Mock server of the items REST API is provided with the source code.

TODO: Playing with the MOCK server.

=== The search criteria:

Our simple search engine, will apply a very simple criteria. The query will be a set of words which should be contained in the title of every item that matches in the results.

For simplicity, the search string will be case sensitive, required words will be separated by "-" and we'll not use any kind of steeming nor soft match algorithm neither.

=== The datastructure:

There is a very simple and efficient datastructure to search for words in a collection, it's called "Inverted Index" and we'll be implementing one in this iteration.

TODO: Explain the inverted index.


=== The computation distribution layout:

This is a very important issue, may be, the most important for the topic of this book. For this reason we will leave this issue till the next iteration which will focus exactly on that.


=== Implementation:

image::figs/ch06-simple-search-engine.jpg[]


TODO: Show simple-search most important source code pieces and explain components.


== 4th Iteration: Shard & Distribute


image::figs/ch06-complete-topology.jpg[]


== 5th Iteration: Deploy in Amazon EC2


== 6th Iteration: Scale!




