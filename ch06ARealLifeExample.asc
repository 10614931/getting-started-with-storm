[[a_real_life_example]]
= A Real Life Example

== Introduction

The idea of this chapter is to illustrate a quite different Architecture and use case of storm. In the most general use case for storm is to receive a huge amount of information through the spouts, transform it using a chain of bolts (generally to join or aggregate this data) and finally store the transformed information.

Typical applications for this architecture are:

* Real time Twitter feeds analysis.
* Real time WebSite performance analysis.
* Stock markets information analysis.

But in this chapter we will change those characters and bring into life a completely different set of applications. Lets make the topology receive queries through the spouts and process them using a set of bolts that will take advantage of distributed CPU and maybe data locality. And in the end another set of bolts will send the response back to the user that requested it.

This architecture is called DRPC (Distributed Remote Process Communication) and maybe one of the mosts attractive algorithms to implement using this architecture is the Google's Map Reduce.

In this chapter we will explore the idea of large e-commerce platform search engine implemented using a Storm Topology and a Node.Js web server.

The most important requirement is: given a query request i need to search through a huge amount of data in real time and bring the information back to the user immediately.  Another constraint is that items information changes all the time, and the search DB needs to be updated.

For example, we query our system to look for an mp3 player:

----
> curl http://localhost/mp3
[
     {
          "id": 1,
          "title": "mp3 player 16Gb",
          "price": 750
     },
     {
          "id": 7,
          "title": "mp3 player 8Gb",
          "price": 450
     }
]
----


We will walk through this chapter implementing a solution for this problem using Storm.

=== Requirements:
TODO: Download the sources:
git clone ...
TODO: Install Node.Js
node.org

== 1st Iteration: The Client-Client problem (The selfish guy).

As we said, we'll read the queries using Spouts. But Spouts are not designed to be WebServers they are more likely to be clients. This is because this isn't easy to locate them, the deploy automatically in the cluster and maybe can be automatically reallocated during runtime. So it's much easier to implement a pull client to get the tasts, than to let other actors in the system to push information to us directly.

So, here we have the first problem we need to solve: We need an intermediate WebServer as an adapter in order to establish a client-client communication. As this isn't the main problem we want to deal with in this book we implemented and shipped an implementation of a server behaving like that. (It's path: node-js-server/node-drpc-server.js).

We won't go in deep detail of how it is implemented, but we will play with this server in order to understand "What it does".

In this iteration, we will use a simple client server example: One client (asking for money) and one server (denying that money) and make them behave like two clients interacting through the server. Maybe the following picture helps you to figure it out: 


image::figs/ch06-im-not-a-bank.jpg[]

=== The Node.JS WebServer
The server listens to HTTP protocol in three different ports, each of them serving for different porpouses:

==== Port 8080: 
The real client connections. Here is where the guy making the questions connects to:

----
> curl localhost:8080/please-give-me-some-money
----

==== Port 8081: 
Is where the client interested in answering something connects to get the questions issued by the previous one.

----
> curl localhost:8081/
127.0.0.1
0
/please-give-me-some-money
----


==== Port 8082: 
Is where the topology pushes the answer back to.

----
> curl -X 'POST' --data "We're not a bank" localhost:8082/?id=0
----

==== Playing with the server:
We strongly suggest you to play with this server for a while in order to understand how it works. Here we list a few experiments that you can do:

1) Try opening a few terminals and excecute different requests to the port 8080. After that curl the 8081 and you'll see all the queries you made with an assigned ID. After that you can POST some content to the port 8082 with the issued ID and see how the different curls you've made to port 8080 unblocks with the content you posted.

2) You can try doing the GET to the 8081 befor issuing queries to the 8080. And you'll see how it blocks until you execute a GET to the 8080.

TIP: To start the server run: node ./node-js-server/node-drpc-server.js in the root of the example directory.


== 2nd Iteration: The Storm Client Iteration.

But the big question now is: What is the relation between those curls and the selfish guy with a storm topology? And also: What is the relation between these and a search engine?. Ok, in this iteration we will focus in the first question, and let the second one for the next.

As we've said, topologies (Spouts & Bolts) can behave as clients, but the guy who makes query two. So, if we want the topology to behave as a server we need the adapter that we discussed in the previous iteration.

So, here we go, lets implement the same example of the selfish guy as a storm topology:

image::figs/ch06-selfish-topology.jpg[]

You can download the sources of this example from: TODO (REPO LINK)

If you want to start the topology




=== Topology components

==== ClientSpout:

As we saw in previous chapters, Spouts are used to feed the topology with data. In this case, data is in the form of user requests that needs to be processed by the topology.

In this specific case, the client Spout is the one which reads pending transactions from the WebServer. Once transaction is read, it emmits the tuple, in order for the cluster to manage this tuple according its defined in the topology.

We use the HttpClient library to get the pending requests from the WebServer executing an HTTP GET method in the port 8081. After that we parse the response and emmit the tuples. As we will widely use this type of Spout in the next iterations, we've created an abstract class called AbstractClientSpout and here we subclass it to implement this particular case specifying only that for this purpouse we will look for pending requests in a localhost server listening in the 8081 port, and that we will accept a maximum of ten requests in one GET excecution at the most.

The AbstractClientSpout subclassing for this example looks like this:

[source, java]
----
package selfish;
...
public class ClientSpout extends AbstractClientSpout {
	@Override
	protected String getPullHost() {
		return "localhost:8081";
	}

	@Override
	protected int getMaxPull() {
		return 10;
	}
}
----

But, we are hiding a lot of interesting details here, let's take a look at the nextTuple() method implemented in the abstract class

[source, java]
----
package storm.utils;
...
public abstract class AbstractClientSpout implements IRichSpout {
...
	transient LinkedBlockingQueue<Request> pendingRequests;

	@Override
	public void nextTuple() {
		Request req;
		try {
			req = pendingRequests.poll(1, TimeUnit.SECONDS);
			collector.emit(new Values(req.origin, req.id, req.content));
		} catch (InterruptedException e) {
			log.error("Polling interrrupted", e);
		}
	}
...
----

As we can notice, there is no action being executed directly here. It is not recommended to run large blocking operations in this method becouse as we saw in previous chapters, this is called in a loop with the ack and fail methods and blocking it will make the other operations to starv. This is the reason why we read from a LinkedBlockingQueue and execute the concrete pulling operation inside a thread in the open method.

TIP: Notice that we're not anchoring the tuples, so, no message delivery warranties are applied in those examples.

[source, java]
----
package storm.utils;
...
public abstract class AbstractClientSpout implements IRichSpout {
...
	@Override
	public void open(@SuppressWarnings("rawtypes") Map conf, TopologyContext context,
			SpoutOutputCollector collector) {
		log = Logger.getLogger(this.getClass());
		this.conf= conf;
		this.context= context;
		this.collector= collector;
		this.pendingRequests = new LinkedBlockingQueue<Request>(1000);
		this.isDying = false;
		reconnect();
		t = new Thread("ClientSpout["+this.getClass().getName()+"] pulling:"+"http://"+getPullHost()+"/?max="+getMaxPull()){
			@Override
			public void run() {
				while(true) {
					try {
						if(isDying)
							return ;
						executeGet();
					} catch(Throwable t) {
						log.error("Error in executeGet", t);
						try {Thread.sleep(1000);} catch (InterruptedException e) {}
					}
				}
			}
		};
		t.start();
	}
	
	...

	public void executeGet(){
		HttpResponse response;
		BufferedReader reader= null;
		try {
			response = httpclient.execute(httpget);
			HttpEntity entity = response.getEntity();
			reader= new BufferedReader(new InputStreamReader(entity.getContent()));
			String origin= reader.readLine();
			while(true) {
				Request req= new Request();
				req.origin = origin;
				req.id = reader.readLine();
				req.content = reader.readLine();
				
				if(req.id == null || req.content==null)
					break;
				else {
					req.content = req.content.substring(1);
					boolean inserted = pendingRequests.offer(req, 10, TimeUnit.SECONDS);
					if(!inserted) 
						log.error("pendingRequests queue is full, discarding request ["+req+"]");
				}
			} 
		} catch (Exception e) {
		...
----

The executeGet method called in a loop inside the mentioned thread is the one who is in charge of running the operation itself feeding the in memory queue.

So, wrapping up, the ClientSpout is an implementation of the AbstractClientSpout which has two abstract methods 


==== SelfishBolt:
In previous chapters we've used Bolts to do the required work with the data feeded by the spouts, and we played a bit with topologies distribution abilities. In this iteration, processing is quite simple, we just need to write answer the WebServer with a predefined answer. So, there is no complex computation at all, but we use it to execute an HTTP POST to the 8082 port of the WebServer using the same library than before.
Again, as we're going to implement several spouts like this during this chapter we created an abstraction, the AbstractAnswerBolt which we extend in order to implement the Bolt we need.

[source, java]
----
package selfish;
...
public class AnswerBolt extends AbstractAnswerBolt {
	private static final long serialVersionUID = 1L;
	
	@Override
	public void execute(Tuple input) {
		String origin= input.getString(0);
		String requestId= input.getString(1);
		sendBack(origin, requestId, "I'm not a bank!");
	}

	@Override
	protected int getDestinationPort() {
		return 8082;
	}
}
----

The most important implementation missing in the previous piece of code is the sendBack method which is in charge of sending the response back to the WebServer. Here is its implementation:

[source, java]
----
...
package storm.utils;
...
	protected void sendBack(String origin, String id, String content){
		String to= "http://"+origin+":"+getDestinationPort()+"/?id="+id;
		log.debug("Answering to:"+to);
		HttpPost post= new HttpPost(to);
		try {
			StringEntity entity= new StringEntity(content);
			post.setEntity(entity);
			HttpResponse response= client.execute(post);
			InputStream is= response.getEntity().getContent();
			is.close();			
		} catch (Exception e) {
			log.error("Answering to:["+to+"] with ["+content+"]", e);
		}
	}
...
----
==== The Topology 

The topology for this example is very simple. We just need to connect both spouts, and in this case we will just run it in a LocalCluster.

[source, java]
----
package selfish;
...
public class SelfishTopologyStarter {
	public static StormTopology createTopology() {
		TopologyBuilder builder= new TopologyBuilder();
		builder.setSpout("questions-spout", new ClientSpout(), 1);
		builder.setBolt("answers-bolt", new AnswerBolt(), 1).allGrouping("questions-spout");
		return builder.createTopology();
	}
	
	
	public static void main(String[] args) {
		Logger.getRootLogger().setLevel(Level.ERROR);
		Logger.getLogger("org.apache.http.wire").setLevel(Level.ERROR);
		Logger.getLogger("selfish").setLevel(Level.DEBUG);
		
		LocalCluster cluster = new LocalCluster();
		StormTopology topology = createTopology();
		cluster.submitTopology("Test-Topology", new Config(), topology);
	}
}
----


=== Running the example and playing with it

You can run this example by opening two ssh terminals and running the following from the example path:

==== First terminal: Run the server and the topology
----
>./prepare-test-environment.sh 
>mvn package
...  
	Lots of standard outputs...
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 32.480s
[INFO] Finished at: Tue Feb 07 01:51:22 GMT-03:00 2012
[INFO] Final Memory: 11M/81M
[INFO] ------------------------------------------------------------------------
>cd bin
>./run_selfish_topology.sh 
----

TIP: You can take a look at this script in the repository to understand how are the topology and the server launched.

==== Second terminal: Run the query as a user of the system
----
>curl localhost:8080/where-is-the-money
I'm not a bank!
>
----


== 3rd Iteration: Searching Items.

Now that we understood the details of how the communication between the Client, the WebServer and the Storm Topology works we'll work in the search engine itself.

If we're going to search for something we need a few things: The information where we will perform the search, A criteria to search this information, an efficient datastructure to solve the searching problem, and an efficient computation distribution layout in order for this query to return fast.

=== The information:

We have an items REST API which provides us with the items information on demand. As the items information changes all the time, and we have the requirement to be 100% up to date someone will notify us when an item changes some of its information executing an HTTP GET int the URL we provide.

     * This notification mechanism is widely used in WebPlatforms APIs like facebook graph API.
     * A Mock server of the items REST API is provided with the source code.

For this iteration we will skip this issues and will just hardcode some items in the Bolt. In further iterations we will cover the full indexing problem.

=== The search criteria:

Our simple search engine, will apply a very simple criteria. The query will be a set of words which should be contained in the title of every item that matches in the results and sort them by price in a descendent order.

For simplicity, the search string will be case sensitive, required words will be separated by "-" and we'll not use any kind of steeming nor soft match algorithm neither.

=== The datastructure:

There is a very simple and efficient datastructure to search for words in a collection, it's called "Inverted Index" and we'll be implementing one in this iteration. We've implemented a quite simple ItemsContainer and an Item class that solves the searching problem by storing the items and the inverted index in memory.

==== The Item entity

A very simple class with public fields representing the item information.
[source, java]
----
package search.model;
...
public class Item implements Serializable {
...
	public long id;
	public String title;
	public double price;
...
----

TIP: Check the full implementation to get some details, like the hashCode and the equals implementations.

==== The ItemContainer class

This class contains the in-memory inverted index and items information. The most important methods are those which allows us to modify the structures (add, remove, update) and to search into it (getItemsContainingWords):

[source, java]
----
package search.model;
...
public class ItemsContainer {
...
	public void add(Item i){
	public void update(Item i){
	public void remove(int itemId){
	public void remove(Item i){
	public Set<Item> getItemsContainingWords(String words){
...
----

TIP: If you want to see how it works in deep detail we suggest you to view the source in the repository.

=== The computation distribution layout:

This is a very important issue, may be, the most important for the topic of this book. For this reason we will leave this issue till the next iteration which will focus exactly on that.


=== Components

image::figs/ch06-simple-search-engine.jpg[]


==== Topology Layout

As we can see in the previous figure, the topology layout is very simple, it's builded with the following code:

[source, java]
----
package simplesearch;
...
public class SimpleSearchEngineTopologyStarter {
	public static StormTopology createTopology() {
		TopologyBuilder builder= new TopologyBuilder();

		builder.setSpout("queries-spout", new QueriesSpout(), 1);
		builder.setBolt("queries-processor", new SearchBolt(), 1).allGrouping("queries-spout");
		builder.setBolt("answer-query", new AnswerQueryBolt(), 2).allGrouping("queries-processor");
		return builder.createTopology();
	}
...
----

==== The QueriesSpout

This is an implementation of the AbstractClientSpout that we used and explained in the previoud iteration. In charge of feeding the topology with new work. There are no big news in that class, we suggest you to see the source code by yourself.

==== The AnswerQueryBolt

The AnswerQueryBolt is quite similar to the previouse iteration AnswerBolt. But now, we generate a JSON output with the search results using json-simple lib.

[source, java]
----
package simplesearch;
...
public class AnswerQueryBolt extends AbstractAnswerBolt {
...
	public void execute(Tuple input) {
		String origin= input.getString(0);
		String requestId= input.getString(1);
		List<Item> finalResult= su.fromByteArray(input.getBinary(2));
		
		JSONArray list = new JSONArray();
		for (Item item : finalResult) {
			JSONObject obj= new JSONObject();
			obj.put("title", item.title);
			obj.put("id", item.id);
			obj.put("price", item.price);
			list.add(obj);
		}
		String json= JSONValue.toJSONString(list);
		sendBack(origin, requestId, json);
	}
...
----

=== The SearchBolt

This is the Bolt in charge of excecuting the search itself and emmiting the top 5 results. For simplicity in this iteration, we cutted indexing features off, so the items are hardcoded in the populateItems method which is called from the Bolt's prepare.
The execute method get the query information needed, executes the query and emmit the results that will be sent to the AnswerQueryBolt. Note that all the searching stuff is done inside of the ItemsContainer class, after that we sort and cut the results to five items.

[source, java]
----
package simplesearch;
...
public class SearchBolt implements IRichBolt {
...
	ItemsContainer myItems;
...
	private void populateItems() {
		myItems.add(new Item(0, "old dvd player", Math.random()*Math.random()*100));
		myItems.add(new Item(1, "new cell phone", Math.random()*100));
		myItems.add(new Item(2, "old pirates coin", Math.random()*100));
		myItems.add(new Item(3, "fashion sun glasses", Math.random()*100));
...
	@Override
	public void execute(Tuple input) {
		String origin= input.getString(0);
		String requestId= input.getString(1);
		String query= input.getString(2);
		List<Item> results= executeQuery(query, 5);
		collector.emit(new Values(origin, requestId, su.toByteArray(results)));
	}
...
	private List<Item> executeQuery(String query, int quantity) {
		List<Item> items= new ArrayList<Item>(myItems.getItemsContainingWords(query));
		Collections.sort(items, new Comparator<Item>() {
			@Override
			public int compare(Item o1, Item o2) {
				double diff= o1.price-o2.price;
				if(diff>0)
					return -1;
				else
					return 1;
			}
		});
		
		if(items.size()>quantity)
			items = items.subList(0, quantity-1);
		return items;
	}
...
----


=== Problems of this approach

This is a very very simple approach to a search engine. But it faces a few sever problems for a real use case. 

* Items volume scalability.
* Throwput scalability.
* Information can't be updated.

In the next iteration we will focus in solving those problems, and really taking advantage of storm features to do so.

== 4th Iteration: Shard & Distribute

=== Scaling the Storm Topology

	TODO: Write

=== Scaling the WebLayer

	TODO: Write

=== Indexing

	TODO: Write

=== Complete Search Engine

	TODO: Write

image::figs/ch06-complete-topology.jpg[]

==== Spouts

	TODO: Write

==== Bolts

	TODO: Write

==== Topology Building

	TODO: Write

== 5th Iteration: Deploy in Amazon EC2

	TODO; Drawing full implementation
	TODO: Write

=== Deploying the Topology

	TODO: Write

==== The storm deploy project

	TODO: Write

=== Deploying the WebServer layer

	TODO: Write

==== WebServers creation

	TODO: Write

==== Elastic Load Balancer

	TODO: Write
