[[topologies]]
== Topologies
In this chapter we'll see more in depth how all our objects are glued and how we can do to deploy our topology into a running storm cluster

=== Streams Grouping 
Ones of the most important things that we need to do when we design a topology is define how the data will be exchanged between the nodes (how the streams will be consumed by the nodes), to do that we'll use the streams grouping which can be set to an *InputDeclarer*, with these streams groping we'll specify which stream will be consumed by the input declarer (normaly the input declared will be the representation of our bolts into the topology) and how the stream will be consumed. 

TIP: One node could emit more than one *stream* of data, the streams grouping enable us to select what stream we want to receive if we have more than one.

The way to set the streams grouping will be in the topology definition as we have saw in the <<getting_started,Getting Started>> chapter:

[source, java]
----
....
    builder.setBolt("word-normalizer", new WordNormalizer())
        .shuffleGrouping("word-reader");
....
----
Where we set a new bolt to the topology builder (the builder object) and after that we add a new source using some stream grouping, shuffleGrouping in this case. Depends on the grouping kind this will take diferent amount of parameters but normally the grouping will take the component id source, where consume the stream.

TIP: We can have more than one source per each *InputDeclarer* and each source could be grouped with a different stream grouping

[[shuffle_grouping]]
==== Shuffle Grouping
We can label the Shuffle Grouping as the default grouping. As we saw into the example this grouping only take one parameter (the source componente), with this grouping each tuple emited by the source will be send in a randomly fashion to the bolt without have the capacity of control what tuple is sent to each bolt. 

The shuffle grouping is excellent when we need to do atomic operations, for example, if we need to do a math operation with the data of tuple and emit the result, is very probable that we want use a Shuffle Grouping but if we need count words, we can’t use this because we’ll need to save state (in this case the word) to count it and as we saw in the chapter 2 is very probable that one word will be received by two bolts and the word will be counted in more than one time.

==== Fields Grouping
The Fields Grouping enable us to control how the tuples are sent to the bolts based on one or more field of the tuple, this grouping will guarantee us that ever than a combination of values of the fields specified in a tuple would be repeated, this tuple will be sent to the same bolt. Comming back to the count word example, if we specify as grouping the field "word" (like we can see below) we'll make that ever the *word-normalizer* bolt send a tuple with one word, this word will be send to the same instance of *word-counter* bolt.

[source,java]
----
....
builder.setBolt("word-counter", new WordCounter(),2)
    .fieldsGrouping("word-normalizer", new Fields("word"));
....
----

TIP: All fields set into the fields grouping must be exist into the sources's fields declarer

==== All Grouping
This group will do an *explode* of the infomation sending the touple to all instances of the bolt that require the stream, it's mean that when we use the *All Groping* one copy of the tuple will be receive by each instance of the bolt. This kind of groping is perfect when we need to send *signals* to our bolts, for example if we need refresh a cache we can send a *refresh cache signal* to our bolts. Here we can see the example modifing the word-count add the capability of clear the *counter* cache (we can found this example running at link:https://github.com/storm-book/examples-ch03-topologies[Topologies Example])

Here we have the mofification to the word-counter bolt:
[source,java]
----
public void execute(Tuple input) {
            String str = null; 
            try{
                str = input.getStringByField("word");
            }catch (IllegalArgumentException e) {
                //Do nothing
            }
            
            if(str!=null){
                ....
            }else{
                str = input.getStringByField("action");
                if("refreshCache".equals(str))
                    counters.clear();
            }
        //Set the tuple as Acknowledge
        collector.ack(input);
}
----

As we can see we've added an "if" to check if the word field is into the tuple, if not we'll asume that is an action.

So in the topology definition we'll specify to the word-counter bolt that it has two streams and the signals-spout stream must be sent to all count-word bolts as we can see below:

[source,java]
----
builder.setBolt("word-counter", new WordCounter(),2)
            .fieldsGrouping("word-normalizer", new Fields("word"))
            .allGrouping("signals-spout");
----

The implementation of signals-spout can be found into the link:https://github.com/storm-book/examples-ch03-topologies[git repository]

==== Custom Grouping
With the custom grouping we'll have de capability to create our own streams grouping implementing the *backtype.storm.grouping.CustomStreamGrouping* interface, using custom groupings we'll have the power to decide on each tuple which (zero, one or more) bolt will receive the tuple. Using other time our word count example we'll modfy it to group by the hash of the first leter of the word, so all words that start with the same leter will be received by the same bolt.

[source,java]
----
        builder.setBolt("word-counter", new WordCounter(),2)
            .customGrouping("word-normalizer", new CustomStreamGrouping() {
                int numTasks=0;
                @Override
                public List<Integer> taskIndices(Tuple tuple) {
                    List<Integer> boltsIds = new ArrayList<Integer>();
                    String str = tuple.getString(0).trim().toUpperCase();
                    if(str.isEmpty())
                        boltsIds.add(0);
                    else
                        boltsIds.add(str.charAt(0) % numTasks);
                    return boltsIds;
                }
                @Override
                public void prepare(int numTasks) {
                    this.numTasks = numTasks; 
                }
            });
----

Here we can see a very simple implementation of *CustomStreamGrouping* where we use the amount of tasks to take module on the first char and select which bolt will receive the tuple

==== Direct Grouping
This is an special grouping where the source decide which component will receive the tuple. Ever that we use direct grouping the stream must be emited direct. As example (using the count word example) we will decide which bolt receive the tuple based on the first letter of the word (like the example before)

So in our *WordNormalizer* bolt we've chage the *emit* by *emitDirect*

[source,java]
----
    public void execute(Tuple input) {
        ....
        for(String word : words){
            if(!word.isEmpty()){
                ....
                collector.emitDirect(getWordCountIndex(word),new Values(word));
            }
        }
        // Acknowledge the tuple
        collector.ack(input);
    }
    
    public Integer getWordCountIndex(String word) {
        word = word.trim().toUpperCase();
        if(word.isEmpty())
            return 0;
        else
            return word.charAt(0) % numCounterTasks;
    }
----

In the prepare method we'll get all the target task:

[source,java]
----
    public void prepare(Map stormConf, TopologyContext context,
            OutputCollector collector) {
        this.collector = collector;
        this.numCounterTasks = context.getComponentTasks("word-counter");
    }
----

And in our topology description we'll specify that the stream will be grouped direct:

[source,java]
----
    builder.setBolt("word-counter", new WordCounter(),2)
            .directGrouping("word-normalizer");
----

==== Global grouping
In the global grouping all the touples generated by the source (in all instances) will be sent to one target instace (Specifically the task with lowest id)

==== None grouping
At this moment (storm version 0.6.2), use this grouping is equals to use <<shuffle_grouping, Shuffle Grouping>>.  When we use this group we don't care about how the streams are grouped

=== LocalCluster vs StormSubmitter
efore, we have an utility named *LocalCluster* to run the topology into our computer, when we use this utility we are running all the storm infrastructure in our computer put this available to run any topology and debug it without any problem, but what
we need to do when we want to submit our topology to a running storm cluster? Well one of the very interestings features of storm is that is very easy put our topology to run in a real cluster. We'll need to change the *LocalCluster* by the *StormSubmitter*, this class has a method *submitTopology* which will be responsible for execute our topology into the cluster. So in our example we'll change the next code:

[source,java]
----
    LocalCluster cluster = new LocalCluster();
    cluster.submitTopology("Count-Word-Toplogy-With-Refresh-Cache", conf, builder.createTopology());
    Thread.sleep(1000);
    cluster.shutdown();
----

by the next code which will be responsible for submit the topology to the cluster

[source,java]
----
    StormSubmitter.submitTopology("Count-Word-Toplogy-With-Refresh-Cache", conf, builder.createTopology());
----

TIP: when you use *StormSubmiter* you can't control the cluster from your code as we can do with the *LocalCluster*

The next thing to do will be package the source into a jar which will be used when we run the storm client command to submit the topology, how we are used maven the only thing that we need to do is go to the source folder and run:

----
    mvn package
----

Ones we have the generated jar we will use the *storm jar* command which enable us to submit a topology using a jar, the sintaxis of  *storm jar allmycode.jar org.me.MyTopology arg1 arg2 arg3* in our example if we are in the topologies source project folder will run:

----
    storm jar target/Topologies-0.0.1-SNAPSHOT.jar countword.TopologyMain src/main/resources/words.txt
----

With these command we have summited the topology to the cluster, if we want to stop/kill it we need to run:

----
    storm kill Count-Word-Toplogy-With-Refresh-Cache
----

TIP: We can't have more than one topology with the same name

TIP: We can see how install the storm client in the <<install_storm_client, Apendix A: Install Storm Client>>

=== DRPC Topologies
There are an special topology type know as *DRPC* (Distributed Remote Procedure Call) this topology type will enable us to execute *RPC* (Remote Procedure Call) using the distributed powerfull of storm to execute the process call. To do it, storm give us some tools, the first is a DRPC server that runs as connector between the client and the storm topology, this server will receive the function to execute and the their parameters, then the server will receive the data and It will assing a request id that will be used through the topology to identify an RPC request unequivocally, when the topology executes the last bolt this must emit the RPC request id and the result as their parameters, then the DRPC server will return to the client the result, we can see the next picture to ilustrate this process

TIP: One DRPC server can execute many functions, and a function is different from other if their name is different

image::figs/ch03-drpc.jpg[]

The second tool that we can use (and we'll use in our example) is the *LinearDRPCTopologyBuilder* this is an abstraction to execute a DRPC topology, when we use this the topology generated will be responsible of create an *DRPCSpout* (which will connect to the DRPC servers and it will emit the data to the rest of topology) and wrap our bolts to return the data from our last bolt. With the LinearDRPCTopologyBuilder all bolts added will be executed in secuential order. 

As example of this topology type we'll do a process where we will add numbers (this sounds simple but one use case of this kind of topology could be complex distributed math opreations)

In our bolt we'll have the next output declarer: 
[source,java]
----
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id","result"));
    }
----
This bolt will be the unique bolt in our topology then It must emit the rpc id and the result as we can see in the source

Our bolt execute method will be responsible of execute the add as we can see here:
[source,java]
----
    public void execute(Tuple input) {
        String[] numbers = input.getString(1).split("\\+");
        Integer added = 0;
        try{
            if(numbers.length<2){
                throw new InvalidParameterException("Should be at least 2 numbers");
            }
            for(String num : numbers){
                added += Integer.parseInt(num);
            }
        }catch(Exception e){
            collector.emit(new Values(input.getValue(0),NULL));
        }
        collector.emit(new Values(input.getValue(0),added));
    }
----

So, we can use the adder bolt into the next topology definition:

[source,java]
----
    public static void main(String[] args) {
        LocalDRPC drpc = new LocalDRPC();
       
        LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("add");
        builder.addBolt(new AdderBolt(),2);
        
        Config conf = new Config();
        conf.setDebug(true);
        
        LocalCluster cluster = new LocalCluster();
        cluster.submitTopology("drpc-adder-topology", conf, builder.createLocalTopology(drpc));
        String result = drpc.execute("add", "1+-1");
        checkResult(result,0);
        result = drpc.execute("add", "1+1+5+10");
        checkResult(result,17);
        
        cluster.shutdown();
        drpc.shutdown();
    }
----

As we can see we've created a *LocalDRPC* object which will be our DRPC server implementation when runs the topology in local mode, next we've created the topology builder and we've added the bolt to the topology. To test this topology, we've used the *execute* method of our drpc object. If we want create a DRPC client to connect to a remote DRPC server we can use the *DRPCClient* class.

TIP: To submit a topology to an storm cluster, we'll use the method *createRemoteTopology* of the builder object instead of method createLocalTopology, this new method we'll use the DRPC configuration from the storm config.

=== Conclusions
In this chapter we've saw how the topologies are responsible for coordinate the streams and how to do it and how to run a topology into an storm cluster, also we've saw how the DRPC topologies works 
