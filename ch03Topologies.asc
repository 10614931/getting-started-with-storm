[[topologies]]
== Topologies
In this chapter, we'll see how to pass tuples between the different components of a Storm _topology_, and how to deploy a topology into a running Storm cluster

=== Stream Grouping
One of the most important things that we need to do when designing a topology is to define how data is exchanged between components (how streams are consumed by the bolts). A _Stream Grouping_ specifies which stream(s) are consumed by each +bolt+ and how the stream will be consumed.

TIP: A node can emit more than one _stream_ of data. A stream grouping allows us to choose which stream to receive.

The stream grouping is set when the topology is defined, as we saw in chapter 2, <<getting_started,Getting Started>>:

[source, java]
----
....
    builder.setBolt("word-normalizer", new WordNormalizer())
        .shuffleGrouping("word-reader");
....
----

Here a bolt is set on the topology builder, and then a source is set using the _shuffle stream grouping_. A stream grouping normally takes the source component id as a parameter, and optionally other parameters as well, depending on the kind of stream grouping.

TIP: There can be more than one source per +InputDeclarer+, and each source can be grouped with a different stream grouping.

[[shuffle_grouping]]
==== Shuffle Grouping

Shuffle Grouping is the most commonly used grouping. It takes a single parameter (the source component), and sends each tuple, emitted by the source, to a randomly chosen bolt warranting that each consumer will receive the same number of tuples    .

The shuffle grouping is useful for doing atomic operations. For example, a math operation. However if the operation can't be randomically distributed, such as the example in chapter 2 where we needed to count words, we should considerate the use of other grouping.

==== Fields Grouping

Fields Grouping allows us to control how tuples are sent to bolts, based on one or more fields of the tuple. It guarantees that a given set of values, for a combination of fields, is always sent to the same bolt. Coming back to the word count example, if we group the stream by the _word_ field, the +word-normalizer+ bolt will always send tuples with a given word to the same instance of the +word-counter+ bolt.

[source,java]
----
....
builder.setBolt("word-counter", new WordCounter(),2)
    .fieldsGrouping("word-normalizer", new Fields("word"));
....
----

TIP: All fields set in the fields grouping must exist in the sources's field declaration.

==== All Grouping

All Grouping sends a single copy of each tuple to all instances of the receiving bolt. This kind of grouping is used to send _signals_ to bolts, for example if we need to refresh a cache we can send a _refresh cache signal_ to all bolts. In the word-count example, we could use an all grouping to add the ability to clear the +counter+ cache (see link:https://github.com/storm-book/examples-ch03-topologies[Topologies Example])

[source,java]
----
    public void execute(Tuple input) {
        String str = null; 
        try{
            if(input.getSourceStreamId().equals("signals")){
                str = input.getStringByField("action");
                if("refreshCache".equals(str))
                    counters.clear();
            }
        }catch (IllegalArgumentException e) {
            //Do nothing
        }
        ....
    }
----

We've added an +if+ to check the stream source. Storm give us the posibility to declare named streams (if we don't send a tuple to a named stream the stream is +"default"+) it's an excelent way to identify the source of the tuples like this case where we want to identify the +signals+

In the topology definition, we add a second stream to the word-counter bolt that sends each tuple from the signals-spout stream to all instances of the bolt.

[source,java]
----
builder.setBolt("word-counter", new WordCounter(),2)
            .fieldsGrouping("word-normalizer", new Fields("word"))
            .allGrouping("signals-spout","signals");
----

The implementation of signals-spout can be found at link:https://github.com/storm-book/examples-ch03-topologies[git repository].

==== Custom Grouping

We can create our own custom stream grouping by implementing the +backtype.storm.grouping.CustomStreamGrouping+ interface. This gives us the power to decide which bolt(s) will receive each tuple. 

Let's modify the word count example, to group tuples so that all words that start with the same letter will be received by the same bolt.

[source,java]
----
public class ModuleGrouping implements CustomStreamGrouping, Serializable{

    int numTasks = 0;
    
    @Override
    public List<Integer> chooseTasks(List<Object> values) {
        List<Integer> boltIds = new ArrayList();
        if(values.size()>0){
            String str = values.get(0).toString();
            if(str.isEmpty())
                boltIds.add(0);
            else
                boltIds.add(str.charAt(0) % numTasks);
        }
        return boltIds;
    }

    @Override
    public void prepare(TopologyContext context, Fields outFields,
            List<Integer> targetTasks) {
        numTasks = targetTasks.size();
    }
}

----

Here we can see a simple implementation of +CustomStreamGrouping+, where we use the amount of tasks to take the modulus of the integer value of the first character of the word, thus selecting which bolt will receive the tuple.

To use this grouping in our example we should change the +word-normalizer+ grouping by the next:
[source, java]
----
       builder.setBolt("word-normalizer", new WordNormalizer())
            .customGrouping("word-reader", new ModuleGrouping()); 
----

==== Direct Grouping

This is a special grouping where the source decides which component will receive the tuple. Similarly to the previous example, the source will decide which bolt receives the tuple based on the first letter of the word.
To use direct grouping, in the +WordNormalizer+ bolt we use the +emitDirect+ method instead of +emit+.

[source,java]
----
    public void execute(Tuple input) {
        ....
        for(String word : words){
            if(!word.isEmpty()){
                ....
                collector.emitDirect(getWordCountIndex(word),new Values(word));
            }
        }
        // Acknowledge the tuple
        collector.ack(input);
    }
    
    public Integer getWordCountIndex(String word) {
        word = word.trim().toUpperCase();
        if(word.isEmpty())
            return 0;
        else
            return word.charAt(0) % numCounterTasks;
    }
----

We work out the number of target tasks in the +prepare+ method:

[source,java]
----
    public void prepare(Map stormConf, TopologyContext context,
            OutputCollector collector) {
        this.collector = collector;
        this.numCounterTasks = context.getComponentTasks("word-counter");
    }
----

And in the topology definition, we specify that the stream will be grouped directly:

[source,java]
----
    builder.setBolt("word-counter", new WordCounter(),2)
            .directGrouping("word-normalizer");
----

==== Global grouping

Global Grouping sends tuples generated by all instances of the source to a single target instance (specifically, the task with lowest id).

==== None grouping

At the time of writing (storm version 0.7.1), using this grouping is the same as using <<shuffle_grouping, Shuffle Grouping>>. In other words, when using this grouping, we don't care how streams are grouped

=== LocalCluster vs StormSubmitter

Up until now, we have used a utility called +LocalCluster+ to run the topology on our local computer. Running the Storm infrastructure on our computer lets us run and debug different topologies easily. But what about when we want to submit our topology to a running Storm cluster? One of the interesting features of storm is that it's easy to send our topology to run in a real cluster. We'll need to change the +LocalCluster+ to a +StormSubmitter+, and implement the +submitTopology+ method, which is responsible for sending the topology to the cluster. 

We can see the chages in the code below: 

[source,java]
----
    //LocalCluster cluster = new LocalCluster();
    //cluster.submitTopology("Count-Word-Topology-With-Refresh-Cache", conf, builder.createTopology());
    StormSubmitter.submitTopology("Count-Word-Topology-With-Refresh-Cache", conf, builder.createTopology());
    //Thread.sleep(1000);
    //cluster.shutdown();
----

TIP: When you use a +StormSubmiter+, you can't control the cluster from your code as you could with a +LocalCluster+.

Next, we package the source into a jar, which is sent when we run the Storm Client command to submit the topology. Because we used maven the only thing that we need to do is go to the source folder and run:

----
    mvn package
----

Once we have the generated jar, we use the +storm jar+ command to submit the topology (we should how to install the storm client into <<install_storm_client,Apendix A: Install Storm Client>> ). The sintax is +storm jar allmycode.jar org.me.MyTopology arg1 arg2 arg3+. 

In our example, from the topologies source project folder we run:

----
    storm jar target/Topologies-0.0.1-SNAPSHOT.jar countword.TopologyMain src/main/resources/words.txt
----

With these commands we have submitted the topology to the cluster. 

To stop/kill it we run:

----
    storm kill Count-Word-Topology-With-Refresh-Cache
----

TIP: The topology name must be unique.

TIP: To install the Storm Client see <<install_storm_client, Appendix A: Install Storm Client>>

=== DRPC Topologies

There is a special type of topology known as _DRPC_ (Distributed Remote Procedure Call), that executes _RPC_ (Remote Procedure Calls) using the distributed power of storm. Storm gives us some tools to enable the use of DRPC. The first, is a DRPC server that runs as a connector between the client and the Storm topology, running as a source for the toplogy spouts. It receives a function to execute and its parameters. Then for each piece of data on which the function operates, the server assigns a request id that is be used through the topology to identify the RPC request. When the topology executes the last bolt it must emit the RPC request id and the result, allowing the DRPC server to return the result to the correct client.

TIP: A single DRPC server can execute many functions. Each function is identified by a unique name.

image::figs/ch03-drpc.jpg[]

The second tool that Storm provides (we'll use it in our example) is the +LinearDRPCTopologyBuilder+, an abstraction to help build DRPC topologies. The topology generated creates +DRPCSpouts+, (which connect to DRPC servers and emit data to the rest of the topology) and wraps bolts so that a result is returned from the last bolt. All bolts added to a +LinearDRPCTopologyBuilder+ are executed in sequential order. 

As an example of this type of topology, we'll create a process that adds numbers (this is a simple example but the concept could be extended to perform complex distributed math operations).

The bolt has the following output declarer: 

[source,java]
----
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("id","result"));
    }
----

Because this is the only bolt in the topology it must emit the RPC id and the result.

The +execute+ method is responsible for executing the add operation:

[source,java]
----
    public void execute(Tuple input) {
        String[] numbers = input.getString(1).split("\\+");
        Integer added = 0;
        if(numbers.length<2){
            throw new InvalidParameterException("Should be at least 2 numbers");
        }
        for(String num : numbers){
            added += Integer.parseInt(num);
        }
        collector.emit(new Values(input.getValue(0),added));
    }
----

We include the added bolt in the topology definition as follows:

[source,java]
----
    public static void main(String[] args) {
        LocalDRPC drpc = new LocalDRPC();
       
        LinearDRPCTopologyBuilder builder = new LinearDRPCTopologyBuilder("add");
        builder.addBolt(new AdderBolt(),2);
        
        Config conf = new Config();
        conf.setDebug(true);
        
        LocalCluster cluster = new LocalCluster();
        cluster.submitTopology("drpc-adder-topology", conf, builder.createLocalTopology(drpc));
        String result = drpc.execute("add", "1+-1");
        checkResult(result,0);
        result = drpc.execute("add", "1+1+5+10");
        checkResult(result,17);
        
        cluster.shutdown();
        drpc.shutdown();
    }
----

We create a +LocalDRPC+ object which runs the DRPC server locally. Next we create a topology builder and add the bolt to the topology. To test the topology, we use the +execute+ method of our DRPC object.

TIP: To connect to a remote DRPC server use the +DRPCClient+ class. The DRPC server expose a Thrift API (link:http://thrift.apache.org/[]) that could be used from many languages and it's the same API if you run DRPC server in locally or remote.

TIP: To submit a topology to a Storm cluster, use the method +createRemoteTopology+ of the builder object instead of +createLocalTopology+, which uses the DRPC configuration from Storm config.

