[[getting_started]]
== Getting Started

In this chapter we'll create a Storm project and our first Storm topology.

TIP: The following assumes that you have at least version 1.6 of the Java Runtime Environment (JRE) installed. Our recommendation is to use the JRE provided by Oracle, which can be found at link:www.java.com/downloads/[]

[[operation_modes]]
=== Operation Modes

Before we start, it's important to understand Storm _operation modes_. There are two ways to run Storm.

==== Local Mode

In Local Mode, Storm topologies run on the local machine into a single JVM, this is used for development, testing, and debugging because it's the easiest way to see all topology components working toghether. In this mode we can adjust parameters that enable us to see how our topology runs in different storm configuration environments.
The only thing that we'll need to run our topologies in _Local Mode_ is's download the storm development dependencies that are all things that we need to develop and test our topologies, we'll see it soon in this chapter because we need of them to create our first storm project

TIP: Running a topology in Local Mode have similar behavior to running it in a Storm cluster, however it's important to make sure that all components are *thread safe*, because when they are deployed in _Remote Mode_ they may run in different JVMs or on different physical machines without direct communication or shared memory.


In all of the examples in this chapter, we'll work in Local Mode. 

==== Remote Mode

In Remote Mode, we submit our topology to the Storm cluster which is composed by different processes, usually running in different machines. Remote Mode doesn't show debugging information, which is why it's considered _Production Mode_. However, it's possible create an storm cluster into a single machine and deploy our topology there, this is a very good practice to execute after the development (where we use the _Local Mode_) because give us the posibility to see the topology running into an environment similar to the production cluster

We'll write more about Remote Mode in the chapter 6, <<a_real_life_example, A Real Life Example>> and we can see how to install a cluster into the <<install_storm_cluster,Apendix B: Install Storm Cluster>> 

=== Hello world Storm

For this project, we'll create a simple topology to count words. We can consider this the "Hello World" of Storm topologies. However, it's a very powerful topology because it can scale to virtually infinite size, and with some small modifications we could even use it to create a statistical system. For example, we could modify the project to find trending topics on Twitter.

To create the topology we'll use a spout that will be responsible for reading words, a first bolt to normalize words, and a second bolt to count words, as we can see in <<FIG201>>.

[[FIG201]]
.Getting started topology
image::figs/getting-started-topology.jpg[Figure 1 - Getting started topology]

You can download the source code of the example as a zip file at link:https://github.com/storm-book/examples-ch02-getting_started/zipball/master[]

if you like to use link:http://git-scm.com/[git] (a distributed revision control and source code management), you can run +git clone git@github.com:storm-book/examples-ch02-getting_started.git+ into the directory where you want to download the source code:

==== Checking Java installation

The first thing to do to set up the environment is to check which version of java you are running. Open a terminal window and run the command +java -version+. We should see something similar to the following:

----
java -version


java version "1.6.0_26"

Java(TM) SE Runtime Environment (build 1.6.0_26-b03)

Java HotSpot(TM) Server VM (build 20.1-b02, mixed mode)
----

If not, check your Java installation. (_See_ link:http://www.java.com/download/[].)


==== Creating the project

To begin our project, create a folder in which to place our application (like you would do for any Java application) this is a folder where we'll put our source code.

The second thing to do it's include the storm dependencies, these are a bunch of jars that we'll need to add to our application classpath, to do that we can take two ways 

- Download the dependencies, unpack it and add these to the classpath
- Use link:http://maven.apache.org/[Apache Maven] (in this book we use this way because maven is much better to control the dependencies of a project and give us other capabilities like packaging and to do our project easier to share)

As a brief approach to Maven we can say: Maven is a software project management and comprehension tool, this tool can manage all our project development ciclye from the dependencies to the release build process, in this book we'll use very intensive the maven capability to manage the project dependencies and dependencies repositories (maven use the concept of dependencies repositories as a site, tipically a web hosted site, where the dependencies can be found and downloaded), the second capability that will use will be generate and package our project.

To check if we have maven installed into our system we can run the command +mvn+ if you get an error running it you will need check your maven installation (_See_ link:http://maven.apache.org/download.html[])

TIP: Although is not necessary to be a Maven expert to use Storm, it's helpful to know the basics of how Maven works. You can find more information on the Apache Maven website (link:http://maven.apache.org/[]). 

Checked our maven instalation, we need to create a _pom.xml_ (project object model) file, this is a descriptor that maven will use to know how our project is formed (dependencies, packaging, source code, etc). We'll use the dependencies and Maven repository set up by nathanmarz (https://github.com/nathanmarz/). These dependencies can be found at link:https://github.com/nathanmarz/storm/wiki/Maven[]. 

TIP: The Storm Maven dependencies reference all the libraries required to run Storm in Local Mode.

Using these dependencies, we can write a _pom.xml_ file with the basic components necessary to run our topology.

----
include::code/ch02-getting-started/pom.xml[]
----
_(our-application-folder/pom.xml)_

In this file the first lines specify the project name and their version, then we'll add a compiler plugin which will say to maven that our code is wrote in _java 1.6_ and should be compiled in the same version, next we can found the repository definition (maven support multiple dependencies and dependencies repositories for the same project) where we are saying to maven that one of the repositories that our project will use will be _clojars_ (where the storm dependencies are located), so we'll add the _storm dependencies_ with this lines maven will download all sub-dependencies required by storm so we'll have all needed to run the _Local Mode_ 

Then we'll need to create our application structure:

----
our-application-folder/
        ├── pom.xml
        └── src
            └── main
                └── java
                |   ├── spouts
                |   └── bolts
                └── resources
----

to create these directories we should run into the application folder:

----
mkdir -p src/main/java/spouts
mkdir src/main/java/bolts
mkdir src/main/resources
----

This structure is the typically structure of a maven java project and running the +mkdir+ command with the +-p+ parameter at the first command we'll create all parents directories

=== Creating our first topology

To build our first topology, we'll create all classes required to run the word count. It's possible that some parts of the example may not be clear at this stage, but we'll explain them further in subsequent chapters.  

==== Spout

The WordReader spout is a class that implements IRichSpout. 
We'll see more about how it doest this in chapter 4, <<spouts,Spouts>>

WordReader will be responsible for reading the file and providing each line to a bolt.

TIP: A spout will _emit_ a list of defined fields. This architecture allows you to have different kinds of bolts reading the same spout stream, which can then define fields for other bolts to consume and so on.

The complete code of this class is (we'll analize each part of this code below):

[source,java]
----
include::code/ch02-getting-started/src/main/java/spouts/WordReader.java[]
----
_(src/main/java/spouts/WordReader.java)_

The first method called in any spout is +public void open(Map conf, TopologyContext context, SpoutOutputCollector collector)+. The parameters it receives are the TopologyContext, which contains all our topology data, the conf object, which is created in the topology definition, and the SpoutOutputCollector, which enables us to emit the data that will be processed by the bolts. Here's the open method implementation:

[source, java]
----
    public void open(Map conf, TopologyContext context,
            SpoutOutputCollector collector) {
        try {
            this.context = context;
            this.fileReader = new FileReader(conf.get("wordsFile").toString());
        } catch (FileNotFoundException e) {
            throw new RuntimeException("Error reading file ["+conf.get("wordFile")+"]");
        }
        this.collector = collector;
    }
----

In this method we also create the reader, which is responsible for reading the files.
Next we need to implement +public void nextTuple()+, from which we'll emit values to be processed by the bolts. In our example, the method will read the file and emit a value per line.

[source,java]
----
public void nextTuple() {
    if(completed){
        try {
                Thread.sleep(1);
        } catch (InterruptedException e) {
                //Do nothing
        }
        return;
    }
    String str;
    BufferedReader reader = new BufferedReader(fileReader);
    try{
        while((str = reader.readLine()) != null){
                this.collector.emit(new Values(str));
        }
    }catch(Exception e){
        throw new RuntimeException("Error reading tuple",e);
    }finally{
        completed = true;
    }
}
----

TIP: Values is an implementation of ArrayList, where the elements of the list are passed to the constructor.

+nextTuple()+ is called periodically from the same loop as the +ack()+ and +fail()+ methods. It must release control of the thread when there is no work to do so that the other methods have a chance to be called. So the first line of nextTuple checks to see if processing has finished. If so, it should sleep for at least one milli-second so as to reduce load on the processor before returning.
If there is work to be done, each line in the file is read into a value and emitted.

TIP: A tuple is a named list of values, which can be of any type of Java object (as long as the object is serializable). By default, Storm can serialize common types like strings, byte arrays, ArrayList, HashMap and HashSet.


==== Bolts

We now have a spout which reads from a file and emits one _tuple_ per line. We need to create two bolts to process these tuples (see <<FIG201>>). The bolts implement the +backtype.storm.topology.IRichBolt+ interface.

The most important method in the bolt is +void execute(Tuple input)+, which is called once per tuple received. The bolt will emit several tuples for each tuple received.

TIP: A bolt or spout can emit as many tuples as needed. When the method +nextTuple+ or +execute+ are called, they can emit 0, 1, or many tuples. We will write more about this in Chapter 5, <<bolts,Bolts>>.

The first bolt, +WordNormalizer+, will be responsible for taking each line and _normalizing_ it. It will split the line into words, convert all words to lower case, and trim them.

First we need to declare the bolt's output parameters:

[source,java]
----
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
        declarer.declare(new Fields("word"));
    }
----

Here we declare that this bolt will emit one Field named word.

Next we implement the +public void execute(Tuple input)+ method, where the input tuples are processed:

[source,java]
----
public void execute(Tuple input) {
    String sentence = input.getString(0);
    String[] words = sentence.split(" ");
    for(String word : words){
        word = word.trim();
        if(!word.isEmpty()){
            word = word.toLowerCase();
            //Emit the word
            collector.emit(new Values(word));
        }
    }
    // Acknowledge the tuple
    collector.ack(input);
}
----

The first line reads the value from the tuple. The value can be read by position or by name. The value is processed and then emitted using the collector object. After each tuple is processed, the collector's +ack()+ method is called to indicate that processing has completed successfully. If the tuple could not be processed, the collector's +fail()+ method should be called.

Here we can see the complete code class:

[source,java]
----
include::code/ch02-getting-started/src/main/java/bolts/WordNormalizer.java[]
----
_(src/main/java/bolts/WordNormalizer.java)_

TIP: In this class we see an example of emitting multiple tuples in a single +execute+ call. If the method receives the sentence _This is the Storm book_, in a single +execute+ call it will emit 5 new tuples.

The next bolt, +WordCounter+, will be responsible for counting words. When the topology finishes (when the +cleanup()+ method is called), we will show the count for each word.

TIP: Here we can see an example of a bolt that emits nothing. In this case the data is added to a map, but in real life the bolt could store data to a database.

[source,java]
----
include::code/ch02-getting-started/src/main/java/bolts/WordCounter.java[]
----
_(code/ch02-getting-started/src/main/java/bolts/WordCounter)_

Into the +execute+ method we are using a _Map_ to collect and count the words then we'll use the +cleanup()+ method, which is called when the topology terminates, to print out the counter map. (This is just an example, but generally you should use the +cleanup()+ method to close active connections or anything else that needs to be cleaned or closed when the topology shuts down)

==== The main class

In the main class, we'll create the topology and a +LocalCluster+ object, which enables us to test and debug the topology locally. In conjunction with the +Config+ object, +LocalCluster+ allows us to try out different cluster configurations. For example, if a global or class variable was accidentally used, you would find the error when testing your topology in configurations with a different number of workers. (We'll write more about config objects in chapter 3, <<topologies, Topologies>>.)

TIP: All your topology nodes should be able to run independently (with no shared data between processes, i.e. no global or class variables), because when you run the topology in a real cluster these processes may run on different machines.

We'll create the topology using a +TopologyBuilder+, which tells Storm how the nodes are arranged and how they exchange data.

[source,java]
----
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("word-reader",new WordReader());
builder.setBolt("word-normalizer", new WordNormalizer()).shuffleGrouping("word-reader");
builder.setBolt("word-counter", new WordCounter()).shuffleGrouping("word-normalizer");
----

The spout and the bolts are connected using +shuffleGrouping+s. This type of grouping tells Storm to send messages from the source node to target nodes in randomly distributed fashion.
 
Nexti, we create a +Config+ object containing the topology configuration, which is merged with the the cluster configuration at run time, and sent to all nodes with the +prepare+ method.

[source,java]
----
Config conf = new Config();
conf.put("wordsFile", args[0]);
conf.setDebug(true);
----

We set the property +wordsFile+ to the name of the file to be read by the spout, and the property +debug+ to *+true+* because we're in development. When debug is +true+, Storm prints all messages exchanged between nodes, and other debug data useful for understanding how the topology is running.

As explained above, we'll use a +LocalCluster+ to run the topology. In a production environment, the topology runs continuously, but for this example we'll just run the topology for a few seconds so we can see the results.

[source,java]
----
LocalCluster cluster = new LocalCluster();
cluster.submitTopology("Getting-Started-Toplogie", conf, builder.createTopology());
Thread.sleep(2000);
cluster.shutdown();
----

We create and run the topology using +createTopology+ and +submitTopology+, sleep for two seconds (the topology runs in a different thread) and then stop the topology by shutting down the cluster.

So if we glue all the before code into a TopologyMain class we'll be get the next code:
[source, java]
----
include::code/ch02-getting-started/src/main/java/TopologyMain.java[]
----
_(src/main/java/TopologyMain.java)_

==== See it in action

We're now ready to run our first topology! 
If you add a file to +src/main/resources/words.txt+ with words one below the other you could run:

----
mvn exec:java -Dexec.mainClass="TopologyMain" -Dexec.args="src/main/resources/words.txt"
----
If you use the next words.txt file:

----
storm
test
are
great
is
an
storm
simple
application
but
very
powerfull
really
StOrm
is
great
----

In the logs you should see something like the following:
----
is: 2
application: 1
but: 1
greate: 1
test: 1
simple: 1
storm: 3
really: 1
are: 1
great: 1
an: 1
powerfull: 1
very: 1
----

In this example, we're only using a single instance of each node. But what if we have a very large log file? We can easily change the number of nodes in the system to parallelize the work. In this case, we'll create two instances of +WordCounter+:

[source,java]
----
builder.setBolt("word-counter", new WordCounter(),2)
            .shuffleGrouping("word-normalizer");
----

If we re-run the program, we'll see:

----
-- Word Counter [word-counter-2] --
application: 1
is: 1
great: 1
are: 1
powerfull: 1
storm: 3
-- Word Counter [word-counter-3] --
really: 1
is: 1
but: 1
great: 1
test: 1
simple: 1
an: 1
very: 1

----

Awesome! It's so easy to change the level of parallelism (in real life, of course, each instance would run on a separate machine). But there seems to be a problem: the words _is_ and _great_ have been counted once in each instance of +WordCounter+. Why? When we use +shuffleGrouping+, we are telling Storm to send each message to an instance of our bolt in randomly distributed fashion. In this example we'd like to always send the same word to the same +WordCounter+. To do so, we can change +shuffleGrouping("word-normalizer")+ to +fieldsGrouping("word-normalizer",new Fields("word"))+. Try it out and re-run the program to confirm the results. We'll see more about groupings and message flow in later chapters.

=== Conclusion

We've discussed the difference between Storm's Local and Remote operation modes, and the power and ease of development with Storm. We also talked more about some basic Storm concepts, which we'll explain in depth in the following chapters.
