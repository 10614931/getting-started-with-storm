[[getting_started]]
== Getting Started

In this chapter we'll create a Storm project and our first Storm topology.

TIP: The following assumes that you have at least version 1.6 of the Java Runtime Environment (JRE) installed. Our recommendation is to use the JRE provided by Oracle, which can be found at (link:www.java.com/downloads/[]) 

[[operation_modes]]
=== Operation Modes

Before we start, it's important to understand _Storm operation modes_. There are two ways to run Storm.

==== Local Mode

In Local mode, Storm topologies run on the local machine. Local Mode is used for development, testing and debugging. You can adjust parameters that enable you to see how your topology runs in different cluster configuration environments. We'll see more about that later (<<testing_our_topology, Testing our topology>>).

In all of the examples in this chapter we'll work in *Local Mode*. 

==== Remote Mode

In Remote Mode you submit your topology to the Storm cluster, so it runs distributed across the machines in the cluster. Remote Mode doesn't show debugging information, which is why it's considered *Production Mode*. However it's possible to run your topology in Remote Mode in a development environment (and it's recommendable to do so). We will show how to do so in a later chapter (<<testing_our_topology, Testing our topology>>)

We'll see more about *Remote Mode* in the chapter <<a_real_life_example,A Real Life Example>>

=== Hello world Storm

For this project we'll create a simple topology to count words. We can consider this the _Hello World_ of Storm topologies, however it's a very powerful topology because it can scale to virtually infinite size, and with some small modifications we could even use it to create a statistical system. For example we could modify the project to find trending topics on Twitter.

To create the topology we'll use a spout that will be responsible for reading words, a first bolt to normalize words and a second bolt to count words, as we can see in <<getting_started_topology_img,figure 1>>


[[getting_started_topology_img]]
image:figs/getting-started-topology.jpg[Fig 1 - Getting started topology]

You can download the source code at link:https://github.com/storm-book/examples-ch02-getting_started[] 

==== Checking Java installation

The first thing to do to set up the environment is check your Java version. Open a console window and run the command *"java -version"*. You should see something similar to the following:

=======================================================
~$ java -version


java version "1.6.0_26"

Java(TM) SE Runtime Environment (build 1.6.0_26-b03)

Java HotSpot(TM) Server VM (build 20.1-b02, mixed mode)

=======================================================

If not please check your Java installation. (See link:http://www.java.com/download/[])


==== Creating the project

TIP: For Storm development we will use link:http://maven.apache.org/[Apache Maven] although is not necessary be a Maven expert to use Storm, it's recommended to know the basics of how Maven works. You can find more information on the Apache Maven website (link:http://maven.apache.org/[]). 

To begin our project, create a folder where we'll place our application (like any Java application).

Next we need to create a pom.xml file. We'll use the dependencies and Maven repository set up by nathanmarz (https://github.com/nathanmarz/). These dependencies can be found at https://github.com/nathanmarz/storm/wiki/Maven. _The Storm Maven dependencies reference all the libraries required to run Storm in_ *Local Mode*

Using these dependencies we can write a pom.xml file with the basic components necessary to run our topology.

----
include::code/getting-started/pom.xml[]
----

The application will have the following structure:

===================================
~$ tree getting-started/

    getting-started/
            ├── pom.xml
            └── src
                └── main
                    └── java
                        ├── spouts
                        └── bolts

===================================


=== Creating our first topology

To build our first topology we'll create all classes required to run the word count. It's possible that some parts of the example may not be clear at this stage, however we'll explain them further in subsequent chapters.  

==== Spout

The WordReader spout is a class that implements IRichSpout. We'll see more about how in the chapter on <<spouts,Spouts>>

WordReader will be responsible for reading the file and providing each line to a bolt.

TIP: A spout will *emit* a list of defined fields. This architecture enables you to have different kinds of bolts reading the same spout stream, which can then define fields for other bolts to consume and so on.

The most important method that we'll need to implement is *public void nextTuple()*, from which we'll _emit_ values to be processed by the bolts. In our example the method will read the file and *emit* a value per line.

[source,java]
-----------------------------------------------------------------
public void nextTuple() {
    if(completed){
        try {
                Thread.sleep(1);
        } catch (InterruptedException e) {
                //Do nothing
        }
        return;
    }
    String str;
    BufferedReader reader = new BufferedReader(fileReader);
    try{
        while((str = reader.readLine()) != null){
                this.collector.emit(new Values(str));
        }
    }catch(Exception e){
        throw new RuntimeException("Error reading tuple",e);
    }finally{
        completed = true;
    }
}
-----------------------------------------------------------------

nextTuple() is called periodically from the same loop as the ack() and fail() methods. It must release control of the Thread when there is no work to do so that the other methods have a chance to be called. So the first line of nextTuple checks to see if processing has finished. If so, it is recommended to sleep for at least one milli-second so as to reduce load on the processor before returning.
If there is work to be done, each line in the file is read into a Value and emitted.

TIP: A tuple is a named list of values, which can be of any type of Java object (as long as the object is serializable - more on that in the <<advanced_topics,Advanced Topics>> chapter). By default Storm can serialize common types like Strings, byte arrays, ArrayList, HashMap and HashSet.



==== Bolts

We now have a spout which reads from a file and emits one *tuple* per line. We need to create two bolts to process these tuples (see <<getting_started_topology_img,fig1>>) 

The most important method in the bolt is *void execute(Tuple input)* which is called once per tuple received. The bolt will emit several tuples for each tuple received.

TIP: It's important to realize that a bolt or spout can emit as many tuples as needed. When the method *nextTuple* or *execute* are called they can emit 0, 1 or many tuples. We will see more about this in the chapter <<bolts,Bolts>>.

The first bolt, *WordNormalizer*, will be responsible for taking each line and *normalizing* it. It will split the line into words, convert all words to lower case and trim them.

[source,java]
----------------------------------------------------
public void execute(Tuple input) {
    String sentence = input.getString(0);
    String[] words = sentence.split(" ");
    for(String word : words){
        word = word.trim();
        if(!word.isEmpty()){
            word = word.toLowerCase();
            //Emit the word
            collector.emit(new Values(word));
        }
    }
    // Acknowledge the tuple
    collector.ack(input);
}
----------------------------------------------------

The first line reads the value from the tuple. The value can be read by position or by name. The value is processed and then emitted using the collector object. After each tuple is processed the collector's ack() method is called to indicate that processing has completed successfully. If the tuple could not be processed, the collector's fail() method should be called.

TIP: In this class we see an example of emitting multiple tuples in a single *execute* call. If the method receives the sentence _This is the Storm book_, in a single *execute* call it will emit 5 new tuples.

The next bolt, *WordCounter*, will be responsible for counting words. When the topology finishes (when the cleanup() method is called), we will show the count for each word.

TIP: Here we can see an example of a bolt that emits nothing.

[source,java]
--------------------------------------------
public void execute(Tuple input) {
    String str = input.getString(0);
    if(!counters.containsKey(str)){
        counters.put(str, 1);
    }else{
        Integer c = counters.get(str) + 1;
        counters.put(str, c);
    }
    //Set the tuple as Acknowledge
    collector.ack(input);
}

--------------------------------------------

We'll use the *cleanup()* method, which is called when the topology terminates, to print out the counter map.

[source,java]
----------------------------------------------------------------
public void cleanup() {
    System.out.println("-- Word Counter ["+name+"-"+id+"] --");
    for(Map.Entry<String, Integer> entry : counters.entrySet()){
        System.out.println(entry.getKey()+": "+entry.getValue());
    }
}

----------------------------------------------------------------

==== The main class

In the main class we'll create the topology and a *LocalCluster* object, which enables us to test and debug the topology locally. In conjunction with the *Config* object, *LocalCluster* allows us to try out different cluster configurations. For example if a global or class variable was accidentally used you'll find the error when testing your topology in configurations with a different number of workers. (We'll see more about config objects in the <<topologies, Topologies>> chapter)

TIP: All your topology nodes should be able to run independently (no shared data between processes, ie. no global or class variables), because when you run the topology in a real cluster these processes may run on different machines.

We'll create the topology using a *TopologyBuilder*, which tells Storm how the nodes are arranged and how they exchange data.

[source,java]
--------------------------------------------------------------------------------------------------------
TopologyBuilder builder = new TopologyBuilder();
builder.setSpout("word-reader",new WordReader());
builder.setBolt("word-normalizer", new WordNormalizer()).shuffleGrouping("word-reader");
builder.setBolt("word-counter", new WordCounter()).shuffleGrouping("word-normalizer");
--------------------------------------------------------------------------------------------------------

The spout and the bolts are connected using *shuffleGrouping*s. This type of grouping tells Storm to send messages from the source node to target nodes in randomly distributed fashion.
 
Next we create a *Config* object containing the topology configuration, which is merged with the the cluster configuration at run time, and sent to all nodes with the *prepare* method.

[source,java]
--------------------------------
Config conf = new Config();
conf.put("wordsFile", args[0]);
conf.setDebug(true);
--------------------------------

We set the property _wordsFile_ to the name of the file to be read by the spout, and the property *debug* to *true* because we're in development. When debug is true, Storm prints all messages exchanged between nodes, and other debug data useful for understanding how the topology is running.

As explained above, we'll use a *LocalCluster* to run the topology. In a production environment the topology runs continuously, but for this example we'll just run the topology for a few seconds so we can see the results.

[source,java]
-----------------------------------------------------------------------------------
LocalCluster cluster = new LocalCluster();
cluster.submitTopology("Getting-Started-Toplogie", conf, builder.createTopology());
Thread.sleep(2000);
cluster.shutdown();
-----------------------------------------------------------------------------------

We create and run the topology using *createTopology* and *submitTopology*, sleep for two seconds (the topology runs in a different thread) and then stop the topology by shutting down the cluster.


==== See it in action

We're now ready to run our first topology! If you've downloaded the examples from the repository you can try it out:

----------------------------------------------------------------------------------------
mvn exec:java -Dexec.mainClass="TopologyMain" -Dexec.args="src/main/resources/words.txt"
----------------------------------------------------------------------------------------

In the logs you should see something like the following:
---------------
is: 2
application: 1
but: 1
greate: 1
test: 1
simple: 1
storm: 3
really: 1
are: 1
great: 1
an: 1
powerfull: 1
very: 1
---------------

In this example we're only using a single instance of each node. But what if we have a very large log file? We can easily change the number of nodes in the system to parallelize the work. In this case we'll create two instances of WordCounter:

[source,java]
--------------------------------------------------
builder.setBolt("word-counter", new WordCounter(),2)
            .shuffleGrouping("word-normalizer");
--------------------------------------------------

If we re-run the program we'll see:

------------------------------------------
-- Word Counter [word-counter-2] --
application: 1
is: 1
great: 1
are: 1
powerfull: 1
storm: 3
-- Word Counter [word-counter-3] --
really: 1
is: 1
but: 1
great: 1
test: 1
simple: 1
an: 1
very: 1

------------------------------------------

Awesome! It's so easy to change the level of parallelism (in real life of course each instance would run on a separate machine). But there seems to be a problem: the words _is_ and _great_ have been counted once in each instance of WordCounter. Why? The reason is that when we use *shuffleGrouping* we are telling Storm to send each message to an instance of our bolt in randomly fashion. In this example we'd like to always send the same word to the same WordCounter. To do so we can change *shuffleGrouping("word-normalizer")* to *fieldsGrouping("word-normalizer",new Fields("word"))*. Try it out and re-run the program to confirm the results. We'll see more about groupings and message flow in later chapters.

=== Conclusion

We've discussed the difference between Storm's Local and Remote operation modes and the power and ease of development with Storm. We also talked more about some basic Storm concepts, which we'll explain in depth in the following chapters.
